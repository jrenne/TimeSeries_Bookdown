[{"path":"index.html","id":"intro","chapter":"Introduction to Time Series","heading":"Introduction to Time Series","text":"Time series constitute prevalent data type several disciplines, notably macroeconomics finance. modeling time series crucial many purposes, including forecasting, understanding macroeconomic mechanisms, risk assessment. course proposes introduction time series analysis. developed Jean-Paul Renne.Codes associated course part AEC package, available GitHub. load package GitHub, need use function install_github devtools package:Useful (R) links:Download R:\nR software: https://cran.r-project.org (basic R software)\nRStudio: https://www.rstudio.com (convenient R editor)\nDownload R:R software: https://cran.r-project.org (basic R software)RStudio: https://www.rstudio.com (convenient R editor)Tutorials:\nRstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)\nR: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)\ntutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/\nTutorials:Rstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)R: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)tutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/","code":"\ninstall.packages(\"devtools\") # in case you do not have that one.\nlibrary(devtools)\ninstall_github(\"jrenne/AEC\")\nlibrary(AEC)"},{"path":"Intro.html","id":"Intro","chapter":"1 Basics","heading":"1 Basics","text":"","code":""},{"path":"Intro.html","id":"shocks-and-lag-operator","chapter":"1 Basics","heading":"1.1 Shocks and lag operator","text":"time series infinite sequence random variables indexed time: \\(\\{y_t\\}_{t=-\\infty}^{+\\infty}=\\{\\dots, y_{-2},y_{-1},y_{0},y_{1},\\dots,y_t,\\dots\\}\\), \\(y_i \\\\mathbb{R}^k\\). practice, observe samples, typically: \\(\\{y_{1},\\dots,y_T\\}\\).Standard time series models built using shocks often denote \\(\\varepsilon_t\\). Typically, \\(\\mathbb{E}(\\varepsilon_t)=0\\). many models, shocks supposed ..d., exist (less restrictive) notions shocks. particular, definition many processes based white noises:Definition 1.1  (White noise) process \\(\\{\\varepsilon_t\\}_{t \\] -\\infty,+\\infty[}\\) white noise , \\(t\\):\\(\\mathbb{E}(\\varepsilon_t)=0\\),\\(\\mathbb{E}(\\varepsilon_t^2)=\\sigma^2<\\infty\\),\\(s\\ne t\\), \\(\\mathbb{E}(\\varepsilon_t \\varepsilon_s)=0\\).Another type shocks commonly used Martingale Difference Sequences:Definition 1.2  (Martingale Difference Sequence) process \\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) martingale difference sequence (MDS) \\(\\mathbb{E}(|\\varepsilon_{t}|)<\\infty\\) , \\(t\\),\n\\[\n\\underbrace{\\mathbb{E}_{t-1}(\\varepsilon_{t})}_{\\mbox{conditional past}}=0.\n\\]definition, \\(y_t\\) martingale, \\(y_{t}-y_{t-1}\\) MDS.Example 1.1  (ARCH process) Autoregressive conditional heteroskedasticity process—studied Section 7—example shock satisfies white noise MDS definitions, ..d.:\n\\[\n\\varepsilon_{t} = \\sigma_t \\times z_{t},\n\\]\n\\(z_t \\sim ..d.\\,\\mathcal{N}(0,1)\\) \\(\\sigma_t^2 = w + \\alpha \\varepsilon_{t-1}^2\\).\nFigure 1.1: Simulation \\(\\varepsilon_t\\), \\(\\varepsilon_{t} = \\sigma_t \\times z_{t}\\), \\(z_t \\sim ..d.\\,\\mathcal{N}(0,1)\\).\nExample 1.2  white noise process necessarily MDS. instance case following process:\n\\[\n\\varepsilon_{t} = z_t + z_{t-1}z_{t-2},\n\\]\n\\(z_t \\sim ..d.\\mathcal{N}(0,1)\\).Indeed, can shown \\(\\varepsilon_t\\)’s correlated time, \\(\\mathbb{E}_{t-1}(\\varepsilon_t)=z_{t-1}z_{t-2} \\ne 0\\).following, simplify exposition, essentially consider strong white noises. strong white noise particular case white noise \\(\\varepsilon_{t}\\)’s serially independent. Using law iterated expectation, can shown strong white noise martingale difference sequence. Indeed, \\(\\varepsilon_{t}\\)’s serially independent, \\(\\mathbb{E}(\\varepsilon_{t}|\\varepsilon_{t-1},\\varepsilon_{t-2},\\dots)=\\mathbb{E}(\\varepsilon_{t})=0\\).Let us now introduce lag operator. lag operator, denoted \\(L\\), defined time series space defined :\n\\[\\begin{equation}\nL: \\{y_t\\}_{t=-\\infty}^{+\\infty} \\rightarrow \\{w_t\\}_{t=-\\infty}^{+\\infty} \\quad \\mbox{} \\quad w_t = y_{t-1}.\\tag{1.1}\n\\end{equation}\\]easily seen \\(L^2 y_t =L(L y_t) = y_{t-2}\\) , generally, \\(L^k y_t = y_{t-k}\\).Consider process \\(y_t\\) whose law motion \\(y_t = \\mu + \\phi y_{t-1} + \\varepsilon_t\\), \\(\\varepsilon_t\\)’s ..d. \\(\\mathcal{N}(0,\\sigma^2)\\) (AR(1) process, see Section 2). Using lag operator, dynamics \\(y_t\\) can expressed follows:\n\\[\n(1-\\phi L) y_t = \\mu + \\varepsilon_t.\n\\]","code":"\nT <- 500\nw <- 1; alpha <- .98\nall_epsilon <- NULL; epsilon_1 <- 0\nfor(t in 1:T){\n  sigma <- sqrt(w + alpha * epsilon_1^2)\n  epsilon <- sigma * rnorm(1)\n  all_epsilon <- c(all_epsilon,epsilon)\n  epsilon_1 <- epsilon\n}\npar(plt=c(.15,.95,.1,.95))\nplot(all_epsilon,type=\"l\",lwd=2,\n     ylab=expression(epsilon[t]),xlab=\"\")"},{"path":"Intro.html","id":"conditional-and-unconditional-moments","chapter":"1 Basics","heading":"1.2 Conditional and unconditional moments","text":"exists, unconditional (marginal) mean random variable \\(y_t\\) given :\n\\[\n\\mu_t := \\mathbb{E}(y_t) = \\int_{-\\infty}^{\\infty} y_t f_{Y_t}(y_t) dy_t,\n\\]\n\\(f_{Y_t}\\) unconditional, marginal, density (p.d.f.) \\(y_t\\). Note , general case, \\(Y_t\\) \\(Y_{t-1}\\), may different densities; , general, \\(f_{Y_t} \\ne f_{Y_{t-1}}\\) (, particular, \\(\\mu_t \\ne \\mu_{t-1}\\)).Similarly, exists, unconditional (marginal) variance random variable \\(y_t\\) :\n\\[\n\\mathbb{V}ar(y_t) = \\int_{-\\infty}^{\\infty} (y_t - \\mathbb{E}(y_t))^2 f_{Y_t}(y_t) dy_t.\n\\]Definition 1.3  (Autocovariance) \\(j^{th}\\) autocovariance \\(y_t\\) given :\n\\[\\begin{eqnarray*}\n\\gamma_{j,t} &:=& \\mathbb{E}([y_t - \\mathbb{E}(y_t)][y_{t-j} - \\mathbb{E}(y_{t-j})])\\\\\n&=& \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\dots \\int_{-\\infty}^{\\infty} [y_t - \\mathbb{E}(y_t)][y_{t-j} - \\mathbb{E}(y_{t-j})] \\times\\\\\n&& f_{Y_t,Y_{t-1},\\dots,Y_{t-j}}(y_t,y_{t-1},\\dots,y_{t-j}) dy_t dy_{t-1} \\dots dy_{t-j},\n\\end{eqnarray*}\\]\n\\(f_{Y_t,Y_{t-1},\\dots,Y_{t-j}}(y_t,y_{t-1},\\dots,y_{t-j})\\) joint distribution \\(y_t,y_{t-1},\\dots,y_{t-j}\\).particular, note \\(\\gamma_{0,t} = \\mathbb{V}ar(y_t)\\).Definition 1.4  (Covariance stationarity) process \\(y_t\\) covariance stationary —weakly stationary— , \\(t\\) \\(j\\),\n\\[\n\\mathbb{E}(y_t) = \\mu \\quad \\mbox{} \\quad \\mathbb{E}\\{(y_t - \\mu)(y_{t-j} - \\mu)\\} = \\gamma_j.\n\\]Figure 1.2 displays simulation process covariance stationary. process follows \\(y_t = 0.1t + \\varepsilon_t\\), \\(\\varepsilon_t \\sim\\,..d.\\,\\mathcal{N}(0,1)\\). Indeed, process, : \\(\\mathbb{E}(y_t)=0.1t\\), depends \\(t\\).\nFigure 1.2: Example process covariance stationary. follows: \\(y_t = 0.1t + \\varepsilon_t\\), \\(\\varepsilon_t \\sim \\mathcal{N}(0,1)\\).\nDefinition 1.5  (Strict stationarity) process \\(y_t\\) strictly stationary , \\(t\\) sets integers \\(J=\\{j_1,\\dots,j_n\\}\\), distribution \\((y_{t},y_{t+j_1},\\dots,y_{t+j_n})\\) depends \\(J\\) \\(t\\).following process covariance stationary strictly stationary:\n\\[\ny_t = \\mathbb{}_{\\{t<1000\\}}\\varepsilon_{1,t}+\\mathbb{}_{\\{t\\ge1000\\}}\\varepsilon_{2,t},\n\\]\n\\(\\varepsilon_{1,t} \\sim \\mathcal{N}(0,1)\\) \\(\\varepsilon_{2,t} \\sim \\sqrt{\\frac{\\nu - 2}{\\nu}} t(\\nu)\\) \\(\\nu = 4\\). simulated path displayed Figure 1.3.\nFigure 1.3: Example process covariance stationary strictly stationary. red lines delineate 99% confidence interval standard normal distribution (\\(\\pm 2.58\\)).\nProposition 1.1  \\(y_t\\) covariance stationary, \\(\\gamma_j = \\gamma_{-j}\\).Proof. Since \\(y_t\\) covariance stationary, covariance \\(y_t\\) \\(y_{t-j}\\) (.e., \\(\\gamma_j\\)) \\(y_{t+j}\\) \\(y_{t+j-j}\\) (.e. \\(\\gamma_{-j}\\)).Definition 1.6  (Auto-correlation) \\(j^{th}\\) auto-correlation covariance-stationary process :\n\\[\n\\rho_j = \\frac{\\gamma_j}{\\gamma_0}.\n\\]Consider long historical time series Swiss GDP growth, taken Jordà, Schularick, Taylor (2017) dataset.1\nFigure 1.4: Annual growth rate Swiss GDP, based Jorda-Schularick-Taylor Macrohistory Database.\nFigure 1.5 shows two scatter plots. first one, coordinates point form \\((y_{t-1},y_t)\\). Hence, slope OLS regression line (blue) \\(\\widehat{\\mathbb{C}ov}(y_t,y_{t-1})/\\widehat{\\mathbb{V}ar}(y_{t-1})\\) (hats indicate moments sample ones). Assuming process covariance stationary, slope therefore estimate auto-correlation order one. logic, slope blue line second plot estimate auto-correlation order three.\nFigure 1.5: order \\(j\\), slope blue line , approximately, \\(\\hat{\\gamma}_j/\\widehat{\\mathbb{V}ar}(y_t)\\), hats indicate sample moments.\n","code":"\nT <- 200\ny <- 0.1*(1:T) + rnorm(T)\npar(plt=c(.1,.95,.1,.95))\nplot(y,xlab=\"\",ylab=\"\",lwd=2,type=\"l\")\nT <- 2000\ny <- rnorm(T)\ny[(T/2):T] <- rt(n = T/2 + 1,df=4)\npar(plt=c(.1,.95,.15,.95))\nplot(y,xlab=\"\",ylab=\"\",type=\"l\")\nabline(h=-2.58,col=\"red\",lty=3)\nabline(h=2.58,col=\"red\",lty=3)\nlibrary(AEC)\ndata(JST);data <- subset(JST,iso==\"CHE\")\npar(plt=c(.1,.95,.1,.95))\nT <- dim(data)[1]\ndata$growth <- c(NaN,log(data$gdp[2:T]/data$gdp[1:(T-1)]))\nplot(data$year,data$growth,type=\"l\",xlab=\"\",ylab=\"\",lwd=2)\nabline(h=mean(data$growth,na.rm = TRUE),col=\"blue\",lty=2,xlab=\"\",ylab=\"\")"},{"path":"Intro.html","id":"central-limit-theorem-clt-for-persistent-processes","chapter":"1 Basics","heading":"1.3 Central Limit Theorem (CLT) for persistent processes","text":"subsection shows Central Limit Theorem (CLT, see Theorem 8.5) can extended cases observations auto-correlated.Theorem 1.1  (Central Limit Theorem covariance-stationary processes) process \\(y_t\\) covariance stationary series autocovariances absolutely summable (\\(\\sum_{j=-\\infty}^{+\\infty} |\\gamma_j| <\\infty\\)), :\n\\[\\begin{eqnarray}\n\\bar{y}_T \\overset{m.s.}{\\rightarrow} \\mu &=& \\mathbb{E}(y_t) \\tag{1.2}\\\\\n\\mbox{lim}_{T \\rightarrow +\\infty} T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] &=& \\sum_{j=-\\infty}^{+\\infty} \\gamma_j \\tag{1.3}\\\\\n\\sqrt{T}(\\bar{y}_T - \\mu) &\\overset{d}{\\rightarrow}& \\mathcal{N}\\left(0,\\sum_{j=-\\infty}^{+\\infty} \\gamma_j \\right) \\tag{1.4}.\n\\end{eqnarray}\\][Mean square (m.s.) distribution (d.) convergences: see Definitions 8.16 8.14.]Proof. Proposition 8.8, Eq. (1.3) implies Eq. (1.2). Eq. (1.3), see Appendix 8.4. Eq. (1.4), see Anderson (1971), p. 429.Definition 1.7  (Long-run variance) assumptions Theorem 1.1, limit appearing Eq. (1.3) exists called long-run variance. denoted \\(S\\), .e.:\n\\[\nS = \\Sigma_{j=-\\infty}^{+\\infty} \\gamma_j  = \\mbox{lim}_{T \\rightarrow +\\infty} T \\mathbb{E}[(\\bar{y}_T - \\mu)^2].\n\\]\\(y_t\\) ergodic second moments (see Def. 8.8), natural estimator \\(S\\) :\n\\[\\begin{equation}\n\\hat\\gamma_0 + 2 \\sum_{\\nu=1}^{q} \\hat\\gamma_\\nu, \\tag{1.5}\n\\end{equation}\\]\n\\(\\hat\\gamma_\\nu = \\frac{1}{T}\\sum_{\\nu+1}^{T} (y_t - \\bar{y})(y_{t-\\nu} - \\bar{y})\\).However, small samples, Eq. (1.5) necessarily result positive definite matrix. Newey West (1987) proposed estimator defect. estimator given :\n\\[\\begin{equation}\nS^{NW}=\\hat\\gamma_0 + 2 \\sum_{\\nu=1}^{q}\\left(1-\\frac{\\nu}{q+1}\\right) \\hat\\gamma_\\nu.\\tag{1.6}\n\\end{equation}\\]Loosely speaking, Theorem 1.1 says , given sample size, higher “persistency” process, lower accuracy sample mean estimate population mean. illustrate, consider three processes feature marginal variance (equal one, say), different autocorrelations: 0%, 70%, 99.9%. Figure 1.6 displays simulated paths three processes. indeed appears , larger autocorrelation process, sample mean (dashed red line) population mean (red solid line).type simulations can performed using web interface (use panel “AR(1)”).\nFigure 1.6: three samples simulated using following data generating process: \\(x_t = \\mu + \\rho (x_{t-1}-\\mu) + \\sqrt{1-\\rho^2}\\varepsilon_t\\), \\(\\varepsilon_t \\sim \\mathcal{N}(0,1)\\). Case : \\(\\rho = 0\\); Case B: \\(\\rho = 0.7\\); Case C: \\(\\rho = 0.999\\). three cases, \\(\\mathbb{E}(x_t)=\\mu=2\\) \\(\\mathbb{V}ar(x_t)=1\\). dashed (respectively solid) red line indicate sample (resp. unconditional) mean.\n","code":""},{"path":"Univariate.html","id":"Univariate","chapter":"2 Univariate processes","heading":"2 Univariate processes","text":"","code":""},{"path":"Univariate.html","id":"moving-average-ma-processes","chapter":"2 Univariate processes","heading":"2.1 Moving Average (MA) processes","text":"Moving average (MA) processes important basic processes. moving-average process order one (MA(1)) defined follows:Definition 2.1  (Moving average process order one) Process \\(y_t\\) first-order moving average process , \\(t\\):\n\\[\\begin{equation}\ny_t = \\mu + \\varepsilon_t + \\theta \\varepsilon_{t-1},\\tag{2.1}\n\\end{equation}\\]\n\\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) white noise process (see Def. 1.1).\\(\\mathbb{E}(\\varepsilon_t^2)=\\sigma^2\\), easily obtained unconditional mean variance \\(y_t\\) :\n\\[\n\\mathbb{E}(y_t) = \\mu, \\quad \\mathbb{V}ar(y_t) = (1+\\theta^2)\\sigma^2.\n\\]first auto-covariance :\n\\[\n\\gamma_1=\\mathbb{E}\\{(y_t - \\mu)(y_{t-1} - \\mu)\\}=\\mathbb{E}\\{(\\varepsilon_t + \\theta \\color{red}{\\varepsilon_{t-1}})(\\color{red}{\\varepsilon_{t-1}} + \\theta \\varepsilon_{t-2})\\} = \\theta \\sigma^2.\n\\]easily seen higher-order auto-covariances zero (\\(\\gamma_j=0\\) \\(j>1\\)). Therefore: MA(1) process covariance-stationary (Def. 1.4).precedes, autocorrelation order \\(j\\) (see Def. 1.6) MA(1) process given :\n\\[\n\\rho_j =\n\\left\\{\n\\begin{array}{lll}\n1 &\\mbox{ }& j=0,\\\\\n\\theta / (1 + \\theta^2) &\\mbox{ }& j = 1\\\\\n0 &\\mbox{ }& j>1.\n\\end{array}\n\\right.\n\\]Notice process \\(y_t\\) defined Eq. (2.1), \\(\\mathbb{V}ar(\\varepsilon_t)=\\sigma^2\\), mean autocovariances \n\\[\ny_t = \\mu + \\varepsilon^*_t +\\frac{1}{\\theta}\\varepsilon^*_{t-1},\n\\]\n\\(\\mathbb{V}ar(\\varepsilon^*_t)=\\theta^2\\sigma^2\\). , knowing mean auto-covariances MA(1) process sufficient identify process, since two different processes possess moments. one two specifications said fundamental, one satisfies \\(|\\theta_1|<1\\) (see Eq. (2.23)).Definition 2.2  (MA(q) process) \\(q^{th}\\)-order Moving Average process \\(\\{y_t\\}\\) defined :\n\\[\ny_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q},\n\\]\n\\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) white noise process (Def. 1.1).Proposition 2.1  (Covariance-stationarity MA(q) process) Finite-order Moving Average processes covariance-stationary.Moreover, autocovariances MA(q) process (defined Def. 2.2) given :\n\\[\\begin{equation}\n\\gamma_j = \\left\\{ \\begin{array}{ll} \\sigma^2(\\theta_j\\theta_0 + \\theta_{j+1}\\theta_{1} +  \\dots + \\theta_{q}\\theta_{q-j}) &\\mbox{} \\quad j \\\\{0,\\dots,q\\} \\\\ 0 &\\mbox{} \\quad j>q, \\end{array} \\right.\\tag{2.2}\n\\end{equation}\\]\nuse notation \\(\\theta_0=1\\), \\(\\mathbb{V}ar(\\varepsilon_t)=\\sigma^2\\).Proof. unconditional expectation \\(y_t\\) depend time, since \\(\\mathbb{E}(y_t)=\\mu\\). Turning autocovariances, can extend series \\(\\theta_j\\)’s setting \\(\\theta_j=0\\) \\(j>q\\). :\n\\[\\begin{eqnarray*}\n\\mathbb{E}((y_t-\\mu)(y_{t-j}-\\mu)) &=& \\mathbb{E}\\left[(\\theta_0 \\varepsilon_t +\\theta_1 \\varepsilon_{t-1} + \\dots +\\theta_j \\color{red}{\\varepsilon_{t-j}}+\\theta_{j+1} \\color{blue}{\\varepsilon_{t-j-1}} + \\dots) \\right.\\times \\\\\n&&\\left. (\\theta_0 \\color{red}{\\varepsilon_{t-j}} +\\theta_1 \\color{blue}{\\varepsilon_{t-j-1}} + \\dots)\\right].\n\\end{eqnarray*}\\]\nUsing fact \\(\\mathbb{E}(\\varepsilon_t\\varepsilon_s)=0\\) \\(t \\ne s\\) (\\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) white noise process) leads result.Figure 2.1 displays simulated paths two MA processes (MA(1) MA(4)). simulations can also produced using panel “ARMA(p,q)” web interface.\nFigure 2.1: Simulation MA processes.\norder \\(q\\) MA(q) process gets infinite? notion infinite-order Moving Average process exists important time series analysis, relates impulse response functions (illustrated Section 2.6). (infinite) sequence \\(\\theta_j\\) satisfy conditions process well-defined (see Theorem 2.1 ). conditions relate “summability” components sequence \\(\\{\\theta_{}\\}_{\\\\mathbb{N}}\\):Definition 2.3  (Absolute square summability) sequence \\(\\{\\theta_{}\\}_{\\\\mathbb{N}}\\) absolutely summable \\(\\sum_{=0}^{\\infty}|\\theta_i| < + \\infty\\), square summable \\(\\sum_{=0}^{\\infty} \\theta_i^2 < + \\infty\\).According Prop. 8.8 (appendix), absolute summability implies square summability.Theorem 2.1  (Existence condition infinite MA process) \\(\\{\\theta_{}\\}_{\\\\mathbb{N}}\\) square summable (see Def. 2.3) \\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) white noise process (see Def. 1.1), \n\\[\n\\mu + \\sum_{=0}^{+\\infty} \\theta_{} \\varepsilon_{t-}\n\\]\ndefines well-behaved [covariance-stationary] process, called infinite-order MA process (MA(\\(\\infty\\))).Proof. See Appendix 3.Hamilton. “Well behaved” means \\(\\Sigma_{=0}^{T} \\theta_{t-} \\varepsilon_{t-}\\) converges mean square (Def. 8.14) random variable \\(Z_t\\). proof makes use fact :\n\\[\n\\mathbb{E}\\left[\\left(\\sum_{=N}^{M}\\theta_{} \\varepsilon_{t-}\\right)^2\\right] = \\sum_{=N}^{M}|\\theta_{}|^2 \\sigma^2,\n\\]\n, \\(\\{\\theta_{}\\}\\) square summable, \\(\\forall \\eta>0\\), \\(\\exists N\\) s.t. right-hand-side term last equation lower \\(\\eta\\) \\(M \\ge N\\) (static Cauchy criterion, Theorem 8.2). implies \\(\\Sigma_{=0}^{T} \\theta_{} \\varepsilon_{t-}\\) converges mean square (stochastic Cauchy criterion, see Theorem 8.3).Proposition 2.2  (First two moments infinite MA process) \\(\\{\\theta_{}\\}_{\\\\mathbb{N}}\\) absolutely summable, .e., \\(\\sum_{=0}^{\\infty}|\\theta_i| < + \\infty\\), \\(y_t = \\mu + \\sum_{=0}^{+\\infty} \\theta_{} \\varepsilon_{t-}\\) exists (Theorem 2.1) :\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y_t) &=& \\mu\\\\\n\\gamma_0 = \\mathbb{E}([y_t-\\mu]^2) &=& \\sigma^2(\\theta_0^2 +\\theta_1^2 + \\dots)\\\\\n\\gamma_j = \\mathbb{E}([y_t-\\mu][y_{t-j}-\\mu]) &=& \\sigma^2(\\theta_0\\theta_j + \\theta_{1}\\theta_{j+1} + \\dots).\n\\end{eqnarray*}\\]Process \\(y_t\\) absolutely summable auto-covariances, implies results Theorem 1.1 (Central Limit) apply.Proof. absolute summability \\(\\{\\theta_{}\\}\\) fact \\(\\mathbb{E}(\\varepsilon^2)<\\infty\\) imply order integration summation interchangeable (see Hamilton, 1994, Footnote p. 52), proves (). (ii), see end Appendix 3.Hamilton (1994).","code":"\nlibrary(AEC)\nT <- 100;nb.sim <- 1\ny.0 <- c(0)\nc <- 1;phi <- c(0);sigma <- 1\ntheta <- c(1,1) # MA(1) specification\ny.sim <- sim.arma(c,phi,theta,sigma,T,y.0,nb.sim)\npar(mfrow=c(1,2))\npar(plt=c(.2,.9,.2,.85))\nplot(y.sim[,1],xlab=\"\",ylab=\"\",type=\"l\",lwd=2,\n     main=expression(paste(theta[0],\"=1, \",theta[1],\"=1\",sep=\"\")))\nabline(h=c)\ntheta <- c(1,1,1,1,1) # MA(4) specification\ny.sim <- sim.arma(c,phi,theta,sigma,T,y.0,nb.sim)\nplot(y.sim[,1],xlab=\"\",ylab=\"\",type=\"l\",lwd=2,\n     main=expression(paste(theta[0],\"=...=\",theta[4],\"=1\",sep=\"\")))\nabline(h=c)"},{"path":"Univariate.html","id":"ARsection","chapter":"2 Univariate processes","heading":"2.2 Auto-Regressive (AR) processes","text":"Definition 2.4  (First-order AR process (AR(1))) Process \\(y_t\\) AR(1) process dynamics defined following difference equation:\n\\[\ny_t = c + \\phi y_{t-1} + \\varepsilon_t,\n\\]\n\\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) white noise process (see Def. 1.1).\\(|\\phi|\\ge1\\), \\(y_t\\) stationary. Indeed, :\n\\[\\begin{eqnarray*}\ny_{t+k} &=& c + \\varepsilon_{t+k} + \\phi  ( c + \\varepsilon_{t+k-1})+ \\phi^2  ( c + \\varepsilon_{t+k-2})+ \\dots + \\\\\n&& \\phi^{k-1}  ( c + \\varepsilon_{t+1}) + \\phi^k y_t.\n\\end{eqnarray*}\\]\nTherefore, conditional variance\n\\[\n\\mathbb{V}ar_t(y_{t+k}) = \\sigma^2(1 + \\phi^2 + \\phi^4 + \\dots + \\phi^{2(k-1)})\n\\]\n(\\(\\sigma^2\\) variance \\(\\varepsilon_t\\)) converge \\(k\\) gets infinitely large. implies \\(\\mathbb{V}ar(y_{t})\\) exist.2 contrast, \\(|\\phi| < 1\\), :\n\\[\ny_t = c + \\varepsilon_t + \\phi  ( c + \\varepsilon_{t-1})+ \\phi^2  ( c + \\varepsilon_{t-2})+ \\dots + \\phi^k  ( c + \\varepsilon_{t-k}) + \\dots\n\\]\nHence, \\(|\\phi| < 1\\), unconditional mean variance \\(y_t\\) :\n\\[\n\\mathbb{E}(y_t) = \\frac{c}{1-\\phi} =: \\mu \\quad \\mbox{} \\quad \\mathbb{V}ar(y_t) = \\frac{\\sigma^2}{1-\\phi^2}.\n\\]Let us compute \\(j^{th}\\) autocovariance AR(1) process:\n\\[\\begin{eqnarray*}\n\\mathbb{E}([y_{t} - \\mu][y_{t-j} - \\mu]) &=& \\mathbb{E}([\\varepsilon_t + \\phi  \\varepsilon_{t-1}+ \\phi^2 \\varepsilon_{t-2} + \\dots + \\color{red}{\\phi^j \\varepsilon_{t-j}} + \\color{blue}{\\phi^{j+1} \\varepsilon_{t-j-1}} \\dots]\\times \\\\\n&&[\\color{red}{\\varepsilon_{t-j}} + \\color{blue}{\\phi \\varepsilon_{t-j-1}} + \\phi^2 \\varepsilon_{t-j-2} + \\dots + \\phi^k \\varepsilon_{t-j-k} + \\dots])\\\\\n&=& \\mathbb{E}(\\color{red}{\\phi^j \\varepsilon_{t-j}^2}+\\color{blue}{\\phi^{j+2} \\varepsilon_{t-j-1}^2}+\\phi^{j+4} \\varepsilon_{t-j-2}^2+\\dots)\\\\\n&=& \\frac{\\phi^j \\sigma^2}{1 - \\phi^2}.\n\\end{eqnarray*}\\]Therefore, auto-correlation given \\(\\rho_j = \\phi^j\\).precedes, :Proposition 2.3  (Covariance-stationarity AR(1) process) AR(1) process, defined Def. 2.4, covariance-stationary iff \\(|\\phi|<1\\).Definition 2.5  (AR(p) process) Process \\(y_t\\) \\(p^{th}\\)-order autoregressive process (AR(p)) dynamics defined following difference equation (\\(\\phi_p \\ne 0\\)):\n\\[\\begin{equation}\ny_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\varepsilon_t,\\tag{2.3}\n\\end{equation}\\]\n\\(\\{\\varepsilon_t\\}_{t = -\\infty}^{+\\infty}\\) white noise process (see Def. 1.1).see, covariance-stationarity process \\(y_t\\) hinges eigenvalues matrix \\(F\\), defined :\n\\[\\begin{equation}\nF = \\left[\n\\begin{array}{ccccc}\n\\phi_1 & \\phi_2 & \\dots& & \\phi_p \\\\\n1 & 0 &\\dots && 0 \\\\\n0 & 1 &\\dots && 0 \\\\\n\\vdots &  & \\ddots && \\vdots \\\\\n0 & 0 &\\dots &1& 0 \\\\\n\\end{array}\n\\right].\\tag{2.4}\n\\end{equation}\\]Note matrix \\(F\\) \\(y_t\\) follows Eq. (2.3), process \\(\\mathbf{y}_t\\) follows:\n\\[\n\\mathbf{y}_t = \\mathbf{c} + F \\mathbf{y}_{t-1} + \\boldsymbol\\xi_t\n\\]\n\n\\[\n\\mathbf{c} =\n\\left[\\begin{array}{c}\nc\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{array}\\right],\n\\quad\n\\boldsymbol\\xi_t =\n\\left[\\begin{array}{c}\n\\varepsilon_t\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{array}\\right],\n\\quad\n\\mathbf{y}_t =\n\\left[\\begin{array}{c}\ny_t\\\\\ny_{t-1}\\\\\n\\vdots\\\\\ny_{t-p+1}\n\\end{array}\\right].\n\\]Proposition 2.4  (eigenvalues matrix F) eigenvalues \\(F\\) (defined Eq. (2.4)) solutions :\n\\[\\begin{equation}\n\\lambda^p - \\phi_1 \\lambda^{p-1} - \\dots - \\phi_{p-1}\\lambda - \\phi_p = 0.\\tag{2.5}\n\\end{equation}\\]Proposition 2.5  (Covariance-stationarity AR(p) process) four statements equivalent:Process \\(\\{y_t\\}\\), defined Def. 2.5, covariance-stationary.eigenvalues \\(F\\) (defined Eq. (2.4)) lie strictly within unit circle.roots Eq. (2.6) () lie strictly outside unit circle.\n\\[\\begin{equation}\n1 - \\phi_1 z - \\dots - \\phi_{p-1}z^{p-1} - \\phi_p z^p = 0.\\tag{2.6}\n\\end{equation}\\]roots Eq. (2.7) () lie strictly inside unit circle.\n\\[\\begin{equation}\n\\lambda^p - \\phi_1 \\lambda^{p-1} - \\dots - \\phi_{p-1}\\lambda - \\phi_p = 0.\\tag{2.7}\n\\end{equation}\\]Proof. consider case eigenvalues \\(F\\) distinct.3 eigenvalues \\(F\\) distinct, \\(F\\) admits following spectral decomposition: \\(F = PDP^{-1}\\), \\(D\\) diagonal. Using notations introduced Eq. (2.4), :\n\\[\n\\mathbf{y}_{t} = \\mathbf{c} + F \\mathbf{y}_{t-1} + \\boldsymbol\\xi_{t}.\n\\]\nLet’s introduce \\(\\mathbf{d} = P^{-1}\\mathbf{c}\\), \\(\\mathbf{z}_t = P^{-1}\\mathbf{y}_t\\) \\(\\boldsymbol\\eta_t = P^{-1}\\boldsymbol\\xi_t\\). :\n\\[\n\\mathbf{z}_{t} = \\mathbf{d} + D \\mathbf{z}_{t-1} + \\boldsymbol\\eta_{t}.\n\\]\n\\(D\\) diagonal, different component \\(\\mathbf{z}_t\\), denoted \\(z_{,t}\\), follow AR(1) processes. (scalar) autoregressive parameters AR(1) processes diagonal entries \\(D\\)—also eigenvalues \\(F\\)—denote \\(\\lambda_i\\).Process \\(y_t\\) covariance-stationary iff \\(\\mathbf{y}_{t}\\) also covariance-stationary, case iff \\(z_{,t}\\), \\(\\\\{1,\\dots,p\\}\\), covariance-stationary. Prop. 2.3, process \\(z_{,t}\\) covariance-stationary iff \\(|\\lambda_i|<1\\). proves () equivalent (ii). Prop. 2.4 proves (ii) equivalent (iv). Finally, easily seen (iii) equivalent (iv) (long \\(\\phi_p \\ne 0\\)).Using lag operator (see Eq (1.1)), \\(y_t\\) covariance-stationary AR(p) process (Def. 2.5), can write:\n\\[\ny_t = \\mu + \\psi(L)\\varepsilon_t,\n\\]\n\n\\[\\begin{equation}\n\\psi(L) = (1 - \\phi_1 L - \\dots - \\phi_p L^p)^{-1},\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\mu = \\mathbb{E}(y_t) = \\dfrac{c}{1-\\phi_1 -\\dots - \\phi_p}.\\tag{2.8}\n\\end{equation}\\]following lines codes, compute eigenvalues \\(F\\) matrices associated following processes (\\(\\varepsilon_t\\) white noise):\n\\[\\begin{eqnarray*}\nx_t &=& 0.9 x_{t-1} -0.2 x_{t-2} + \\varepsilon_t\\\\\ny_t &=& 1.1 y_{t-1} -0.3 y_{t-2} + \\varepsilon_t\\\\\nw_t &=& 1.4 w_{t-1} -0.7 w_{t-2} + \\varepsilon_t\\\\\nz_t &=& 0.9 z_{t-1} +0.2 z_{t-2} + \\varepsilon_t.\n\\end{eqnarray*}\\]absolute values eigenvalues associated process \\(w_t\\) equal 0.837. Therefore, according Proposition 2.5, processes \\(x_t\\), \\(y_t\\), \\(w_t\\) covariance-stationary, \\(z_t\\) (, latter process, absolute value one eigenvalues matrix \\(F\\) larger 1).computation autocovariances \\(y_t\\) based -called Yule-Walker equations (Eq. (2.9)). Let’s rewrite Eq. (2.3):\n\\[\n(y_t-\\mu) = \\phi_1 (y_{t-1}-\\mu) + \\phi_2 (y_{t-2}-\\mu) + \\dots + \\phi_p (y_{t-p}-\\mu) + \\varepsilon_t.\n\\]\nMultiplying sides \\(y_{t-j}-\\mu\\) taking expectations leads (Yule-Walker) equations:\n\\[\\begin{equation}\n\\gamma_j = \\left\\{\n\\begin{array}{l}\n\\phi_1 \\gamma_{j-1}+\\phi_2 \\gamma_{j-2}+ \\dots + \\phi_p \\gamma_{j-p} \\quad \\quad j>0\\\\\n\\phi_1 \\gamma_{1}+\\phi_2 \\gamma_{2}+ \\dots + \\phi_p \\gamma_{p} + \\sigma^2 \\quad \\quad j=0,\n\\end{array}\n\\right.\\tag{2.9}\n\\end{equation}\\]\n\\(\\sigma^2\\) variance \\(\\varepsilon_t\\).Using \\(\\gamma_j = \\gamma_{-j}\\) (Prop. 1.1), one can express \\((\\gamma_0,\\gamma_1,\\dots,\\gamma_{p})\\) functions \\((\\sigma^2,\\phi_1,\\dots,\\phi_p)\\). Indeed, :\n\\[\n\\left[\\begin{array}{c}\n\\gamma_0 \\\\\n\\gamma_1 \\\\\n\\gamma_2 \\\\\n\\vdots\\\\\n\\gamma_p\n\\end{array}\\right] =\n\\underbrace{\\left[\\begin{array}{cccccccc}\n0 & \\phi_1 & \\phi_2 & \\dots &&& \\phi_p \\\\\n\\phi_1 & \\phi_2 & \\dots &&& \\phi_p & 0 \\\\\n\\phi_2 & (\\phi_1 + \\phi_3) & \\phi_4 & \\dots & \\phi_p& 0& 0 \\\\\n\\vdots\\\\\n\\phi_p & \\phi_{p-1} & \\dots &&\\phi_2& \\phi_1 & 0\n\\end{array}\\right]}_{=H}\\left[\\begin{array}{c}\n\\gamma_0 \\\\\n\\gamma_1 \\\\\n\\gamma_2 \\\\\n\\vdots\\\\\n\\gamma_p\n\\end{array}\\right] +\n\\left[\\begin{array}{c}\n\\sigma^2 \\\\\n0 \\\\\n0 \\\\\n\\vdots\\\\\n0\n\\end{array}\\right],\n\\]easily solved inversing matrix \\(Id - H\\).","code":"\nF <- matrix(c(.9,1,-.2,0),2,2)\nlambda_x <- eigen(F)$values\nF[1,] <- c(1.1,-.3)\nlambda_y <- eigen(F)$values\nF[1,] <- c(1.4,-.7)\nlambda_w <- eigen(F)$values\nF[1,] <- c(.9,.2)\nlambda_z <- eigen(F)$values\nrbind(lambda_x,lambda_y,lambda_w,lambda_z)##                         [,1]                  [,2]\n## lambda_x 0.500000+0.0000000i  0.4000000+0.0000000i\n## lambda_y 0.600000+0.0000000i  0.5000000+0.0000000i\n## lambda_w 0.700000+0.4582576i  0.7000000-0.4582576i\n## lambda_z 1.084429+0.0000000i -0.1844289+0.0000000i"},{"path":"Univariate.html","id":"ar-ma-processes","chapter":"2 Univariate processes","heading":"2.3 AR-MA processes","text":"Definition 2.6  (ARMA(p,q) process) \\(\\{y_t\\}\\) ARMA(\\(p\\),\\(q\\)) process dynamics described following equation:\n\\[\\begin{equation}\ny_t = c + \\underbrace{\\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p}}_{\\mbox{AR part}} + \\underbrace{\\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q}}_{\\mbox{MA part}},\\tag{2.10}\n\\end{equation}\\]\n\\(\\{\\varepsilon_t\\}_{t \\] -\\infty,+\\infty[}\\), variance \\(\\sigma^2\\) (say), white noise process (see Def. 1.1).Proposition 2.6  (Stationarity ARMA(p,q) process) ARMA(\\(p\\),\\(q\\)) process defined 2.6 covariance stationary iff roots \n\\[\n1 - \\phi_1 z - \\dots - \\phi_p z^p=0\n\\]\nlie strictly outside unit circle , equivalently, iff \n\\[\n\\lambda^p - \\phi_1 \\lambda^{p-1} - \\dots - \\phi_p=0\n\\]\nlie strictly within unit circle.Proof. proof Prop. 2.5 can adapted present case.can write:\n\\[\n(1 - \\phi_1 L - \\dots - \\phi_p L^p)y_t = c + (1 + \\theta_1 L + \\dots + \\theta_q L^q)\\varepsilon_t.\n\\]roots \\(1 - \\phi_1 z - \\dots - \\phi_p z^p=0\\) lie outside unit circle, :\n\\[\\begin{equation}\ny_t = \\mu + \\psi(L)\\varepsilon_t,\\tag{2.11}\n\\end{equation}\\]\n\n\\[\n\\psi(L) = \\frac{1 + \\theta_1 L + \\dots + \\theta_q L^q}{1 - \\phi_1 L - \\dots - \\phi_p L^p} \\quad \\quad \\mu = \\dfrac{c}{1-\\phi_1 -\\dots - \\phi_p}.\n\\]Eq. (2.11) Wold representation ARMA process (see Theorem 2.2 ). Section 2.6 (precisely Proposition 2.7), see obtain first \\(h\\) terms infinite polynomial \\(\\Psi(L)\\).Importantly, note stationarity process depends AR specification (eigenvalues matrix \\(F\\), exactly Prop. 2.5). ARMA process stationary, weights \\(\\psi(L)\\) decay geometric rate.","code":""},{"path":"Univariate.html","id":"PACFapproach","chapter":"2 Univariate processes","heading":"2.4 PACF approach to identify AR/MA processes","text":"seen \\(k^{th}\\)-order auto-correlation MA(q) process null \\(k>q\\). exploited, practice, determine order MA process. Moreover, since case AR process, can used distinguish AR MA process.exists equivalent condition satisfied AR processes, can used determine whether process can modeled process. condition relates partial auto-correlations:Definition 2.7  (Partial auto-correlation) partial auto-correlation (\\(\\phi_{h,h}\\)) process \\(\\{y_t\\}\\) defined partial correlation \\(y_{t+h}\\) \\(y_t\\) given \\(y_{t+h-1},\\dots,y_{t+1}\\). (see Def. 8.5 definition partial correlation.)\\(h>p\\), regression \\(y_{t+h}\\) \\(y_{t+h-1},\\dots,y_{t+1}\\) :\n\\[\ny_{t+h} = c + \\phi_1 y_{t+h-1}+\\dots+ \\phi_p  y_{t+h-p} + \\varepsilon_{t+h}.\n\\]\nresiduals latter regressions (\\(\\varepsilon_{t+h}\\)) uncorrelated \\(y_t\\). partial autocorrelation zero \\(h>p\\).Besides, can shown \\(\\phi_{p,p}=\\phi_p\\). Hence \\(\\phi_{p,p}=\\phi_p\\) \\(\\phi_{h,h}=0\\) \\(h>p\\). can used determine order AR process. contrast (importantly) \\(y_t\\) follows MA(q) process, \\(\\phi_{k,k}\\) asymptotically approaches zero instead cutting abruptly.illustrated , functions acf pacf R allow conveniently implement (P)ACF approach. (lines codes, note also use function sim.arma simulate ARMA processes.)\nFigure 2.2: ACF/PACF analysis two processes (MA process left, AR right). correlations computed samples length 1000.\n","code":"\nlibrary(AEC)\npar(mfrow=c(3,2))\npar(plt=c(.2,.9,.2,.95))\ntheta <- c(1,2,1);phi=0\ny.sim <- sim.arma(c=0,phi,theta,sigma=1,T=1000,y.0=0,nb.sim=1)\npar(mfg=c(1,1));plot(y.sim,type=\"l\",lwd=2)\npar(mfg=c(2,1));acf(y.sim)\npar(mfg=c(3,1));pacf(y.sim)\ntheta <- c(1);phi=0.9\ny.sim <- sim.arma(c=0,phi,theta,sigma=1,T=1000,y.0=0,nb.sim=1)\npar(mfg=c(1,2));plot(y.sim,type=\"l\",lwd=2)\npar(mfg=c(2,2));acf(y.sim)\npar(mfg=c(3,2));pacf(y.sim)"},{"path":"Univariate.html","id":"wold-decomposition","chapter":"2 Univariate processes","heading":"2.5 Wold decomposition","text":"Wold decomposition important result time series analysis:Theorem 2.2  (Wold decomposition) covariance-stationary process admits following representation:\n\\[\ny_t = \\mu + \\sum_{0}^{+\\infty} \\theta_i \\varepsilon_{t-} + \\kappa_t,\n\\]\n\\(\\theta_0 = 1\\), \\(\\sum_{=0}^{\\infty} \\theta_i^2 < +\\infty\\) (square summability, see Def. 2.3).\\(\\{\\varepsilon_t\\}\\) white noise (see Def. 1.1); \\(\\varepsilon_t\\) error made forecasting \\(y_t\\) based linear combination lagged \\(y_t\\)’s (\\(\\varepsilon_t = y_t - \\hat{\\mathbb{E}}[y_t|y_{t-1},y_{t-2},\\dots]\\)).\\(j \\ge 1\\), \\(\\kappa_t\\) correlated \\(\\varepsilon_{t-j}\\); \\(\\kappa_t\\) can perfectly forecasted based linear combination lagged \\(y_t\\)’s (.e. \\(\\kappa_t = \\hat{\\mathbb{E}}(\\kappa_t|y_{t-1},y_{t-2},\\dots)\\)). \\(\\kappa_t\\) called deterministic component \\(y_t\\).Proof. See Anderson (1971). Partial proof L. Christiano’s lecture notes.ARMA process, Wold representation given Eq. (2.11). detailed Prop. 2.7, can computed recursively replacing lagged \\(y_t\\)’s Eq. (2.10). case, deterministic component (\\(\\kappa\\)) null.","code":""},{"path":"Univariate.html","id":"IRFARMA","chapter":"2 Univariate processes","heading":"2.6 Impulse Response Functions (IRFs) in ARMA models","text":"Consider ARMA(p,q) process defined Def. 2.6. Let us construct novel (counterfactual) sequence shocks \\(\\{\\tilde\\varepsilon_t^{(s)}\\}\\):\n\\[\n\\tilde\\varepsilon_t^{(s)} = \\left\\{\n\\begin{array}{lcc}\n\\varepsilon_{t} & & t \\ne s,\\\\\n\\varepsilon_{t} + 1 && t=s.\n\\end{array}\n\\right.\n\\]\nHence, difference processes \\(\\{\\varepsilon_t^{(s)}\\}\\) \\(\\{\\tilde\\varepsilon_t^{(s)}\\}\\) pertains date \\(s\\), \\(\\varepsilon_s\\) replaced \\(\\varepsilon_s + 1\\) \\(\\{\\tilde\\varepsilon_t^{(s)}\\}\\).denote \\(\\{\\tilde{y}_t^{(s)}\\}\\) process following Eq. (2.10) \\(\\{\\varepsilon_t\\}\\) replaced \\(\\{\\tilde\\varepsilon_t^{(s)}\\}\\). time series \\(\\{\\tilde{y}_t^{(s)}\\}\\) counterfactual series \\(\\{y_t\\}\\) prevailed \\(\\varepsilon_t\\) shifted one unit date \\(s\\) (change).relationship \\(\\{y_t\\}\\) \\(\\{\\tilde{y}_t^{(s)}\\}\\) defines dynamic multipliers \\(\\{y_t\\}\\). dynamic multiplier \\(\\frac{\\partial y_t}{\\partial \\varepsilon_{s}}\\) corresponds impact \\(y_t\\) unit increase \\(\\varepsilon_s\\) (date \\(s\\)). Using notation introduced \\(\\tilde{y}_t^{(s)}\\), :\n\\[\n\\tilde{y}_t^{(s)} = y_t + \\frac{\\partial y_t}{\\partial \\varepsilon_{s}}.\n\\]\nLet us show dynamic multipliers closely related infinite MA representation (Wold decomposition, Theorem 2.2) \\(y_t\\):\n\\[\ny_t = \\mu + \\sum_{=0}^{+\\infty} \\psi_i \\varepsilon_{t-}.\n\\]\n\\(t<s\\), \\(y_t = \\tilde{y}_t^{(s)}\\) (\\(\\tilde{\\varepsilon}_{t-}= \\varepsilon_{t-}\\) \\(\\ge 0\\) \\(t<s\\)).\\(t \\ge s\\):\n\\[\n\\tilde{y}_t^{(s)} = \\mu + \\left( \\sum_{=0}^{t-s-1} \\psi_i \\varepsilon_{t-} \\right) + \\psi_{t-s}(\\varepsilon_{s}+1) + \\left( \\sum_{=t-s+1}^{+\\infty} \\psi_i \\varepsilon_{t-} \\right)=y_t + \\frac{\\partial y_t}{\\partial \\varepsilon_{s}}.\n\\]\nTherefore, comes difference \\(\\tilde{y}_t^{(s)}\\) \\(y_t\\) \\(\\psi_{t-s}\\). result, \\(t \\ge s\\), :\n\\[\n\\boxed{\\dfrac{\\partial y_t}{\\partial \\varepsilon_{s}}=\\psi_{t-s}.}\n\\]\n, \\(\\{y_t\\}\\)’s dynamic multiplier order \\(k\\) object \\(k^{th}\\) loading \\(\\psi_k\\) Wold decomposition \\(\\{y_t\\}\\). sequence \\(\\left\\{\\dfrac{\\partial y_{t+h}}{\\partial \\varepsilon_{t}}\\right\\}_{h \\ge 0} \\equiv \\left\\{\\psi_h\\right\\}_{h \\ge 0}\\) defines impulse response function (IRF) \\(y_t\\) shock \\(\\varepsilon_t\\).ARMA processes, one can compute IRFs (Wold decomposition) using simple recursive algorithm:Proposition 2.7  (IRF ARMA(p,q) process) coefficients \\(\\psi_h\\), define IRF process \\(y_t\\) \\(\\varepsilon_t\\), can computed recursively follows:Set \\(\\psi_{-1}=\\dots=\\psi_{-p}=0\\).\\(h \\ge 0\\), (recursively) apply:\n\\[\n\\psi_h = \\phi_1 \\psi_{h-1} + \\dots + \\phi_p \\psi_{h-p} + \\theta_h,\n\\]\n\\(\\theta_h = 0\\) \\(h>q\\).Proof. obtained applying operator \\(\\frac{\\partial}{\\partial \\varepsilon_{t}}\\) sides Eq. (2.10):\n\\[\ny_{t+h} = c + \\phi_1 y_{t+h-1} + \\dots + \\phi_p y_{t+h-p} + \\varepsilon_{t+h} + \\theta_1 \\varepsilon_{t+h-1} + \\dots + \\theta_q \\varepsilon_{t+h-q}.\n\\]Note Proposition 2.7 constitutes simple way compute MA(\\(\\infty\\)) representation (Wold representation) ARMA process.One can use function sim.arma package AEC compute ARMA’s IRFs (argument make.IRF = 1):\nFigure 2.3: IRFs associated three processes. Process 1 (MA(2)): \\(y_t = \\varepsilon_t + \\varepsilon_{t-1} + \\varepsilon_{t-2}\\). Process 2 (ARMA(1,1)): \\(y_{t}=0.6y_{t-1} + \\varepsilon_t + 0.5\\varepsilon_{t-1}\\). Process 3 (ARMA(4,2)): \\(y_{t}=0.5y_{t-3} + 0.4y_{t-4} + \\varepsilon_t + \\varepsilon_{t-1} + \\varepsilon_{t-2}\\).\nConsider annual Swiss GDP growth JST macro-history database. Let us first determine relevant orders AR MA processes using (P)ACF approach.\nFigure 2.4: (P)ACF analysis Swiss GDP growth.\ntwo bottom plots Figure 2.4 suggest either MA(2) AR(1) used model GDP growth rate series. Figure 2.5 shows IRFs based two respective specifications.\nFigure 2.5: Dynamic response Swiss annual growth shock innovation \\(\\varepsilon_t\\) date \\(t=0\\). solid line corresponds AR(1) specification; dashed line corresponds MA(2) specification.\nkind algorithm (Prop. 2.7) can used compute impact increase exogenous variable \\(x_t\\) within ARMAX(p,q,r) model (see next section).","code":"\nT <- 21 # number of periods for IRF\ntheta <- c(1,1,1);phi <- c(0);c <- 0\ny.sim <- sim.arma(c,phi,theta,sigma=1,T,y.0=rep(0,length(phi)),\n                  nb.sim=1,make.IRF = 1)\npar(mfrow=c(1,3));par(plt=c(.25,.95,.2,.85))\nplot(0:(T-1),y.sim[,1],type=\"l\",lwd=2,\n     main=\"(a) Process 1\",xlab=\"Time after shock on epsilon\",\n     ylab=\"Dynamic multiplier (shock on epsilon at t=0)\",col=\"red\")\n\ntheta <- c(1,.5);phi <- c(0.6)\ny.sim <- sim.arma(c,phi,theta,sigma=1,T,y.0=rep(0,length(phi)),\n                  nb.sim=1,make.IRF = 1)\nplot(0:(T-1),y.sim[,1],type=\"l\",lwd=2,\n     main=\"(b) Process 2\",xlab=\"Time after shock on epsilon\",\n     ylab=\"\",col=\"red\")\n\ntheta <- c(1,1,1);phi <- c(0,0,.5,.4)\ny.sim <- sim.arma(c,phi,theta,sigma=1,T,y.0=rep(0,length(phi)),\n                  nb.sim=1,make.IRF = 1)\nplot(0:(T-1),y.sim[,1],type=\"l\",lwd=2,\n     main=\"(c) Process 3\",xlab=\"Time after shock on epsilon\",\n     ylab=\"\",col=\"red\")\nlibrary(AEC)\ndata(JST);data <- subset(JST,iso==\"CHE\")\npar(plt=c(.1,.95,.1,.95))\nT <- dim(data)[1]\ngrowth <- log(data$gdp[2:T]/data$gdp[1:(T-1)])\npar(mfrow=c(3,1));par(plt=c(.1,.95,.15,.95))\nplot(data$year[2:T],growth,type=\"l\",xlab=\"\",ylab=\"\",lwd=2)\nabline(h=0,lty=2)\nacf(growth);pacf(growth)\n# Fit an AR process:\nres <- arima(growth,order=c(1,0,0))\nphi <- res$coef[1]\nT <- 11\ny.sim <- sim.arma(c=0,phi,theta=1,sigma=1,T,y.0=rep(0,length(phi)),\n                  nb.sim=1,make.IRF = 1)\npar(plt=c(.15,.95,.25,.95))\nplot(0:(T-1),y.sim[,1],type=\"l\",lwd=3,\n     xlab=\"Time after shock on epsilon\",\n     ylab=\"Dynamic multiplier (shock on epsilon at t=0)\",col=\"red\")\n# Fit a MA process:\nres <- arima(growth,order=c(0,0,2))\nphi <- 0;theta <- c(1,res$coef[1:2])\ny.sim <- sim.arma(c=0,phi,theta,sigma=1,T,y.0=rep(0,length(phi)),\n                  nb.sim=1,make.IRF = 1)\nlines(0:(T-1),y.sim[,1],lwd=3,col=\"red\",lty=2)\nabline(h=0)"},{"path":"Univariate.html","id":"ARMAIRF","chapter":"2 Univariate processes","heading":"2.7 ARMA processes with exogenous variables (ARMA-X)","text":"ARMA processes allow investigate influence exogenous variable (say \\(x_t\\)) variable interest (say \\(y_t\\)). \\(x_t\\) \\(y_t\\) reciprocal influences, Vector Autoregressive (VAR) model may used (tools studied later, Section 3). However, one suspects \\(x_t\\) “exogenous” influence \\(y_t\\), simple extension ARMA processes may considered. Loosely speaking, \\(x_t\\) “exogenous” influence \\(y_t\\) \\(y_t\\) affect \\(x_t\\). extension referred ARMA-X.begin , let us formalize notion exogeneity. Consider white noise sequence \\(\\{\\varepsilon_t\\}\\) (Def. 1.1). white noise enter dynamics \\(y_t\\), alongside \\(x_t\\); \\(x_t\\) exogenous \\(\\varepsilon_t\\). (also say \\(x_t\\) exogenous \\(y_t\\).)Definition 2.8  (Exogeneity) say \\(x_t\\) (strictly) exogenous \\(\\{\\varepsilon_t\\}\\) \n\\[\n\\mathbb{E}(\\varepsilon_t|\\underbrace{\\dots,x_{t+1}}_{\\mbox{future}},\\underbrace{x_t,x_{t-1},\\dots}_{\\mbox{present past}}) = 0.\n\\]Hence, \\(\\{x_t\\}\\) strictly exogenous \\(\\varepsilon_t\\), past, present future values \\(x_t\\) allow predict \\(\\varepsilon_t\\)’s.following, assume \\(\\{x_t\\}\\) covariance stationary process.Definition 2.9  (ARMAX(p,q,r) model) process \\(\\{y_t\\}\\) ARMAX(p,q,r) follows difference equation form:\n\\[\\begin{eqnarray}\ny_t &=& \\underbrace{c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p}}_{\\mbox{AR(p) part}} + \\underbrace{\\beta_0 x_t + \\dots + \\beta_{r} x_{t-r}}_{\\mbox{X(r) part}} + \\nonumber \\\\\n&&\\underbrace{\\varepsilon_t + \\theta_1\\varepsilon_{t-1}+\\dots +\\theta_{q}\\varepsilon_{t-q},}_{\\mbox{MA(q) part}} \\tag{2.12}\n\\end{eqnarray}\\]\n\\(\\{\\varepsilon_t\\}\\) ..d. white noise sequence \\(\\{x_t\\}\\) exogenous \\(y_t\\).effect one-unit increase \\(x_t\\) \\(y_t\\)? address question, notion “effect” formalized. Let us introduce two related sequences values \\(\\{x\\}\\). Denote first \\(\\{\\}\\) second \\(\\{\\tilde{}^t\\}\\). , posit \\(a_s = \\tilde{}_s^t\\) \\(s \\ne t\\), \\(\\tilde{}_t^t = a_t+1\\).notations, define \\(\\frac{\\partial y_{t+h}}{\\partial x_t}\\) follows:\n\\[\\begin{equation}\n\\frac{\\partial y_{t+h}}{\\partial x_t} := \\mathbb{E}_{t-1}(y_{t+h}|\\{x\\} = \\{\\tilde{}^t\\}) - \\mathbb{E}_{t-1}(y_{t+h}|\\{x\\} = \\{\\}).\\tag{2.13}\n\\end{equation}\\]\nexogeneity assumption, easily seen \n\n\n\n\n\\[\n\\frac{\\partial y_t}{\\partial x_t} = \\beta_0.\n\\]\nNow, since\n\\[\\begin{eqnarray*}\ny_{t+1} &=& c + \\phi_1 y_{t} + \\dots + \\phi_p y_{t+1-p} + \\beta_0 x_{t+1} + \\dots + \\beta_{r} x_{t+1-r} +\\\\\n&&\\varepsilon_{t+1} + \\theta_1\\varepsilon_{t}+\\dots +\\theta_{q}\\varepsilon_{t+1-q},\n\\end{eqnarray*}\\]\nusing exogeneity assumption, obtain:\n\\[\n\\frac{\\partial y_{t+1}}{\\partial x_t} := \\phi_1 \\frac{\\partial y_{t}}{\\partial x_t} + \\beta_1 = \\phi_1\\beta_0 + \\beta_1.\n\\]\ncan applied recursively give \\(\\dfrac{\\partial y_{t+h}}{\\partial x_t}\\) \\(h \\ge 0\\):Proposition 2.8  (Dynamic multipliers ARMAX models) One can recursively compute dynamic multipliers \\(\\frac{\\partial y_{t+h}}{\\partial x_t}\\) follows:Initialization: \\(\\dfrac{\\partial y_{t+h}}{\\partial x_t}=0\\) \\(h<0\\).\\(h \\ge 0\\) assuming first \\(h-1\\) multipliers computed, :\n\\[\\begin{eqnarray}\n\\dfrac{\\partial y_{t+h}}{\\partial x_t} &=& \\phi_1 \\dfrac{\\partial y_{t+h-1}}{\\partial x_t} + \\dots + \\phi_p \\dfrac{\\partial y_{t+h-p}}{\\partial x_t} + \\beta_h,\\tag{2.13}\n\\end{eqnarray}\\]\nuse notation \\(\\beta_h=0\\) \\(h>r\\).Remark resulting dynamic multipliers obtained ARMA(p,r) model \\(\\theta_i\\)’s replaced \\(\\beta_i\\)’s (see Proposition 2.7 Section 2.7).stressed definition dynamic multipliers (Eq. (2.13)) reflect potential persistency shock occuring date \\(t\\) process \\(\\{x\\}\\) . Going direction necessitate model joint dynamics \\(x_t\\) (instance using VAR model, see Section 3).Example 2.1  (Influence number freezing days price orange juice) example based data used J. Stock Watson (2003) (Chapter 16). objective study influence number freezing days price orange juice. Let us first estimate ARMAX(0,0,12) model:Let us now use function estim.armax, package AECto fit ARMA-X(2,0,1) model:Figure 2.6 shows IRF associated two models.\nFigure 2.6: Response changes orange juice price (percent) number freezing days. solid (respectively dashed) line corresponds ARMAX(0,0,3) (resp. ARMAX(3,0,1)) model. first model estimated OLS (see ), second MLE.\nExample 2.2  (Real effect monetary policy shock) example, make use monetary shocks identified high-frequency data (see Gertler Karadi (2015)). dataset comes Valerie Ramey’s website (see Ramey (2016)).\nFigure 2.7: blue line corresponds monetary-policy shocks identified means Gertler Karadi (2015)’s approach (high-frequency change Euro-dollar futures). black slid line year--year growth rate industrial production.\nFigure 2.8 displays resulting IRF, 95% confidence band. code used produce confidence bands (.e., compute standard deviation dynamic multipliers different horizons) based Delta method (see Appendix 8.5.2).4\nFigure 2.8: Response industrial-production growth monetary-policy shocks. Dashed lines correpsond \\(\\pm\\) 2-standard-deviation bands.\n","code":"\nlibrary(AEC);library(AER)\ndata(\"FrozenJuice\")\nFJ <- as.data.frame(FrozenJuice)\ndate <- time(FrozenJuice)\nprice <- FJ$price/FJ$ppi\nT <- length(price)\nk <- 1\ndprice <- 100*log(price[(k+1):T]/price[1:(T-k)])\nfdd <- FJ$fdd[(k+1):T]\npar(mfrow=c(3,1))\npar(plt=c(.1,.95,.15,.75))\nplot(date,price,type=\"l\",xlab=\"\",ylab=\"\",\n     main=\"(a) Price of orange Juice\")\nplot(date,c(NaN,dprice),type=\"l\",xlab=\"\",ylab=\"\",\n     main=\"(b) Monthly pct Change (y)\")\nplot(date,FJ$fdd,type=\"l\",xlab=\"\",ylab=\"\",\n     main=\"(c) Number of freezing days (x)\")\nnb.lags <- 3\nFDD <- FJ$fdd[(nb.lags+1):T]\nnames.FDD <- NULL\nfor(i in 1:nb.lags){\n  FDD <- cbind(FDD,FJ$fdd[(nb.lags+1-i):(T-i)])\n  names.FDD <- c(names.FDD,paste(\" Lag \",toString(i),sep=\"\"))}\ncolnames(FDD) <- c(\" Lag 0\",names.FDD)\ndprice <- dprice[(length(dprice)-dim(FDD)[1]+1):length(dprice)]\neq <- lm(dprice~FDD)\n# Compute the Newey-West std errors:\nvar.cov.mat <- NeweyWest(eq,lag = 7, prewhite = FALSE)\nrobust_se <- sqrt(diag(var.cov.mat))\n# Stargazer output (with and without Robust SE)\nstargazer::stargazer(eq, eq, type = \"text\",\n                     column.labels=c(\"(no HAC)\",\"(HAC)\"),keep.stat=\"n\",\n                     se = list(NULL,robust_se),no.space = TRUE)## \n## =========================================\n##                  Dependent variable:     \n##              ----------------------------\n##                         dprice           \n##                 (no HAC)        (HAC)    \n##                   (1)            (2)     \n## -----------------------------------------\n## FDD Lag 0       0.467***      0.467***   \n##                 (0.057)        (0.135)   \n## FDD Lag 1       0.140**        0.140*    \n##                 (0.057)        (0.083)   \n## FDD Lag 2        0.055          0.055    \n##                 (0.057)        (0.056)   \n## FDD Lag 3        0.073          0.073    \n##                 (0.057)        (0.047)   \n## Constant       -0.599***      -0.599***  \n##                 (0.204)        (0.213)   \n## -----------------------------------------\n## Observations      609            609     \n## =========================================\n## Note:         *p<0.1; **p<0.05; ***p<0.01\nnb.lags.exog <- 1 # number of lags of exog. variable\nFDD <- FJ$fdd[(nb.lags.exog+1):T]\nfor(i in 1:nb.lags.exog){\n  FDD <- cbind(FDD,FJ$fdd[(nb.lags.exog+1-i):(T-i)])}\ndprice <- 100*log(price[(k+1):T]/price[1:(T-k)])\ndprice <- dprice[(length(dprice)-dim(FDD)[1]+1):length(dprice)]\nres.armax <- estim.armax(Y = dprice,p=3,q=0,X=FDD)## [1] \"==================================================\"\n## [1] \"  ESTIMATING\"\n## [1] \"==================================================\"\n## [1] \"  END OF ESTIMATION\"\n## [1] \"==================================================\"\n## [1] \"\"\n## [1] \"  RESULTS:\"\n## [1] \"  -----------------------\"\n##                 THETA     st.dev   t.ratio\n## c         -0.46556249 0.19554352 -2.380864\n## phi   t-1  0.09788977 0.04025907  2.431496\n## phi   t-2  0.05049849 0.03827488  1.319364\n## phi   t-3  0.07155170 0.03764750  1.900570\n## sigma      4.64917949 0.13300769 34.954215\n## beta  t-0  0.47015552 0.05665344  8.298800\n## beta  t-1  0.10015862 0.05972526  1.676989\n## [1] \"==================================================\"\nnb.periods <- 20\nIRF1 <- sim.arma(c=0,phi=c(0),theta=eq$coefficients[2:(nb.lags+1)],sigma=1,\n                 T=nb.periods,y.0=c(0),nb.sim=1,make.IRF=1)\nIRF2 <- sim.arma(c=0,phi=res.armax$phi,theta=res.armax$beta,sigma=1,\n                 T=nb.periods,y.0=rep(0,length(res.armax$phi)),\n                 nb.sim=1,make.IRF=1)\npar(plt=c(.15,.95,.2,.95))\nplot(IRF1,type=\"l\",lwd=2,col=\"red\",xlab=\"months after shock\",\n     ylab=\"Chge in price (percent)\")\nlines(IRF2,lwd=2,col=\"red\",lty=2)\nabline(h=0,col=\"grey\")\nlibrary(AEC)\nT <- dim(Ramey)[1]\n# Construct growth series:\nRamey$growth <- Ramey$LIP - c(rep(NaN,12),Ramey$LIP[1:(length(Ramey$LIP)-12)])\n# Prepare matrix of exogenous variables:\nvec.lags <- c(9,12,18)\nMatrix.of.Exog <- NULL\nshocks <- Ramey$ED2_TC\nfor(i in 1:length(vec.lags)){Matrix.of.Exog <-\n  cbind(Matrix.of.Exog,c(rep(NaN,vec.lags[i]),shocks[1:(T-vec.lags[i])]))}\n# Look for dates where data are available:\nindic.good.dates <- complete.cases(Matrix.of.Exog)\n# Estimate ARMAX:\np <- 1; q <- 0\nx <- estim.armax(Ramey$growth[indic.good.dates],p,q,\n                 X=Matrix.of.Exog[indic.good.dates,])## [1] \"==================================================\"\n## [1] \"  ESTIMATING\"\n## [1] \"==================================================\"\n## [1] \"  END OF ESTIMATION\"\n## [1] \"==================================================\"\n## [1] \"\"\n## [1] \"  RESULTS:\"\n## [1] \"  -----------------------\"\n##                   THETA       st.dev    t.ratio\n## c         -0.0001716198 0.0005845907 -0.2935726\n## phi   t-1  0.9825608412 0.0120458531 81.5683897\n## sigma      0.0087948724 0.0003211748 27.3834438\n## beta  t-0 -0.0193570616 0.0087331529 -2.2165032\n## beta  t-1 -0.0225707935 0.0086750938 -2.6017925\n## beta  t-2 -0.0070131593 0.0086387440 -0.8118263\n## [1] \"==================================================\"\n# Compute IRF:\nirf <- sim.arma(0,x$phi,x$beta,x$sigma,T=60,y.0=rep(0,length(x$phi)),\n                nb.sim=1,make.IRF=1,X=NaN,beta=NaN)"},{"path":"Univariate.html","id":"estimARMA","chapter":"2 Univariate processes","heading":"2.8 Maximum Likelihood Estimation (MLE) of ARMA processes","text":"Consider general case (time series); assume observe sample \\(\\mathbf{y}=[y_1,\\dots,y_T]'\\). order implement ML techniques, need evaluate joint p.d.f. (“likelihood”) \\(\\mathbf{y}\\), .e., \\(\\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\), \\(\\boldsymbol\\theta\\) vector parameters characterizes dynamics \\(y_t\\). Maximum Likelihood (ML) estimate \\(\\boldsymbol\\theta\\) given :\n\\[\n\\boxed{\\boldsymbol\\theta_{MLE} = \\arg \\max_{\\boldsymbol\\theta} \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})  = \\arg \\max_{\\boldsymbol\\theta} \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}).}\n\\]time series context, process \\(y_t\\) Markovian, exists useful way rewrite likelihood \\(\\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\). Let us first recall definition Markovian process:Definition 2.10  (Markovian process) Process \\(y_t\\) Markovian order one \\(f_{Y_t|Y_{t-1},Y_{t-2},\\dots} = f_{Y_t|Y_{t-1}}\\). generally, Markovian order \\(k\\) \\(f_{Y_t|Y_{t-1},Y_{t-2},\\dots} = f_{Y_t|Y_{t-1},\\dots,Y_{t-k}}\\).Now, remember Bayes’ formula:\n\\[\n\\mathbb{P}(X_2=x,X_1=y) = \\mathbb{P}(X_2=x|X_1=y)\\mathbb{P}(X_1=y).\n\\]\nUsing leads following decomposition likelihood function:\n\\[\\begin{eqnarray*}\nf_{Y_T,\\dots,Y_1}(y_T,\\dots,y_1;\\boldsymbol\\theta) &=&f_{Y_T|Y_{T-1},\\dots,Y_1}(y_T,\\dots,y_1;\\boldsymbol\\theta) \\times \\\\\n&& f_{Y_{T-1},\\dots,Y_1}(y_{T-1},\\dots,y_1;\\boldsymbol\\theta).\n\\end{eqnarray*}\\]\nUsing previous expression recursively, one obtains:\n\\[\\begin{equation}\nf_{Y_T,\\dots,Y_1}(y_T,\\dots,y_1;\\boldsymbol\\theta) = f_{Y_1}(y_1;\\boldsymbol\\theta) \\prod_{t=2}^{T} f_{Y_t|Y_{t-1},\\dots,Y_1}(y_t,\\dots,y_1;\\boldsymbol\\theta).\\tag{2.14}\n\\end{equation}\\]Let us start Gaussian AR(1) process (Markovian order one):\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\varepsilon_t, \\quad \\varepsilon_t \\sim\\,..d.\\, \\mathcal{N}(0,\\sigma^2).\n\\]\n\\(t>1\\):\n\\[\nf_{Y_t|Y_{t-1},\\dots,Y_1}(y_t,\\dots,y_1;\\boldsymbol\\theta) = f_{Y_t|Y_{t-1}}(y_t,y_{t-1};\\boldsymbol\\theta)\n\\]\n\n\\[\nf_{Y_t|Y_{t-1}}(y_t,y_{t-1};\\boldsymbol\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y_t - c - \\phi_1 y_{t-1})^2}{2\\sigma^2}\\right).\n\\]expressions can plugged Eq. (2.14). \\(f_{Y_1}(y_1;\\boldsymbol\\theta)\\)? exist two possibilities:Case 1: use marginal distribution: \\(y_1 \\sim \\mathcal{N}\\left(\\dfrac{c}{1-\\phi_1},\\dfrac{\\sigma^2}{1-\\phi_1^2}\\right)\\).Case 2: \\(y_1\\) considered deterministic. way, means first observation “sacrificed”.Gaussian AR(1) process, :Case 1: (exact) log-likelihood :\n\\[\\begin{eqnarray}\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})  &=& - \\frac{T}{2} \\log(2\\pi) - T\\log(\\sigma) + \\frac{1}{2}\\log(1-\\phi_1^2)\\nonumber \\\\\n&& - \\frac{(y_1 - c/(1-\\phi_1))^2}{2\\sigma^2/(1-\\phi_1^2)} - \\sum_{t=2}^T \\left[\\frac{(y_t - c - \\phi_1 y_{t-1})^2}{2\\sigma^2} \\right].\n\\end{eqnarray}\\]\nMaximum Likelihood Estimator \\(\\boldsymbol\\theta= [c,\\phi_1,\\sigma^2]\\) obtained numerical optimization.Case 1: (exact) log-likelihood :\n\\[\\begin{eqnarray}\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})  &=& - \\frac{T}{2} \\log(2\\pi) - T\\log(\\sigma) + \\frac{1}{2}\\log(1-\\phi_1^2)\\nonumber \\\\\n&& - \\frac{(y_1 - c/(1-\\phi_1))^2}{2\\sigma^2/(1-\\phi_1^2)} - \\sum_{t=2}^T \\left[\\frac{(y_t - c - \\phi_1 y_{t-1})^2}{2\\sigma^2} \\right].\n\\end{eqnarray}\\]\nMaximum Likelihood Estimator \\(\\boldsymbol\\theta= [c,\\phi_1,\\sigma^2]\\) obtained numerical optimization.Case 2: (conditional) log-likelihood :\n\\[\\begin{eqnarray}\n\\log \\mathcal{L}^*(\\boldsymbol\\theta;\\mathbf{y})  &=& - \\frac{T-1}{2} \\log(2\\pi) - (T-1)\\log(\\sigma)\\nonumber\\\\\n&& - \\sum_{t=2}^T \\left[\\frac{(y_t - c - \\phi_1 y_{t-1})^2}{2\\sigma^2} \\right].\\tag{2.15}\n\\end{eqnarray}\\]Case 2: (conditional) log-likelihood :\n\\[\\begin{eqnarray}\n\\log \\mathcal{L}^*(\\boldsymbol\\theta;\\mathbf{y})  &=& - \\frac{T-1}{2} \\log(2\\pi) - (T-1)\\log(\\sigma)\\nonumber\\\\\n&& - \\sum_{t=2}^T \\left[\\frac{(y_t - c - \\phi_1 y_{t-1})^2}{2\\sigma^2} \\right].\\tag{2.15}\n\\end{eqnarray}\\]Exact MLE conditional MLE asymptotic (.e. large-sample) distribution. Indeed, process stationary, \\(f_{Y_1}(y_1;\\boldsymbol\\theta)\\) makes relatively negligible contribution \\(\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\).conditional MLE substantial advantage: Gaussian case, conditional MLE simply obtained OLS. Indeed, let us introduce notations:\n\\[\nY = \\left[\\begin{array}{c}\ny_2\\\\\n\\vdots\\\\\ny_T\n\\end{array}\\right] \\quad \\quad\nX = \\left[\\begin{array}{cc}\n1 &y_1\\\\\n\\vdots&\\vdots\\\\\n1&y_{T-1}\n\\end{array}\\right].\n\\]\nEq. (2.15) rewrites:\n\\[\\begin{eqnarray}\n\\log \\mathcal{L}^*(\\boldsymbol\\theta;\\mathbf{y})  &=& - \\frac{T-1}{2} \\log(2\\pi) - (T-1)\\log(\\sigma) \\nonumber \\\\\n&& - \\frac{1}{2\\sigma^2} (Y-X[c,\\phi_1]')'(Y-X[c,\\phi_1]'),\n\\end{eqnarray}\\]\nmaximised :\n\\[\\begin{eqnarray}\n[\\hat{c},\\hat\\phi_1]' &=& (X'X)^{-1}X'Y \\tag{2.16} \\\\\n\\hat{\\sigma^2} &=& \\frac{1}{T-1} \\sum_{t=2}^T (y_t - \\hat{c} - \\hat{\\phi_1}y_{t-1})^2 \\nonumber \\\\\n&=& \\frac{1}{T-1} Y'(- X(X'X)^{-1}X')Y. \\tag{2.17}\n\\end{eqnarray}\\]Let us turn case AR(p) process. :\n\\[\\begin{eqnarray*}\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}) &=& \\log f_{Y_p,\\dots,Y_1}(y_p,\\dots,y_1;\\boldsymbol\\theta) +\\\\\n&& \\underbrace{\\sum_{t=p+1}^{T} \\log f_{Y_t|Y_{t-1},\\dots,Y_{t-p}}(y_t,\\dots,y_{t-p};\\boldsymbol\\theta)}_{\\log \\mathcal{L}^*(\\boldsymbol\\theta;\\mathbf{y})}.\n\\end{eqnarray*}\\]\n\\(f_{Y_p,\\dots,Y_{1}}(y_p,\\dots,y_{1};\\boldsymbol\\theta)\\) marginal distribution \\(\\mathbf{y}_{1:p} := [y_p,\\dots,y_1]'\\). marginal distribution \\(\\mathbf{y}_{1:p}\\) Gaussian; therefore fully characterised mean covariance matrix:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\mathbf{y}_{1:p})&=&\\frac{c}{1-\\phi_1-\\dots-\\phi_p} \\mathbf{1}_{p\\times 1} \\\\\n\\mathbb{V}ar(\\mathbf{y}_{1:p}) &=& \\left[\\begin{array}{cccc}\n\\gamma_0 & \\gamma_1 & \\dots & \\gamma_{p-1} \\\\\n\\gamma_1 & \\gamma_0 & \\dots & \\gamma_{p-2} \\\\\n\\vdots &  & \\ddots & \\vdots \\\\\n\\gamma_{p-1} & \\gamma_{p-2} & \\dots & \\gamma_{0} \\\\\n\\end{array}\\right],\n\\end{eqnarray*}\\]\n\\(\\gamma_i\\)’s computed using Yule-Walker equations (Eq. (2.9)). Note depend, non-linear way, model parameters. Hence, maximization exact log-likelihood necessitates numerical oprimization procedures. contrast, maximization conditional log-likelihood \\(\\log \\mathcal{L}^*(\\boldsymbol\\theta;\\mathbf{y})\\) requires OLS, using Eqs. (2.16) (2.17), :\n\\[\nY = \\left[\\begin{array}{c}\ny_{p+1}\\\\\n\\vdots\\\\\ny_T\n\\end{array}\\right] \\quad \\quad\nX = \\left[\\begin{array}{cccc}\n1 & y_p & \\dots & y_1\\\\\n\\vdots&\\vdots&&\\vdots\\\\\n1&y_{T-1}&\\dots&y_{T-p}\n\\end{array}\\right].\n\\], stationary processes, conditional exact MLE asymptotic (large-sample) distribution. small samples, OLS formula however biased. Indeed, consider regression (\\(y_t\\) follows AR(p) process):\n\\[\\begin{equation}\ny_t = \\boldsymbol\\beta'\\mathbf{x}_t + \\varepsilon_t,\\tag{2.18}\n\\end{equation}\\]\n\\(\\mathbf{x}_t = [1,y_{t-1},\\dots,y_{t-p}]'\\) \\(\\boldsymbol\\beta = [c,\\phi_1,\\dots,\\phi_p]'\\).bias results fact \\(\\mathbf{x}_t\\) correlates \\(\\varepsilon_s\\)’s \\(s<t\\). sure:\n\\[\\begin{equation}\n\\mathbf{b} = \\boldsymbol{\\beta} + (X'X)^{-1}X'\\boldsymbol\\varepsilon,\\tag{2.19}\n\\end{equation}\\]\nspecific form \\(X\\), non-zero correlation \\(\\mathbf{x}_t\\) \\(\\varepsilon_s\\) \\(s<t\\), therefore \\(\\mathbb{E}[(X'X)^{-1}X'\\boldsymbol\\varepsilon] \\ne 0\\). , asymptotically, previous expectation goes zero, :Proposition 2.9  (Large-sample porperties OLS estimator AR(p) models) Assume \\(\\{y_t\\}\\) follows AR(p) process:\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + \\varepsilon_t\n\\]\n\\(\\{\\varepsilon_{t}\\}\\) ..d. white noise process. \\(\\mathbf{b}\\) OLS estimator \\(\\boldsymbol\\beta\\) (Eq. (2.18)), :\n\\[\n\\sqrt{T}(\\mathbf{b}-\\boldsymbol{\\beta}) =  \\underbrace{\\left[\\frac{1}{T}\\sum_{t=p}^T \\mathbf{x}_t\\mathbf{x}_t' \\right]^{-1}}_{\\overset{p}{\\rightarrow} \\mathbf{Q}^{-1}}\n\\underbrace{\\sqrt{T} \\left[\\frac{1}{T}\\sum_{t=1}^T \\mathbf{x}_t\\varepsilon_t \\right]}_{\\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2\\mathbf{Q})},\n\\]\n\\(\\mathbf{Q} = \\mbox{plim }\\frac{1}{T}\\sum_{t=p}^T \\mathbf{x}_t\\mathbf{x}_t'= \\mbox{plim }\\frac{1}{T}\\sum_{t=1}^T \\mathbf{x}_t\\mathbf{x}_t'\\) given :\n\\[\\begin{equation}\n\\mathbf{Q} = \\left[\n\\begin{array}{ccccc}\n1 & \\mu &\\mu & \\dots & \\mu \\\\\n\\mu & \\gamma_0 + \\mu^2 & \\gamma_1 + \\mu^2 & \\dots & \\gamma_{p-1} + \\mu^2\\\\\n\\mu & \\gamma_1 + \\mu^2 & \\gamma_0 + \\mu^2 & \\dots & \\gamma_{p-2} + \\mu^2\\\\\n\\vdots &\\vdots &\\vdots &\\dots &\\vdots \\\\\n\\mu & \\gamma_{p-1} + \\mu^2 & \\gamma_{p-2} + \\mu^2 & \\dots & \\gamma_{0} + \\mu^2\n\\end{array}\n\\right].\\tag{2.20}\n\\end{equation}\\]Proof. Rearranging Eq. (2.19), :\n\\[\n\\sqrt{T}(\\mathbf{b}-\\boldsymbol{\\beta}) =  (X'X/T)^{-1}\\sqrt{T}(X'\\boldsymbol\\varepsilon/T).\n\\]\nLet us consider autocovariances \\(\\mathbf{v}_t = \\mathbf{x}_t \\varepsilon_t\\), denoted \\(\\gamma^v_j\\). Using fact \\(\\mathbf{x}_t\\) linear combination past \\(\\varepsilon_t\\)’s \\(\\varepsilon_t\\) white noise, get \\(\\mathbb{E}(\\varepsilon_t\\mathbf{x}_t)=0\\). Therefore\n\\[\n\\gamma^v_j = \\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}\\mathbf{x}_t\\mathbf{x}_{t-j}').\n\\]\n\\(j>0\\), \n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}\\mathbf{x}_t\\mathbf{x}_{t-j}')&=&\\mathbb{E}(\\mathbb{E}[\\varepsilon_t\\varepsilon_{t-j}\\mathbf{x}_t\\mathbf{x}_{t-j}'|\\varepsilon_{t-j},\\mathbf{x}_t,\\mathbf{x}_{t-j}])\\\\\n&=&\\mathbb{E}(\\varepsilon_{t-j}\\mathbf{x}_t\\mathbf{x}_{t-j}'\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},\\mathbf{x}_t,\\mathbf{x}_{t-j}])=0.\n\\end{eqnarray*}\\]\nNote , \\(j>0\\), \\(\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},\\mathbf{x}_t,\\mathbf{x}_{t-j}]=0\\) \\(\\{\\varepsilon_t\\}\\) ..d. white noise sequence. \\(j=0\\), :\n\\[\n\\gamma^v_0 = \\mathbb{E}(\\varepsilon_t^2\\mathbf{x}_t\\mathbf{x}_{t}')= \\mathbb{E}(\\varepsilon_t^2) \\mathbb{E}(\\mathbf{x}_t\\mathbf{x}_{t}')=\\sigma^2\\mathbf{Q}.\n\\]\nconvergence distribution \\(\\sqrt{T}(X'\\boldsymbol\\varepsilon/T)=\\sqrt{T}\\frac{1}{T}\\sum_{t=1}^Tv_t\\) results Theorem 1.1 (applied \\(\\mathbf{v}_t=\\mathbf{x}_t\\varepsilon_t\\)), using \\(\\gamma_j^v\\) computed .two cases (exact conditional log-likelihoods) can implemented asking R fit AR process means function arima. Let us instance use output gap US3var dataset (US quarterly data, covering period 1959:2 2015:1, used Gouriéroux, Monfort, Renne (2017)).two sets estimated coefficients appear close .Let us now turn Moving-Average processes. Start MA(1):\n\\[\ny_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1},\\quad \\varepsilon_t \\sim ..d.\\mathcal{N}(0,\\sigma^2).\n\\]\n\\(\\varepsilon_t\\)’s easily computed recursively, starting \\(\\varepsilon_t = y_t - \\mu - \\theta_1 \\varepsilon_{t-1}\\). obtain:\n\\[\n\\varepsilon_t = y_t - \\theta_1 y_{t-1} + \\theta_1^2 y_{t-2}^2 + \\dots + (-1)^{t-1} \\theta_1^{t-1} y_{1} + (-1)^t\\theta_1^{t}\\varepsilon_{0}.\n\\]\nAssume one wants recover sequence \\(\\{\\varepsilon_t\\}\\)’s based observed values \\(y_t\\) (date 1 date \\(t\\)). One can use previous expression, value used \\(\\varepsilon_0\\)? one use true value \\(\\varepsilon_0\\) 0 (say), one obtain \\(\\varepsilon_t\\), estimate (\\(\\hat\\varepsilon_t\\), say), :\n\\[\n\\hat\\varepsilon_t = \\varepsilon_t - (-1)^t\\theta_1^{t}\\varepsilon_{0}.\n\\]\nClearly, \\(|\\theta_1|<1\\), error becomes small large \\(t\\). Formally, \\(|\\theta_1|<1\\), :\n\\[\n\\hat\\varepsilon_t \\overset{p}{\\rightarrow} \\varepsilon_t.\n\\]\nHence, \\(|\\theta_1|<1\\), consistent estimate conditional log-likelihood given :\n\\[\\begin{equation}\n\\log \\hat{\\mathcal{L}}^*(\\boldsymbol\\theta;\\mathbf{y}) = -\\frac{T}{2}\\log(2\\pi) - \\frac{T}{2}\\log(\\sigma^2) - \\sum_{t=1}^T \\frac{\\hat\\varepsilon_t^2}{2\\sigma^2}.\\tag{2.21}\n\\end{equation}\\]\nLoosely speaking, \\(|\\theta_1|<1\\) \\(T\\) sufficiently large:\n\\[\n\\mbox{approximate conditional MLE $\\approx$ exact MLE.}\n\\]Note \\(\\hat{\\mathcal{L}}^*(\\boldsymbol\\theta;\\mathbf{y})\\) complicated nonlinear function \\(\\mu\\) \\(\\theta\\). maximization therefore based numerical optimization procedures.Let us consider case Gaussian MA(\\(q\\)) process:\n\\[\\begin{equation}\ny_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q} , \\quad \\varepsilon_t \\sim ..d.\\mathcal{N}(0,\\sigma^2). \\tag{2.22}\n\\end{equation}\\]Let us assume process invertible MA process. , assume roots :\n\\[\\begin{equation}\n\\lambda^q + \\theta_1 \\lambda^{q-1} + \\dots + \\theta_{q-1} \\lambda + \\theta_q = 0 \\tag{2.23}\n\\end{equation}\\]\nlie strictly inside unit circle. case, polynomial \\(\\Theta(L)=1 + \\theta_1 L + \\dots + \\theta_q L^q\\) invertible Eq. (2.22) writes:\n\\[\n\\varepsilon_t = \\Theta(L)^{-1}(y_t - \\mu),\n\\]\nimplies , knew past values \\(y_t\\), also know \\(\\varepsilon_t\\). case, can consistently estimate \\(\\varepsilon_t\\)’s recursively computing \\(\\hat\\varepsilon_t\\)’s follows (\\(t>0\\)):\n\\[\\begin{equation}\n\\hat\\varepsilon_t = y_t - \\mu - \\theta_1 \\hat\\varepsilon_{t-1} - \\dots  - \\theta_q \\hat\\varepsilon_{t-q},\\tag{2.24}\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\hat\\varepsilon_{0}=\\dots=\\hat\\varepsilon_{-q+1}=0.\\tag{2.25}\n\\end{equation}\\]context, consistent estimate conditional log-likelihood still given Eq. (2.21), using Eqs. (2.24) (2.25) recursively compute \\(\\hat\\varepsilon_t\\)’s.Note determine exact likelihood MA process. Indeed, vector \\(\\mathbf{y} = [y_1,\\dots,y_T]'\\) Gaussian-distributed vector mean \\(\\boldsymbol\\mu = [\\mu,\\dots,\\mu]'\\) variance:\n\\[\n\\boldsymbol\\Omega = \\left[\\begin{array}{ccccccc}\n\\gamma_0 & \\gamma_1&\\dots&\\gamma_q&{\\color{red}0}&{\\color{red}\\dots}&{\\color{red}0}\\\\\n\\gamma_1 & \\gamma_0&\\gamma_1&&\\ddots&{\\color{red}\\ddots}&{\\color{red}\\vdots}\\\\\n\\vdots & \\gamma_1&\\ddots&\\ddots&&\\ddots&{\\color{red}0}\\\\\n\\gamma_q &&\\ddots&&&&\\gamma_q\\\\\n{\\color{red}0} &&&\\ddots&\\ddots&\\ddots&\\vdots\\\\\n{\\color{red}\\vdots}&{\\color{red}\\ddots}&\\ddots&&\\gamma_1&\\gamma_0&\\gamma_1\\\\\n{\\color{red}0}&{\\color{red}\\dots}&{\\color{red}0}&\\gamma_q&\\dots&\\gamma_1&\\gamma_0\n\\end{array}\\right],\n\\]\n\\(\\gamma_j\\)’s given Eq. (2.2). p.d.f. \\(\\mathbf{y}\\) given (see Prop. 8.10):\n\\[\n(2\\pi)^{-T/2}|\\boldsymbol\\Omega|^{-1/2}\\exp\\left( -\\frac{1}{2} (\\mathbf{y}-\\boldsymbol\\mu)' \\boldsymbol\\Omega^{-1} (\\mathbf{y}-\\boldsymbol\\mu)\\right).\n\\]\nlarge samples, computation likelihood however becomes numerically demanding.Finally, let us consider MLE ARMA(\\(p\\),\\(q\\)) processes:\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} +\n\\dots + \\theta_q \\varepsilon_{t-q} , \\; \\varepsilon_t \\sim ..d.\\,\\mathcal{N}(0,\\sigma^2).\n\\]\nMA part process invertible, log-likelihood function can consistently approximated conditional counterpart (form Eq. (2.21)), using consistent estimates \\(\\hat\\varepsilon_t\\) \\(\\varepsilon_t\\). \\(\\hat\\varepsilon_t\\)’s computed recursively :\n\\[\\begin{equation}\n\\hat\\varepsilon_t = y_t - c - \\phi_1 y_{t-1} - \\dots - \\phi_p y_{t-p} - \\theta_1 \\hat\\varepsilon_{t-1} - \\dots - \\theta_q \\hat\\varepsilon_{t-q},\\tag{2.26}\n\\end{equation}\\]\ngiven initial conditions, instance:\\(\\hat\\varepsilon_0=\\dots=\\hat\\varepsilon_{-q+1}=0\\) \\(y_{0}=\\dots=y_{-p+1}=\\mathbb{E}(y_i)=\\mu\\). (Recursions Eq. (2.26) start \\(t=1\\).)\\(\\hat\\varepsilon_p=\\dots=\\hat\\varepsilon_{p-q+1}=0\\) actual values \\(y_{}\\)’s \\(\\[1,p]\\). case, first \\(p\\) observations \\(y_t\\) used. Recursions Eq. (2.26) start \\(t=p+1\\).","code":"\nlibrary(AEC)\ny <- US3var$y.gdp.gap\nar3.Case1 <- arima(y,order = c(3,0,0),method=\"ML\")\nar3.Case2 <- arima(y,order = c(3,0,0),method=\"CSS\")\nrbind(ar3.Case1$coef,ar3.Case2$coef)##           ar1         ar2        ar3  intercept\n## [1,] 1.191267 -0.08934705 -0.1781163 -0.9226007\n## [2,] 1.192003 -0.08811150 -0.1787662 -1.0341696"},{"path":"Univariate.html","id":"specification-choice","chapter":"2 Univariate processes","heading":"2.9 Specification choice","text":"previous section explains fit given ARMA specification. choose appropriate specification? possibility employ (P)ACF approach (see Figure 2.2). However, previous approach leads either AR MA process (ARMA process). one wants consider various ARMA(p,q) specifications, \\(p \\\\{1,\\dots,P\\}\\) \\(q \\\\{1,\\dots,Q\\}\\), say, one can resort information criteria.general, choosing specification, one faces following dilemma:rich specification may lead “overfitting”/misspecification, implying additional estimation errors (--sample forecasts).simple specification may lead potential omission valuable information (e.g., contained older lags).lag selection approach based -called information criteria consists maximizing fit data, adding penalty “richness” model. precisely, using approach amounts minimizing loss function () negatively depends fitting errors (b) positively depends number parameters model.Definition 2.11  (Information Criteria) Akaike (AIC), Hannan-Quinn (HQ) Schwarz information (BIC) criteria form\n\\[\nc^{()}(k) = \\underbrace{\\frac{- 2 \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k);\\mathbf{y})}{T}}_{\\mbox{decreases w.r.t. $k$}} \\quad +\n\\underbrace{\n\\frac{k\\phi^{()}(T)}{T},}_{\\mbox{increases w.r.t. $k$}}\n\\]\n\\(() \\\\{AIC,HQ,BIC\\}\\) \\(\\hat{\\boldsymbol\\theta}_T(k)\\) denotes ML estimate \\(\\boldsymbol\\theta_0(k)\\), vector parameters length \\(k\\).(#tabcriteria) table shows \\(\\phi\\) functions used different criteria.lag suggested criterion \\(()\\) given :\n\\[\n\\boxed{\\hat{k}^{()} = \\underset{k}{\\mbox{argmin}} \\quad c^{()}(k).}\n\\]case ARMA(p,q) process, \\(k=2+p+q\\).Proposition 2.10  (Consistency criteria-based lag selection) lag selection procedure consistent \n\\[\n\\lim_{T \\rightarrow \\infty} \\phi(T) = \\infty \\quad \\quad \\lim_{T \\rightarrow \\infty} \\phi(T)/T = 0.\n\\]\nnotably case HQ BIC criteria.Proof. true number lags denoted \\(k_0\\). show \\(\\lim_{T \\rightarrow \\infty} \\mathbb{P}(\\hat{k}_T \\ne k_0)=0\\).Case \\(k < k_0\\): model \\(k\\) parameter misspecified, therefore:\n\\[\n\\mbox{plim}_{T \\rightarrow \\infty}  \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k);\\mathbf{y})/T < \\mbox{plim}_{T \\rightarrow \\infty}  \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k_0);\\mathbf{y})/T.\n\\]\nHence, \\(\\lim_{T \\rightarrow \\infty} \\phi(T)/T = 0\\), : \\(\\lim_{T \\rightarrow \\infty} \\mathbb{P}(c(k_0) \\ge c(k)) \\rightarrow 0\\) \n\\[\n\\lim_{T \\rightarrow \\infty} \\mathbb{P}(\\hat{k}<k_0) \\le \\lim_{T \\rightarrow \\infty} \\mathbb{P}\\left\\{c(k_0) \\ge c(k) \\mbox{ $k < k_0$}\\right\\} = 0.\n\\]Case \\(k > k_0\\): null hypothesis, likelihood ratio (LR) test statistic satisfies:\n\\[\n2 \\left(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k);\\mathbf{y})-\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k_0);\\mathbf{y})\\right) \\sim \\chi^2(k-k_0).\n\\]\n\\(\\lim_{T \\rightarrow \\infty} \\phi(T) = \\infty\\), : \\(\\mbox{plim}_{T \\rightarrow \\infty} -2 \\left(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k);\\mathbf{y})-\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k_0);\\mathbf{y})\\right)/\\phi(T) = 0\\). Hence \\(\\mbox{plim}_{T \\rightarrow \\infty} T[c(k_0) - c(k)]/\\phi(T) \\le -1\\) \\(\\lim_{T \\rightarrow \\infty} \\mathbb{P}(c(k_0) \\ge c(k)) \\rightarrow 0\\), implies, spirit , \\(\\lim_{T \\rightarrow \\infty} \\mathbb{P}(\\hat{k}>k_0) = 0\\).Therefore, \\(\\lim_{T \\rightarrow \\infty} \\mathbb{P}(\\hat{k}=k_0) = 1\\).Example 2.3  (Linear regression) Consider linear regression normal disturbances:\n\\[\ny_t = \\mathbf{x}_t' \\boldsymbol\\beta + \\varepsilon_t, \\quad \\varepsilon_t \\sim ..d. \\mathcal{N}(0,\\sigma^2).\n\\]\nassociated log-likelihood form Eq. (2.21). case, :\n\\[\\begin{eqnarray*}\nc^{()}(k) &=& \\frac{- 2 \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_T(k);\\mathbf{y})}{T} + \\frac{k\\phi^{()}(T)}{T}\\\\\n&\\approx& \\log(2\\pi) + \\log(\\widehat{\\sigma^2}) + \\frac{1}{T}\\sum_{t=1}^T \\frac{\\varepsilon_t^2}{\\widehat{\\sigma^2}} + \\frac{k\\phi^{()}(T)}{T}.\n\\end{eqnarray*}\\]\nlarge \\(T\\), consistent estimation scheme, :\n\\[\n\\widehat{\\sigma^2} \\approx \\frac{1}{T}\\sum_{t=1}^T \\varepsilon_t^2 = SSR/T.\n\\]\nHence \\(\\hat{k}^{()} \\approx \\underset{k}{\\mbox{argmin}} \\quad \\log(SSR/T) + \\dfrac{k\\phi^{()}(T)}{T}\\).Example 2.4  (Swiss GDP growth) Consider long historical time series Swiss GDP growth (see Figure 1.4), taken Jordà, Schularick, Taylor (2017) dataset. Let us look best ARMA specification using AIC criteria:best specification therefore AR(1) model. , although AR(2) (say) result better fit data, fit improvement large enough compensate additional AIC cost associated additional parameter.","code":"\nlibrary(AEC);data(JST)\ndata <- subset(JST,iso==\"CHE\")\nT <- dim(data)[1]\ny <- c(NaN,log(data$gdp[2:T]/data$gdp[1:(T-1)]))\n# Use AIC criteria to look for appropriate specif:\nmax.p <- 3;max.q <- 3;\nall.AIC <- NULL\nfor(p in 0:max.p){\n  for(q in 0:max.q){\n    res <- arima(y,order=c(p,0,q))\n    if(res$aic<min(all.AIC)){best.p<-p;best.q<-q}\n    all.AIC <- c(all.AIC,res$aic)}}\nprint(c(best.p,best.q))## [1] 1 0"},{"path":"VAR.html","id":"VAR","chapter":"3 Multivariate models","heading":"3 Multivariate models","text":"section presents Vector Auto-Regressive Moving-Average (SVARMA) models. models widely used macroeconomic analysis. simple easy estimate, make possible conveniently capture dynamics complex multivariate systems. VAR popularity notably due Sims (1980)’s influential work. comprehensive survey proposed J. H. Stock Watson (2016).economics, VAR models often employed order identify structural shocks, independent primitive exogenous forces drive economic variables (Ramey (2016)). often given specific economic meaning (e.g., demand supply shocks). structural shocks identified, researchers use resulting Structural VAR model produce Impulse Response Functions describe dynamic reactions endogenous variables shocks.Working models (VAR VARMA models) often involves two steps: first step, reduced-form version model estimated; second step, structural shocks identified IRFs produced.","code":""},{"path":"VAR.html","id":"definition-of-vars-and-svarma-models","chapter":"3 Multivariate models","heading":"3.1 Definition of VARs (and SVARMA) models","text":"Definition 3.1  ((S)VAR model) Let \\(y_{t}\\) denote \\(n \\times1\\) vector (endogenous) random variables. Process \\(y_{t}\\) follows \\(p^{th}\\)-order (S)VAR , \\(t\\), \n\\[\\begin{eqnarray}\n\\begin{array}{rllll}\nVAR:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\varepsilon_t,\\\\\nSVAR:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + B \\eta_t,\n\\end{array}\\tag{3.1}\n\\end{eqnarray}\\]\n\\(\\varepsilon_t = B\\eta_t\\), \\(\\{\\eta_{t}\\}\\) white noise sequence whose components mutually serially independent.first line Eq. (3.1) corresponds reduced-form VAR model (structural form second line). structural shocks (components \\(\\eta_t\\)) mutually uncorrelated, case innovations, components \\(\\varepsilon_t\\). However, boths cases, vectors \\(\\eta_t\\) \\(\\varepsilon_t\\) serially correlated (time).case univariate models, VARs can extended MA terms \\(\\eta_t\\), giving rise VARMA models:Definition 3.2  ((S)VARMA model) Let \\(y_{t}\\) denote \\(n \\times1\\) vector random variables. Process \\(y_{t}\\) follows VARMA model order (p,q) , \\(t\\), \n\\[\\begin{eqnarray}\n\\begin{array}{rllll}\nVARMA:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\\\\n&&&\\varepsilon_t + \\Theta_1\\varepsilon_{t-1} + \\dots + \\Theta_q \\varepsilon_{t-q},\\\\\nSVARMA:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\\\\n&&& B_0 \\eta_t+ B_1 \\eta_{t-1} + \\dots +  B_q \\eta_{t-q},\n\\end{array}\\tag{3.2}\n\\end{eqnarray}\\]\n\\(\\varepsilon_t = B_0\\eta_t\\), \\(B_j = \\Theta_j B_0\\), \\(j \\ge 0\\) (\\(\\Theta_0 = Id\\)), \\(\\{\\eta_{t}\\}\\) white noise sequence whose components mutually serially independent.","code":""},{"path":"VAR.html","id":"IRFSVARMA","chapter":"3 Multivariate models","heading":"3.2 IRFs in SVARMA","text":"One main objectives macro-econometrics derive IRFs, represent dynamic effects structural shocks (components \\(\\eta_t\\)) though system variables \\(y_t\\).Formally, IRF difference conditional expectations:\n\\[\\begin{equation}\n\\boxed{\\Psi_{,j,h} = \\mathbb{E}(y_{,t+h}|\\eta_{j,t}=1) - \\mathbb{E}(y_{,t+h})}\\tag{3.3}\n\\end{equation}\\]\n(effect \\(y_{,t+h}\\) one-unit shock \\(\\eta_{j,t}\\)).IRFs closely relate Wold decomposition \\(y_t\\). Indeed, dynamics process \\(y_t\\) can described VARMA model, \\(y_t\\) covariance stationary (see Def. 1.4), \\(y_t\\) admits following infinite MA representation (Wold decomposition):\n\\[\\begin{equation}\ny_t = \\mu + \\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-h}.\\tag{3.4}\n\\end{equation}\\]\nnotations, get \\(\\mathbb{E}(y_{,t+h}|\\eta_{j,t}=1) = \\mu_i + \\Psi_{,j,h}\\), \\(\\Psi_{,j,h}\\) component \\((,j)\\) matrix \\(\\Psi_h\\) \\(\\mu_i\\) \\(^{th}\\) entry vector \\(\\mu\\). Since also \\(\\mathbb{E}(y_{,t+h})=\\mu_i\\), obtain Eq. (3.3).Hence, estimating IRFs amounts estimating \\(\\Psi_{h}\\)’s. general, exist three main approaches :Calibrate solve (purely structural) Dynamic Stochastic General Equilibrium (DSGE) model first order (linearization). solution takes form Eq. (3.4).Directly estimate \\(\\Psi_{h}\\) based projection approaches à la Jordà (2005) (see web-page).Approximate infinite MA representation estimating parsimonious type model, e.g. VAR(MA) models (see Section 3.4). (Structural) VARMA representation obtained, Eq. (3.4) easily deduced using following proposition:Proposition 2.7  (IRF VARMA(p,q) process) \\(y_t\\) follows VARMA model described Def. 3.2, matrices \\(\\Psi_h\\) appearing Eq. (3.4) can computed recursively follows:Set \\(\\Psi_{-1}=\\dots=\\Psi_{-p}=0\\).\\(h \\ge 0\\), (recursively) apply:\n\\[\n\\Psi_h = \\Phi_1 \\Psi_{h-1} + \\dots + \\Phi_p \\Psi_{h-p} + \\Theta_h B_0,\n\\]\n\\(\\Theta_0 = Id\\) \\(\\Theta_h = 0\\) \\(h>q\\).Proof. obtained applying operator \\(\\frac{\\partial}{\\partial \\varepsilon_{t}}\\) sides Eq. (3.2).Typically, consider VAR(2) case. first steps algorithm mentioned last bullet point follows:\n\\[\\begin{eqnarray*}\ny_t &=& \\Phi_1 {\\color{blue}y_{t-1}} + \\Phi_2 y_{t-2} + B \\eta_t  \\\\\n&=& \\Phi_1 \\color{blue}{(\\Phi_1 y_{t-2} + \\Phi_2 y_{t-3} + B \\eta_{t-1})} + \\Phi_2 y_{t-2} + B \\eta_t  \\\\\n&=& B \\eta_t + \\Phi_1 B \\eta_{t-1} + (\\Phi_2 + \\Phi_1^2) \\color{red}{y_{t-2}} + \\Phi_1\\Phi_2 y_{t-3}  \\\\\n&=& B \\eta_t + \\Phi_1 B \\eta_{t-1} + (\\Phi_2 + \\Phi_1^2) \\color{red}{(\\Phi_1 y_{t-3} + \\Phi_2 y_{t-4} + B \\eta_{t-2})} + \\Phi_1\\Phi_2 y_{t-3} \\\\\n&=& \\underbrace{B}_{=\\Psi_0} \\eta_t + \\underbrace{\\Phi_1 B}_{=\\Psi_1} \\eta_{t-1} + \\underbrace{(\\Phi_2 + \\Phi_1^2)B}_{=\\Psi_2} \\eta_{t-2} + f(y_{t-3},y_{t-4}).\n\\end{eqnarray*}\\]particular, \\(B = \\Psi_0\\). Matrix \\(B\\) indeed captures contemporaneous impact \\(\\eta_t\\) \\(y_t\\). matrix \\(B\\) sometimes called impulse matrix.Example 3.1  (IRFs SVARMA model) Consider following VARMA(1,1) model:\n\\[\\begin{eqnarray}\n\\quad y_t &=&\n\\underbrace{\\left[\\begin{array}{cc}\n0.5 & 0.3 \\\\\n-0.4 & 0.7\n\\end{array}\\right]}_{\\Phi_1}\ny_{t-1} +  \n\\underbrace{\\left[\\begin{array}{cc}\n1 & 2 \\\\\n-1 & 1\n\\end{array}\\right]}_{B}\\eta_t - \\underbrace{\\left[\\begin{array}{cc}\n-0.4 & 0 \\\\\n1 & 0.5\n\\end{array}\\right]}_{\\Theta_1} \\underbrace{\\left[\\begin{array}{cc}\n1 & 2 \\\\\n-1 & 1\n\\end{array}\\right]}_{B}\\eta_{t-1}.\\tag{3.5}\n\\end{eqnarray}\\]can use function simul.VARMA package AEC produce IRFs (using indic.IRF=1 list arguments):\nFigure 3.1: Impulse response functions (SVARMA(1,1) specified ).\nweb interface allows explore IRFs context simple VAR(1) model \\(n=2\\). (Select VAR(1) Simulation tick IRFs.)","code":"\nlibrary(AEC)\ndistri <- list(type=c(\"gaussian\",\"gaussian\"),df=c(4,4))\nn <- length(distri$type) # dimension of y_t\nnb.sim <- 30\neps <- simul.distri(distri,nb.sim)\nPhi <- array(NaN,c(n,n,1))\nPhi[,,1] <- matrix(c(.5,-.4,.3,.7),2,2)\np <- dim(Phi)[3]\nTheta <- array(NaN,c(n,n,1))\nTheta[,,1] <- matrix(c(-0.4,1,0,.5),2,2)\nq <- dim(Theta)[3]\nMu <- rep(0,n)\nC <- matrix(c(1,-1,2,1),2,2)\nModel <- list(\n  Mu = Mu,Phi = Phi,Theta = Theta,C = C,distri = distri)\nY0 <- rep(0,n)\neta0 <- c(1,0)\nres.sim.1 <- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1)\neta0 <- c(0,1)\nres.sim.2 <- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1)\npar(plt=c(.15,.95,.25,.8))\npar(mfrow=c(2,2))\nfor(i in 1:2){\n  if(i == 1){res.sim <- res.sim.1\n  }else{res.sim <- res.sim.2}\n  for(j in 1:2){\n    plot(res.sim$Y[j,],las=1,\n         type=\"l\",lwd=3,xlab=\"\",ylab=\"\",\n         main=paste(\"Resp. of y\",j,\n                    \" to a 1-unit increase in eta\",i,sep=\"\"))\n    abline(h=0,col=\"grey\",lty=3)\n  }}"},{"path":"VAR.html","id":"covariance-stationary-varma-models","chapter":"3 Multivariate models","heading":"3.3 Covariance-stationary VARMA models","text":"Let’s come back infinite MA case (Eq. (3.4)):\n\\[\ny_t = \\mu + \\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-h}.\n\\]\n\\(y_t\\) covariance-stationary (ergodic mean), case \n\\[\\begin{equation}\n\\sum_{=0}^\\infty \\|\\Psi_i\\| < \\infty,\\tag{3.6}\n\\end{equation}\\]\n\\(\\|\\|\\) denotes norm matrix \\(\\) (e.g. \\(\\|\\|=\\sqrt{tr(AA')}\\)). notably implies \\(y_t\\) stationary (ergodic mean), \\(\\|\\Psi_h\\|\\rightarrow 0\\) \\(h\\) gets large.satisfied \\(\\Phi_k\\)’s \\(\\Theta_k\\)’s VARMA-based process (Eq. (3.2)) stationary? conditions similar univariate case. Let us introduce following notations:\n\\[\\begin{eqnarray}\ny_t &=& c + \\underbrace{\\Phi_1 y_{t-1} + \\dots +\\Phi_p y_{t-p}}_{\\color{blue}{\\mbox{AR component}}} +  \\tag{3.7}\\\\\n&&\\underbrace{B \\eta_t - \\Theta_1 B \\eta_{t-1} - \\dots - \\Theta_q B \\eta_{t-q}}_{\\color{red}{\\mbox{MA component}}} \\nonumber\\\\\n&\\Leftrightarrow& \\underbrace{ \\color{blue}{(- \\Phi_1 L - \\dots - \\Phi_p L^p)}}_{= \\color{blue}{\\Phi(L)}}y_t = c +  \\underbrace{ \\color{red}{(- \\Theta_1 L - \\ldots - \\Theta_q L^q)}}_{=\\color{red}{\\Theta(L)}} B \\eta_{t}. \\nonumber\n\\end{eqnarray}\\]Process \\(y_t\\) stationary iff roots \\(\\det(\\Phi(z))=0\\) strictly outside unit circle , equivalently, iff eigenvalues \n\\[\\begin{equation}\n\\Phi = \\left[\\begin{array}{cccc}\n\\Phi_{1} & \\Phi_{2} & \\cdots & \\Phi_{p}\\\\\n& 0 & \\cdots & 0\\\\\n0 & \\ddots & 0 & 0\\\\\n0 & 0 & & 0\\end{array}\\right]\\tag{3.8}\n\\end{equation}\\]\nlie strictly within unit circle. Hence, case univariate processes, covariance-stationarity VARMA model depends specification AR part.Let’s derive first two unconditional moments (covariance-stationary) VARMA process.Eq. (3.7) gives \\(\\mathbb{E}(\\Phi(L)y_t)=c\\), therefore \\(\\Phi(1)\\mathbb{E}(y_t)=c\\), \n\\[\n\\mathbb{E}(y_t) = (- \\Phi_1 - \\dots - \\Phi_p)^{-1}c.\n\\]\nautocovariances \\(y_t\\) can deduced infinite MA representation (Eq. (3.4)). :\n\\[\n\\gamma_j \\equiv \\mathbb{C}ov(y_t,y_{t-j}) = \\sum_{=j}^\\infty \\Psi_i \\Psi_{-j}'.\n\\]\n(infinite sum exists soon Eq. (3.6) satisfied.)Conditional means autocovariances can also deduced Eq. (3.4). \\(0 \\le h\\) \\(0 \\le h_1 \\le h_2\\):\n\\[\\begin{eqnarray*}\n\\mathbb{E}_t(y_{t+h}) &=& \\mu + \\sum_{k=0}^\\infty \\Psi_{k+h} \\eta_{t-k} \\\\\n\\mathbb{C}ov_t(y_{t+1+h_1},y_{t+1+h_2}) &=& \\sum_{k=0}^{h_1} \\Psi_{k}\\Psi_{k+h_2-h_1}'.\n\\end{eqnarray*}\\]previous formula implies particular forecasting error \\(y_{t+h} - \\mathbb{E}_t(y_{t+h})\\) variance equal :\n\\[\n\\mathbb{V}ar_t(y_{t+1+h}) = \\sum_{k=0}^{h} \\Psi_{k}\\Psi_{k}'.\n\\]\n\\(\\eta_t\\) mutually serially independent (therefore uncorrelated), :\n\\[\n\\mathbb{V}ar(\\Psi_k \\eta_{t-k}) = \\mathbb{V}ar\\left(\\sum_{=1}^n \\psi_{k,} \\eta_{,t-k}\\right)  = \\sum_{=1}^n \\psi_{k,}\\psi_{k,}',\n\\]\n\\(\\psi_{k,}\\) denotes \\(^{th}\\) column \\(\\Psi_k\\). suggests following decomposition variance forecast error (called variance decomposition):\n\\[\n\\mathbb{V}ar_t(y_{t+1+h}) = \\sum_{=1}^n \\underbrace{\\sum_{k=0}^{h}  \\psi_{k,}\\psi_{k,}'.}_{\\mbox{Contribution $\\eta_{,t}$}}\n\\]Let us now turn estimation VAR models. Note MA component (.e., consider VARMA model), OLS regressions yield biased estimates (even asymptotically large samples). Assume instance \\(y_t\\) follows VARMA(1,1) model:\n\\[\ny_{,t} = \\phi_i y_{t-1} + \\varepsilon_{,t},\n\\]\n\\(\\phi_i\\) \\(^{th}\\) row \\(\\Phi_1\\), \\(\\varepsilon_{,t}\\) linear combination \\(\\eta_t\\) \\(\\eta_{t-1}\\). Since \\(y_{t-1}\\) (regressor) correlated \\(\\eta_{t-1}\\), also correlated \\(\\varepsilon_{,t}\\). OLS regression \\(y_{,t}\\) \\(y_{t-1}\\) yields biased estimator \\(\\phi_i\\) (see Figure 3.2). Hence, SVARMA models consistently estimated simple OLS regressions (contrary VAR models, see next section); instrumental-variable approaches can employed estimate SVARMA models (using past values \\(y_t\\) instruments, see, e.g., Gouriéroux, Monfort, Renne (2020)).\nFigure 3.2: Illustration bias obtained estimating auto-regressive parameters ARMA process (standard) OLS.\n","code":"\nN <- 1000 # number of replications\nT <- 100 # sample length\nphi <- .8 # autoregressive parameter\nsigma <- 1\npar(mfrow=c(1,2))\nfor(theta in c(0,-0.4)){\n  all.y <- matrix(0,1,N)\n  y     <- all.y\n  eta_1 <- rnorm(N)\n  for(t in 1:(T+1)){\n    eta <- rnorm(N)\n    y <- phi * y + sigma * eta + theta * sigma * eta_1\n    all.y <- rbind(all.y,y)\n    eta_1 <- eta}\n  all.y_1 <- all.y[1:T,]\n  all.y   <- all.y[2:(T+1),]\n  XX_1 <- 1/apply(all.y_1 * all.y_1,2,sum)\n  XY   <- apply(all.y_1 * all.y,2,sum)\n  phi.est.OLS <- XX_1 * XY\n  plot(density(phi.est.OLS),xlab=\"OLS estimate of phi\",ylab=\"\",\n       main=paste(\"theta = \",theta,sep=\"\"))\n  abline(v=phi,col=\"red\",lwd=2)}"},{"path":"VAR.html","id":"estimVAR","chapter":"3 Multivariate models","heading":"3.4 VAR estimation","text":"section discusses estimation VAR models. Eq. (3.1) can written:\n\\[\ny_{t}=c+\\Phi(L)y_{t-1}+\\varepsilon_{t},\n\\]\n\\(\\Phi(L) = \\Phi_1 + \\Phi_2 L + \\dots + \\Phi_p L^{p-1}\\).Consequently:\n\\[\ny_{t}\\mid y_{t-1},y_{t-2},\\ldots,y_{-p+1}\\sim \\mathcal{N}(c+\\Phi_{1}y_{t-1}+\\ldots\\Phi_{p}y_{t-p},\\Omega).\n\\]Using Hamilton (1994)’s notations, denote \\(\\Pi\\) matrix \\(\\left[\\begin{array}{ccccc} c & \\Phi_{1} & \\Phi_{2} & \\ldots & \\Phi_{p}\\end{array}\\right]'\\) \\(x_{t}\\) vector \\(\\left[\\begin{array}{ccccc} 1 & y'_{t-1} & y'_{t-2} & \\ldots & y'_{t-p}\\end{array}\\right]'\\), :\n\\[\\begin{equation}\ny_{t}= \\Pi'x_{t} + \\varepsilon_{t}. \\tag{3.9}\n\\end{equation}\\]\nprevious representation convenient discuss estimation VAR model, parameters gathered two matrices : \\(\\Pi\\) \\(\\Omega\\).Let us start case shocks Gaussian.Proposition 3.1  (MLE Gaussian VAR) \\(y_t\\) follows VAR(p) (see Definition 3.1), \\(\\varepsilon_t \\sim \\,..d.\\,\\mathcal{N}(0,\\Omega)\\), ML estimate \\(\\Pi\\), denoted \\(\\hat{\\Pi}\\) (see Eq. (3.9)), given \n\\[\\begin{equation}\n\\hat{\\Pi}=\\left[\\sum_{t=1}^{T}x_{t}x'_{t}\\right]^{-1}\\left[\\sum_{t=1}^{T}y_{t}'x_{t}\\right]= (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y},\\tag{3.10}\n\\end{equation}\\]\n\\(\\mathbf{X}\\) \\(T \\times (1+np)\\) matrix whose \\(t^{th}\\) row \\(x_t\\) \\(\\mathbf{y}\\) \\(T \\times n\\) matrix whose \\(t^{th}\\) row \\(y_{t}'\\)., \\(^{th}\\) column \\(\\hat{\\Pi}\\) (\\(b_i\\), say) OLS estimate \\(\\beta_i\\), :\n\\[\\begin{equation}\ny_{,t} = \\beta_i'x_t + \\varepsilon_{,t},\\tag{3.11}\n\\end{equation}\\]\n(.e., \\(\\beta_i' = [c_i,\\phi_{,1}',\\dots,\\phi_{,p}']'\\)).ML estimate \\(\\Omega\\), denoted \\(\\hat{\\Omega}\\), coincides sample covariance matrix \\(n\\) series OLS residuals Eq. (3.11), .e.:\n\\[\\begin{equation}\n\\hat{\\Omega} = \\frac{1}{T} \\sum_{=1}^T \\hat{\\varepsilon}_t\\hat{\\varepsilon}_t',\\quad\\mbox{} \\hat{\\varepsilon}_t= y_t - \\hat{\\Pi}'x_t.\n\\end{equation}\\]asymptotic distributions estimators ones resulting standard OLS formula.Proof. See Appendix 8.4.stated Proposition 3.2, shocks Gaussian, OLS regressions still provide consistent estimates model parameters. However, since \\(x_t\\) correlates \\(\\varepsilon_s\\) \\(s<t\\), OLS estimator \\(\\mathbf{b}_i\\) \\(\\boldsymbol\\beta_i\\) biased small sample. (also case ML estimator.) Indeed, denoting \\(\\boldsymbol\\varepsilon_i\\) \\(T \\times 1\\) vector \\(\\varepsilon_{,t}\\)’s, using notations \\(b_i\\) \\(\\beta_i\\) introduced Proposition 3.1, :\n\\[\\begin{equation}\n\\mathbf{b}_i = \\beta_i + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon_i.\\tag{2.19}\n\\end{equation}\\]\nnon-zero correlation \\(x_t\\) \\(\\varepsilon_{,s}\\) \\(s<t\\) , therefore, \\(\\mathbb{E}[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon_i] \\ne 0\\).However, \\(y_t\\) covariance stationary, \\(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\) converges positive definite matrix \\(\\mathbf{Q}\\), \\(\\frac{1}{n}X'\\boldsymbol\\varepsilon_i\\) converges 0. Hence \\(\\mathbf{b}_i \\overset{p}{\\rightarrow} \\beta_i\\). precisely:Proposition 3.2  (Asymptotic distribution OLS estimate) \\(y_t\\) follows VAR model, defined Definition 3.1, :\n\\[\n\\sqrt{T}(\\mathbf{b}_i-\\beta_i) =  \\underbrace{\\left[\\frac{1}{T}\\sum_{t=p}^T x_t x_t' \\right]^{-1}}_{\\overset{p}{\\rightarrow} \\mathbf{Q}^{-1}}\n\\underbrace{\\sqrt{T} \\left[\\frac{1}{T}\\sum_{t=1}^T x_t\\varepsilon_{,t} \\right]}_{\\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma_i^2\\mathbf{Q})},\n\\]\n\\(\\sigma_i = \\mathbb{V}ar(\\varepsilon_{,t})\\) \\(\\mathbf{Q} = \\mbox{plim }\\frac{1}{T}\\sum_{t=p}^T x_t x_t'\\) given :\n\\[\\begin{equation}\n\\mathbf{Q} = \\left[\n\\begin{array}{ccccc}\n1 & \\mu' &\\mu' & \\dots & \\mu' \\\\\n\\mu & \\gamma_0 + \\mu\\mu' & \\gamma_1 + \\mu\\mu' & \\dots & \\gamma_{p-1} + \\mu\\mu'\\\\\n\\mu & \\gamma_1 + \\mu\\mu' & \\gamma_0 + \\mu\\mu' & \\dots & \\gamma_{p-2} + \\mu\\mu'\\\\\n\\vdots &\\vdots &\\vdots &\\dots &\\vdots \\\\\n\\mu & \\gamma_{p-1} + \\mu\\mu' & \\gamma_{p-2} + \\mu\\mu' & \\dots & \\gamma_{0} + \\mu\\mu'\n\\end{array}\n\\right].\\tag{2.20}\n\\end{equation}\\]Proof. See Appendix 8.4.following proposition extends previous proposition includes covariances different \\(\\beta_i\\)’s well asymptotic distribution ML estimates \\(\\Omega\\).Proposition 3.3  (Asymptotic distribution OLS estimates) \\(y_t\\) follows VAR model, defined Definition 3.1, :\n\\[\\begin{equation}\n\\sqrt{T}\\left[\n\\begin{array}{c}\nvec(\\hat\\Pi - \\Pi)\\\\\nvec(\\hat\\Omega - \\Omega)\n\\end{array}\n\\right]\n\\sim \\mathcal{N}\\left(0,\n\\left[\n\\begin{array}{cc}\n\\Omega \\otimes \\mathbf{Q}^{-1} & 0\\\\\n0 & \\Sigma_{22}\n\\end{array}\n\\right]\\right),\\tag{3.12}\n\\end{equation}\\]\ncomponent \\(\\Sigma_{22}\\) corresponding covariance \\(\\hat\\sigma_{,j}\\) \\(\\hat\\sigma_{k,l}\\) (\\(,j,l,m \\\\{1,\\dots,n\\}^4\\)) equal \\(\\sigma_{,l}\\sigma_{j,m}+\\sigma_{,m}\\sigma_{j,l}\\).Proof. See Hamilton (1994), Appendix Chapter 11.practice, use previous proposition (instance implement Monte-Carlo simulations, see Section 8.5.1), \\(\\Omega\\) replaced \\(\\hat{\\Omega}\\), \\(\\mathbf{Q}\\) replaced \\(\\hat{\\mathbf{Q}} = \\frac{1}{T}\\sum_{t=p}^T x_t x_t'\\) \\(\\Sigma\\) matrix whose components form \\(\\hat\\sigma_{,l}\\hat\\sigma_{j,m}+\\hat\\sigma_{,m}\\hat\\sigma_{j,l}\\), \\(\\hat\\sigma_{,l}\\)’s components \\(\\hat\\Omega\\).simplicity VAR framework tractability MLE open way convenient econometric testing. Let’s illustrate likelihood ratio test.5 maximum value achieved MLE \n\\[\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\hat{\\Omega}) = -\\frac{Tn}{2}\\log(2\\pi)+\\frac{T}{2}\\log\\left|\\hat{\\Omega}^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\]\nlast term :\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t} &=& \\mbox{Tr}\\left[\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\right] = \\mbox{Tr}\\left[\\sum_{t=1}^{T}\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t}'\\right]\\\\\n&=&\\mbox{Tr}\\left[\\hat{\\Omega}^{-1}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t}'\\right] = \\mbox{Tr}\\left[\\hat{\\Omega}^{-1}\\left(T\\hat{\\Omega}\\right)\\right]=Tn.\n\\end{eqnarray*}\\]\nTherefore, optimized log-likelihood simply obtained :\n\\[\\begin{equation}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\hat{\\Omega})=-(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\hat{\\Omega}^{-1}\\right|-Tn/2.\\tag{3.13}\n\\end{equation}\\]Assume want test null hypothesis set variables follows VAR(\\(p_{0}\\)) alternative specification \\(p_{1}\\) (\\(>p_{0}\\)). Let us denote \\(\\hat{L}_{0}\\) \\(\\hat{L}_{1}\\) maximum log-likelihoods obtained \\(p_{0}\\) \\(p_{1}\\) lags, respectively. null hypothesis (\\(H_0\\): \\(p=p_0\\)), :\n\\[\\begin{eqnarray*}\n2\\left(\\hat{L}_{1}-\\hat{L}_{0}\\right)&=&T\\left(\\log\\left|\\hat{\\Omega}_{1}^{-1}\\right|-\\log\\left|\\hat{\\Omega}_{0}^{-1}\\right|\\right)  \\sim \\chi^2(n^{2}(p_{1}-p_{0})).\n\\end{eqnarray*}\\]precedes can used help determine appropriate number lags use specification. VAR, using many lags consumes numerous degrees freedom: \\(p\\) lags, \\(n\\) equations VAR contains \\(n\\times p\\) coefficients plus intercept term. Adding lags improve -sample fit, likely result -parameterization affect --sample prediction performance.select appropriate lag length, selection criteria often used. context VAR models, using Eq. (3.13) (Gaussian case), instance:\n\\[\\begin{eqnarray*}\nAIC & = & cst + \\log\\left|\\hat{\\Omega}\\right|+\\frac{2}{T}N\\\\\nBIC & = & cst + \\log\\left|\\hat{\\Omega}\\right|+\\frac{\\log T}{T}N,\n\\end{eqnarray*}\\]\n\\(N=p \\times n^{2}\\).","code":"\nlibrary(vars);library(AEC)\ndata <- US3var[,c(\"y.gdp.gap\",\"infl\")]\nVARselect(data,lag.max = 6)## $selection\n## AIC(n)  HQ(n)  SC(n) FPE(n) \n##      3      3      2      3 \n## \n## $criteria\n##                 1          2          3          4          5           6\n## AIC(n) -0.3394120 -0.4835525 -0.5328327 -0.5210835 -0.5141079 -0.49112812\n## HQ(n)  -0.3017869 -0.4208439 -0.4450407 -0.4082080 -0.3761491 -0.32808581\n## SC(n)  -0.2462608 -0.3283005 -0.3154798 -0.2416298 -0.1725534 -0.08747275\n## FPE(n)  0.7121914  0.6165990  0.5869659  0.5939325  0.5981364  0.61210908\nestimated.var <- VAR(data,p=3)\n#print(estimated.var$varresult)\nPhi <- Acoef(estimated.var)\nPHI <- make.PHI(Phi) # autoregressive matrix of companion form.\nprint(abs(eigen(PHI)$values)) # check stationarity## [1] 0.9114892 0.9114892 0.6319554 0.4759403 0.4759403 0.3246995"},{"path":"VAR.html","id":"BlockGranger","chapter":"3 Multivariate models","heading":"3.5 Block exogeneity and Granger causality","text":"","code":""},{"path":"VAR.html","id":"block-exogeneity","chapter":"3 Multivariate models","heading":"3.5.1 Block exogeneity","text":"Let’s decompose \\(y_t\\) two subvectors \\(y^{(1)}_{t}\\) (\\(n_1 \\times 1\\)) \\(y^{(2)}_{t}\\) (\\(n_2 \\times 1\\)), \\(y_t' = [{y^{(1)}_{t}}',{y^{(2)}_{t}}']\\) (therefore \\(n=n_1 +n_2\\)), :\n\\[\n\\left[\n\\begin{array}{c}\ny^{(1)}_{t}\\\\\ny^{(2)}_{t}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\n\\Phi^{(1,1)} & \\Phi^{(1,2)}\\\\\n\\Phi^{(2,1)} & \\Phi^{(2,2)}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\ny^{(1)}_{t-1}\\\\\ny^{(2)}_{t-1}\n\\end{array}\n\\right] + \\varepsilon_t.\n\\]\nUsing, e.g., likelihood ratio test, one can easily test block exogeneity \\(y_t^{(2)}\\) (say). null assumption can expressed \\(\\Phi^{(2,1)}=0\\).","code":""},{"path":"VAR.html","id":"granger-causality","chapter":"3 Multivariate models","heading":"3.5.2 Granger Causality","text":"Granger (1969) developed method explore causal relationships among variables. approach consists determining whether past values \\(y_{1,t}\\) can help explain current \\(y_{2,t}\\) (beyond information already included past values \\(y_{2,t}\\)).Formally, let us denote three information sets:\n\\[\\begin{eqnarray*}\n\\mathcal{}_{1,t} & = & \\left\\{ y_{1,t},y_{1,t-1},\\ldots\\right\\} \\\\\n\\mathcal{}_{2,t} & = & \\left\\{ y_{2,t},y_{2,t-1},\\ldots\\right\\} \\\\\n\\mathcal{}_{t} & = & \\left\\{ y_{1,t},y_{1,t-1},\\ldots y_{2,t},y_{2,t-1},\\ldots\\right\\}.\n\\end{eqnarray*}\\]\nsay \\(y_{1,t}\\) Granger-causes \\(y_{2,t}\\) \n\\[\n\\mathbb{E}\\left[y_{2,t}\\mid \\mathcal{}_{2,t-1}\\right]\\neq \\mathbb{E}\\left[y_{2,t}\\mid \\mathcal{}_{t-1}\\right].\n\\]get intuition behind testing procedure, consider following\nbivariate VAR(\\(p\\)) process:\n\\[\\begin{eqnarray*}\ny_{1,t} & = & c_1+\\Sigma_{=1}^{p}\\Phi_i^{(11)}y_{1,t-}+\\Sigma_{=1}^{p}\\Phi_i^{(12)}y_{2,t-}+\\varepsilon_{1,t}\\\\\ny_{2,t} & = & c_2+\\Sigma_{=1}^{p}\\Phi_i^{(21)}y_{1,t-}+\\Sigma_{=1}^{p}\\Phi_i^{(22)}y_{2,t-}+\\varepsilon_{2,t},\n\\end{eqnarray*}\\]\n\\(\\Phi_k^{(ij)}\\) denotes element \\((,j)\\) \\(\\Phi_k\\). , \\(y_{1,t}\\) said Granger-cause \\(y_{2,t}\\) \n\\[\n\\Phi_1^{(21)}=\\Phi_2^{(21)}=\\ldots=\\Phi_p^{(21)}=0.\n\\]\nnull alternative hypotheses therefore :\n\\[\n\\begin{cases}\nH_{0}: & \\Phi_1^{(21)}=\\Phi_2^{(21)}=\\ldots=\\Phi_p^{(21)}=0\\\\\nH_{1}: & \\Phi_1^{(21)}\\neq0\\mbox{ }\\Phi_2^{(21)}\\neq0\\mbox{ }\\ldots\\Phi_p^{(21)}\\neq0.\\end{cases}\n\\]\nLoosely speaking, reject \\(H_{0}\\) coefficients lagged \\(y_{1,t}\\)’s statistically significant. Formally, can tested using \\(F\\)-test asymptotic chi-square test. \\(F\\)-statistic \n\\[\nF=\\frac{(RSS-USS)/p}{USS/(T-2p-1)},\n\\]\nRSS Restricted sum squared residuals USS Unrestricted sum squared residuals. \\(H_{0}\\), \\(F\\)-statistic distributed \\(\\mathcal{F}(p,T-2p-1)\\) (See Table 8.4).6According following lines code, output gap Granger-causes inflation, reverse true:","code":"\ngrangertest(US3var[,c(\"y.gdp.gap\",\"infl\")],order=3)## Granger causality test\n## \n## Model 1: infl ~ Lags(infl, 1:3) + Lags(y.gdp.gap, 1:3)\n## Model 2: infl ~ Lags(infl, 1:3)\n##   Res.Df Df      F   Pr(>F)   \n## 1    214                      \n## 2    217 -3 3.9761 0.008745 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ngrangertest(US3var[,c(\"infl\",\"y.gdp.gap\")],order=3)## Granger causality test\n## \n## Model 1: y.gdp.gap ~ Lags(y.gdp.gap, 1:3) + Lags(infl, 1:3)\n## Model 2: y.gdp.gap ~ Lags(y.gdp.gap, 1:3)\n##   Res.Df Df      F Pr(>F)\n## 1    214                 \n## 2    217 -3 1.5451 0.2038"},{"path":"VAR.html","id":"identifStruct","chapter":"3 Multivariate models","heading":"3.6 Standard identification techniques","text":"","code":""},{"path":"VAR.html","id":"IdentifPbm","chapter":"3 Multivariate models","heading":"3.6.1 The identification problem","text":"Section 3.4, seen estimate \\(\\mathbb{V}ar(\\varepsilon_t) =\\Omega\\) \\(\\Phi_k\\) matrices context VAR model. IRFs functions \\(B\\) \\(\\Phi_k\\)’s, \\(\\Omega\\) \\(\\Phi_k\\)’s (see Section 3.2). \\(\\Omega = BB'\\), provides restrictions components \\(B\\), sufficient fully identify \\(B\\). Indeed, seen system equations whose unknowns \\(b_{,j}\\)’s (components \\(B\\)), system \\(\\Omega = BB'\\) contains \\(n(n+1)/2\\) linearly independent equations. instance, \\(n=2\\):\n\\[\\begin{eqnarray*}\n&&\\left[\n\\begin{array}{cc}\n\\omega_{11} & \\omega_{12} \\\\\n\\omega_{12} & \\omega_{22}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22}\n\\end{array}\n\\right]\\left[\n\\begin{array}{cc}\nb_{11} & b_{21} \\\\\nb_{12} & b_{22}\n\\end{array}\n\\right]\\\\\n&\\Leftrightarrow&\\left[\n\\begin{array}{cc}\n\\omega_{11} & \\omega_{12} \\\\\n\\omega_{12} & \\omega_{22}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\nb_{11}^2+b_{12}^2 & \\color{red}{b_{11}b_{21}+b_{12}b_{22}} \\\\\n\\color{red}{b_{11}b_{21}+b_{12}b_{22}} & b_{22}^2 + b_{21}^2\n\\end{array}\n\\right].\n\\end{eqnarray*}\\]3 linearly independent equations 4 unknowns. Therefore, \\(B\\) identified based second-order moments. Additional restrictions required identify \\(B\\). section covers two standard identification schemes: short-run long-run restrictions.short-run restriction (SRR) prevents structural shock affecting endogenous variable contemporaneously.easy implement: appropriate entries \\(B\\) set 0.particular (popular) case Cholesky, recursive approach.Examples include Bernanke (1986), Sims (1986), Galí (1992), Ruibio-Ramírez, Waggoner, Zha (2010).long-run restriction (LRR) prevents structural shock cumulative impact one endogenous variables.Additional computations required implement . One needs compute cumulative effect one structural shocks \\(u_{t}\\) one endogenous variable.Examples include Blanchard Quah (1989), Faust Leeper (1997), Galí (1999), Erceg, Guerrieri, Gust (2005), Christiano, Eichenbaum, Vigfusson (2007).two approaches can combined (see, e.g., Gerlach Smets (1995)).","code":""},{"path":"VAR.html","id":"a-stylized-example-motivating-short-run-restrictions","chapter":"3 Multivariate models","heading":"3.6.2 A stylized example motivating short-run restrictions","text":"Let us consider simple example motivate short-run restrictions. Consider following stylized macro model:\n\\[\\begin{equation}\n\\begin{array}{clll}\ng_{t}&=& \\bar{g}-\\lambda(i_{t-1}-\\mathbb{E}_{t-1}\\pi_{t})+ \\underbrace{{\\color{blue}\\sigma_d \\eta_{d,t}}}_{\\mbox{demand shock}}& (\\mbox{curve})\\\\\n\\Delta \\pi_{t} & = & \\beta (g_{t} - \\bar{g})+ \\underbrace{{\\color{blue}\\sigma_{\\pi} \\eta_{\\pi,t}}}_{\\mbox{cost push shock}} & (\\mbox{Phillips curve})\\\\\ni_{t} & = & \\rho i_{t-1} + \\left[ \\gamma_\\pi \\mathbb{E}_{t}\\pi_{t+1}  + \\gamma_g (g_{t} - \\bar{g}) \\right]\\\\\n&& \\qquad \\qquad+\\underbrace{{\\color{blue}\\sigma_{mp} \\eta_{mp,t}}}_{\\mbox{Mon. Pol. shock}} & (\\mbox{Taylor rule}),\n\\end{array}\\tag{3.14}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n\\eta_t =\n\\left[\n\\begin{array}{c}\n\\eta_{\\pi,t}\\\\\n\\eta_{d,t}\\\\\n\\eta_{mp,t}\n\\end{array}\n\\right]\n\\sim ..d.\\,\\mathcal{N}(0,).\\tag{3.15}\n\\end{equation}\\]Vector \\(\\eta_t\\) assumed vector structural shocks, mutually serially independent. date \\(t\\):\\(g_t\\) contemporaneously affected \\(\\eta_{d,t}\\) ;\\(\\pi_t\\) contemporaneously affected \\(\\eta_{\\pi,t}\\) \\(\\eta_{d,t}\\);\\(i_t\\) contemporaneously affected \\(\\eta_{mp,t}\\), \\(\\eta_{\\pi,t}\\) \\(\\eta_{d,t}\\).System (3.14) rewritten follows:\n\\[\\begin{equation}\n\\left[\\begin{array}{c}\nd_t\\\\\n\\pi_t\\\\\ni_t\n\\end{array}\\right]\n= \\Phi(L)\n\\left[\\begin{array}{c}\nd_{t-1}\\\\\n\\pi_{t-1}\\\\\ni_{t-1} +\n\\end{array}\\right] +\\underbrace{\\underbrace{\n\\left[\n\\begin{array}{ccc}\n0 & \\bullet & 0 \\\\\n\\bullet & \\bullet & 0 \\\\\n\\bullet & \\bullet & \\bullet\n\\end{array}\n\\right]}_{=B} \\eta_t.}_{=\\varepsilon_t}\\tag{3.16}\n\\end{equation}\\]reduced-form model. representation suggests three additional restrictions entries \\(B\\); latter matrix therefore identified soon \\(\\Omega = BB'\\) known (signs columns).","code":""},{"path":"VAR.html","id":"cholesky-a-specific-short-run-restriction-situation","chapter":"3 Multivariate models","heading":"3.6.3 Cholesky: a specific short-run-restriction situation","text":"particular cases well-known matrix decomposition \\(\\Omega=\\mathbb{V}ar(\\varepsilon_t)\\) can used easily estimate specific SVAR. case -called Cholesky decomposition. Consider following context:first shock (say, \\(\\eta_{n_1,t}\\)) can affect instantaneously\n(.e., date \\(t\\)) one endogenous variable (say, \\(y_{n_1,t}\\));second shock (say, \\(\\eta_{n_2,t}\\)) can affect instantaneously\n(.e., date \\(t\\)) two endogenous variables, \\(y_{n_1,t}\\) () \\(y_{n_2,t}\\);\\(\\dots\\)impliesthat column \\(n_1\\) \\(B\\) 1 non-zero entry (\\(n_1^{th}\\) entry),column \\(n_2\\) \\(B\\) 2 non-zero entries (\\(n_1^{th}\\) \\(n_2^{th}\\) ones), etc.Without loss generality, can set \\(n_1=n\\), \\(n_2=n-1\\), etc. context, matrix \\(B\\) lower triangular. Cholesky decomposition \\(\\Omega_{\\varepsilon}\\) provides appropriate estimate \\(B\\), since matrix decomposition yields lower triangular matrix satisfying:\n\\[\n\\Omega_\\varepsilon = BB'.\n\\]instance, Dedola Lippi (2005) estimate 5 structural VAR models US, UK, Germany, France Italy analyse monetary-policy transmission mechanisms. estimate SVAR(5) models period 1975-1997. shock-identification scheme based Cholesky decompositions, ordering endogenous variables : industrial production, consumer price index, commodity price index, short-term rate, monetary aggregate effective exchange rate (except US). ordering implies monetary policy reacts shocks affecting first three variables latter react monetary policy shocks one-period lag .Cholesky approach can employed one interested one specific structural shock. case, e.g., Christiano, Eichenbaum, Evans (1996). identification based following relationship \\(\\varepsilon_t\\) \\(\\eta_t\\):\n\\[\n\\left[\\begin{array}{c}\n\\boldsymbol\\varepsilon_{S,t}\\\\\n\\varepsilon_{r,t}\\\\\n\\boldsymbol\\varepsilon_{F,t}\n\\end{array}\\right] =\n\\left[\\begin{array}{ccc}\nB_{SS} & 0 & 0 \\\\\nB_{rS} & B_{rr} & 0 \\\\\nB_{FS} & B_{Fr} & B_{FF}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n\\boldsymbol\\eta_{S,t}\\\\\n\\eta_{r,t}\\\\\n\\boldsymbol\\eta_{F,t}\n\\end{array}\\right],\n\\]\n\\(S\\), \\(r\\) \\(F\\) respectively correspond slow-moving variables, policy variable (short-term rate) fast-moving variables. \\(\\eta_{r,t}\\) scalar, \\(\\boldsymbol\\eta_{S,t}\\) \\(\\boldsymbol\\eta_{F,t}\\) may vectors. space spanned \\(\\boldsymbol\\varepsilon_{S,t}\\) spanned \\(\\boldsymbol\\eta_{S,t}\\). result, \\(\\varepsilon_{r,t}\\) linear combination \\(\\eta_{r,t}\\) \\(\\boldsymbol\\eta_{S,t}\\) (\\(\\perp\\)), comes \\(B_{rr}\\eta_{r,t}\\)’s (population) residuals regression \\(\\varepsilon_{r,t}\\) \\(\\boldsymbol\\varepsilon_{S,t}\\). \\(\\mathbb{V}ar(\\eta_{r,t})=1\\), \\(B_{rr}\\) given square root variance \\(B_{rr}\\eta_{r,t}\\). \\(B_{F,r}\\) finally obtained regressing components \\(\\boldsymbol\\varepsilon_{F,t}\\) estimates \\(\\eta_{r,t}\\).equivalent approach consists computing Cholesky decomposition \\(BB'\\) contemporaneous impacts monetary policy shock (\\(n\\) endogenous variables) components column \\(B\\) corresponding policy variable.\nFigure 3.3: Response monetary-policy shock. Identification approach Christiano, Eichenbaum Evans (1996). Confidence intervals obtained boostrapping estimated VAR model (see inference section).\n","code":"\nlibrary(AEC);library(vars)\ndata(\"USmonthly\")\n# Select sample period:\nFirst.date <- \"1965-01-01\";Last.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables <- c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\ny <- as.matrix(USmonthly[considered.variables])\nres.svar.ordering <- svar.ordering(y,p=3,\n                                   posit.of.shock = 5,\n                                   nb.periods.IRF = 20,\n                                   nb.bootstrap.replications = 100,\n                                   confidence.interval = 0.90, # expressed in pp.\n                                   indic.plot = 1 # Plots are displayed if = 1.\n)"},{"path":"VAR.html","id":"long-run-restrictions","chapter":"3 Multivariate models","heading":"3.6.4 Long-run restrictions","text":"Let us now turn Long-run restrictions. restriction concerns long-run influence shock endogenous variable. Let us consider instance structural shock assumed “long-run influence” GDP. express ? long-run change GDP can expressed \\(GDP_{t+h} - GDP_t\\), \\(h\\) large. Note :\n\\[\nGDP_{t+h} - GDP_t = \\Delta GDP_{t+h} +\\Delta GDP_{t+h-1} + \\dots + \\Delta GDP_{t+1}.\n\\]\nHence, fact given structural shock (\\(\\eta_{,t}\\), say) long-run influence GDP means \n\\[\n\\lim_{h\\rightarrow\\infty}\\frac{\\partial GDP_{t+h}}{\\partial \\eta_{,t}} = \\lim_{h\\rightarrow\\infty} \\frac{\\partial}{\\partial \\eta_{,t}}\\left(\\sum_{k=1}^h \\Delta  GDP_{t+k}\\right)= 0.\n\\]long-run effect can formulated function \\(B\\) matrices \\(\\Phi_i\\) \\(y_t\\) (including \\(\\Delta GDP_t\\)) follows VAR process.Without loss generality, consider VAR(1) case. Indeed, one can always write VAR(\\(p\\)) VAR(1): , stack last \\(p\\) values vector \\(y_t\\) vector \\(y_{t}^{*}=[y_t',\\dots,y_{t-p+1}']'\\); Eq. (3.1) can rewritten companion form:\n\\[\\begin{equation}\ny_{t}^{*} =\n\\underbrace{\\left[\\begin{array}{c}\nc\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right]}_{=c^*}+\n\\underbrace{\\left[\\begin{array}{cccc}\n\\Phi_{1} & \\Phi_{2} & \\cdots & \\Phi_{p}\\\\\n& 0 & \\cdots & 0\\\\\n0 & \\ddots & 0 & 0\\\\\n0 & 0 & & 0\\end{array}\\right]}_{=\\Phi}\ny_{t-1}^{*}+\n\\underbrace{\\left[\\begin{array}{c}\n\\varepsilon_{t}\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right]}_{\\varepsilon_t^*},\\tag{3.17}\n\\end{equation}\\]\nmatrices \\(\\Phi\\) \\(\\Omega^* = \\mathbb{V}ar(\\varepsilon_t^*)\\) dimension \\(np \\times np\\); \\(\\Omega^*\\) filled zeros, except \\(n\\times n\\) upper-left block equal \\(\\Omega = \\mathbb{V}ar(\\varepsilon_t)\\). (Matrix \\(\\Phi\\) introduced Eq. (3.8).)Let us focus VAR(1) case:\n\\[\\begin{eqnarray*}\ny_{t} &=& c+\\Phi y_{t-1}+\\varepsilon_{t}\\\\\n& = & c+\\varepsilon_{t}+\\Phi(c+\\varepsilon_{t-1})+\\ldots+\\Phi^{k}(c+\\varepsilon_{t-k})+\\ldots \\\\\n& = & \\mu +\\varepsilon_{t}+\\Phi\\varepsilon_{t-1}+\\ldots+\\Phi^{k}\\varepsilon_{t-k}+\\ldots \\\\\n& = & \\mu +B\\eta_{t}+\\Phi B\\eta_{t-1}+\\ldots+\\Phi^{k}B\\eta_{t-k}+\\ldots,\n\\end{eqnarray*}\\]\nWold representation \\(y_t\\).sequence shocks \\(\\{\\eta_t\\}\\) determines sequence \\(\\{y_t\\}\\). \\(\\{\\eta_t\\}\\) replaced \\(\\{\\tilde{\\eta}_t\\}\\), \\(\\tilde{\\eta}_t=\\eta_t\\) \\(t \\ne s\\) \\(\\tilde{\\eta}_s=\\eta_s + \\gamma\\)? Assume \\(\\{\\tilde{y}_t\\}\\) associated “perturbated” sequence. \\(\\tilde{y}_t = y_t\\) \\(t<s\\). \\(t \\ge s\\), Wold decomposition \\(\\{\\tilde{y}_t\\}\\) implies:\n\\[\n\\tilde{y}_t = y_t + \\Phi^{t-s} B \\gamma.\n\\]\nTherefore, cumulative impact \\(\\gamma\\) \\(\\tilde{y}_t\\) (\\(t \\ge s\\)):\n\\[\\begin{eqnarray}\n(\\tilde{y}_t - y_t) +  (\\tilde{y}_{t-1} - y_{t-1}) + \\dots +  (\\tilde{y}_s - y_s) &=& \\nonumber \\\\\n(Id + \\Phi + \\Phi^2 + \\dots + \\Phi^{t-s}) B \\gamma.&& \\tag{3.18}\n\\end{eqnarray}\\]Consider shock \\(\\eta_{1,t}\\), magnitude \\(1\\). shock corresponds \\(\\gamma = [1,0,\\dots,0]'\\). Given Eq. (3.18), long-run cumulative effect shock endogenous variables given :\n\\[\n\\underbrace{(Id+\\Phi+\\ldots+\\Phi^{k}+\\ldots)}_{=(Id - \\Phi)^{-1}}B\\left[\\begin{array}{c}\n1\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right],\n\\]\nfirst column \\(n \\times n\\) matrix \\(\\Theta \\equiv (Id - \\Phi)^{-1}B\\).context, consider following long-run restriction: “\\(j^{th}\\) structural shock cumulative impact \\(^{th}\\) endogenous variable”. equivalent \n\\[\n\\Theta_{ij}=0,\n\\]\n\\(\\Theta_{ij}\\) element \\((,j)\\) \\(\\Theta\\).Blanchard Quah (1989) implemented long-run restrictions small-scale VAR. Two variables considered: GDP unemployment. Consequently, VAR affected two types shocks. Specifically, authors want identify supply shocks (can permanent effect output) demand shocks (permanent effect output).7Blanchard Quah (1989)’s dataset quarterly, spanning period 1950:2 1987:4. VAR features 8 lags. data use:Estimate reduced-form VAR(8) model:Now, let us define loss function (loss) equal zero () \\(BB'=\\Omega\\) (b) element (1,1) \\(\\Theta = (Id - \\Phi)^{-1} B\\) equal zero:(Note: one can use type approach, based loss function, mix short- long-run restrictions.)Figure 3.4 displays resulting IRFs. Note , GDP, cumulate GDP growth IRF, response GDP level.\nFigure 3.4: IRF GDP unemployment demand supply shocks.\n","code":"\nlibrary(AEC)\ndata(BQ)\npar(mfrow=c(1,2))\nplot(BQ$Date,BQ$Dgdp,type=\"l\",main=\"GDP quarterly growth rate\",\n     xlab=\"\",ylab=\"\",lwd=2)\nplot(BQ$Date,BQ$unemp,type=\"l\",ylim=c(-3,6),main=\"Unemployment rate (gap)\",\n     xlab=\"\",ylab=\"\",lwd=2)\nlibrary(vars)\ny <- BQ[,2:3]\nest.VAR <- VAR(y,p=8)\nOmega <- var(residuals(est.VAR))\n# Compute (Id - Phi)^{-1}:\nPhi <- Acoef(est.VAR)\nPHI <- make.PHI(Phi)\nsum.PHI.k <- solve(diag(dim(PHI)[1]) - PHI)[1:2,1:2]\nloss <- function(param){\n  B <- matrix(param,2,2)\n  X <- Omega - B %*% t(B)\n  Theta <- sum.PHI.k[1:2,1:2] %*% B\n  loss <- 10000 * ( X[1,1]^2 + X[2,1]^2 + X[2,2]^2 + Theta[1,1]^2 )\n  return(loss)\n}\nres.opt <- optim(c(1,0,0,1),loss,method=\"BFGS\",hessian=FALSE)\nprint(res.opt$par)## [1]  0.8570358 -0.2396345  0.1541395  0.1921221\nB.hat <- matrix(res.opt$par,2,2)\nprint(cbind(Omega,B.hat %*% t(B.hat)))##             Dgdp       unemp                       \n## Dgdp   0.7582704 -0.17576173  0.7582694 -0.17576173\n## unemp -0.1757617  0.09433658 -0.1757617  0.09433558\nnb.sim <- 40\npar(mfrow=c(2,2));par(plt=c(.15,.95,.15,.8))\nY <- simul.VAR(c=matrix(0,2,1),Phi,B.hat,nb.sim,y0.star=rep(0,2*8),\n               indic.IRF = 1,u.shock = c(1,0))\nplot(cumsum(Y[,1]),type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Demand shock on GDP\")\nplot(Y[,2],type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Demand shock on UNEMP\")\nY <- simul.VAR(c=matrix(0,2,1),Phi,B.hat,nb.sim,y0.star=rep(0,2*8),\n               indic.IRF = 1,u.shock = c(0,1))\nplot(cumsum(Y[,1]),type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Supply shock on GDP\")\nplot(Y[,2],type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Supply shock on UNEMP\")"},{"path":"forecasting.html","id":"forecasting","chapter":"4 Forecasting","heading":"4 Forecasting","text":"Forecasting always important part time series field (De Gooijer Hyndman (2006)). Macroeconomic forecasts done many places: Public Administration (notably Treasuries), Central Banks, International Institutions (e.g. IMF, OECD), banks, big firms. institutions interested point estimates (\\(\\sim\\) likely value) variable interest. also sometimes need measure uncertainty (\\(\\sim\\) dispersion likely outcomes) associated point estimates.8Forecasts produced professional forecasters available web pages:Philly Fed Survey Professional Forecasters.ECB Survey Professional Forecasters.IMF World Economic Outlook.OECD Global Economic Outlook.European Commission Economic Forecasts.formalize forecasting problem? Assume current date \\(t\\). want forecast value variable \\(y_t\\) take date \\(t+1\\) (.e., \\(y_{t+1}\\)) based observation set variables gathered vector \\(x_t\\) (\\(x_t\\) may contain lagged values \\(y_t\\)).forecaster aims minimizing (function ) forecast error. usal consider following (quadratic) loss function:\n\\[\n\\underbrace{\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)}_{\\mbox{Mean square error (MSE)}}\n\\]\n\\(y^*_{t+1}\\) forecast \\(y_{t+1}\\) (function \\(x_t\\)).Proposition 4.1  (Smallest MSE) smallest MSE obtained expectation \\(y_{t+1}\\) conditional \\(x_t\\).Proof. See Appendix 8.4.Proposition 4.2  Among class linear forecasts, smallest MSE obtained linear projection \\(y_{t+1}\\) \\(x_t\\).\nprojection, denoted \\(\\hat{P}(y_{t+1}|x_t):=\\boldsymbol\\alpha'x_t\\), satisfies:\n\\[\\begin{equation}\n\\mathbb{E}\\left( [y_{t+1} - \\boldsymbol\\alpha'x_t]x_t \\right)=\\mathbf{0}.\\tag{4.1}\n\\end{equation}\\]Proof. Consider function \\(f:\\) \\(\\boldsymbol\\alpha \\rightarrow \\mathbb{E}\\left( [y_{t+1} - \\boldsymbol\\alpha'x_t]^2 \\right)\\). :\n\\[\nf(\\boldsymbol\\alpha) = \\mathbb{E}\\left( y_{t+1}^2 - 2 y_t x_t'\\boldsymbol\\alpha + \\boldsymbol\\alpha'x_t x_t'\\boldsymbol\\alpha] \\right).\n\\]\n\\(\\partial f(\\boldsymbol\\alpha)/\\partial \\boldsymbol\\alpha = \\mathbb{E}(-2 y_{t+1} x_t + 2 x_t x_t'\\boldsymbol\\alpha)\\). function minimised \\(\\partial f(\\boldsymbol\\alpha)/\\partial \\boldsymbol\\alpha =0\\).Eq. (4.1) implies \\(\\mathbb{E}\\left( y_{t+1}x_t \\right)=\\mathbb{E}\\left(x_tx_t' \\right)\\boldsymbol\\alpha\\). (Note \\(x_t x_t'\\boldsymbol\\alpha=x_t (x_t'\\boldsymbol\\alpha)=(\\boldsymbol\\alpha'x_t) x_t'\\).)Hence, \\(\\mathbb{E}\\left(x_tx_t' \\right)\\) nonsingular,\n\\[\\begin{equation}\n\\boldsymbol\\alpha=[\\mathbb{E}\\left(x_tx_t' \\right)]^{-1}\\mathbb{E}\\left( y_{t+1}x_t \\right).\\tag{4.2}\n\\end{equation}\\]MSE :\n\\[\n\\mathbb{E}([y_{t+1} - \\boldsymbol\\alpha'x_t]^2) = \\mathbb{E}{(y_{t+1}^2)} - \\mathbb{E}\\left( y_{t+1}x_t' \\right)[\\mathbb{E}\\left(x_tx_t' \\right)]^{-1}\\mathbb{E}\\left(x_ty_{t+1} \\right).\n\\]Consider regression \\(y_{t+1} = \\boldsymbol\\beta'\\mathbf{x}_t + \\varepsilon_{t+1}\\). OLS estimate :\n\\[\n\\mathbf{b} = \\left[ \\underbrace{ \\frac{1}{T} \\sum_{=1}^T \\mathbf{x}_t\\mathbf{x}_t'}_{\\mathbf{m}_1} \\right]^{-1}\\left[  \\underbrace{ \\frac{1}{T} \\sum_{=1}^T \\mathbf{x}_t'y_{t+1}}_{\\mathbf{m}_2} \\right].\n\\]\n\\(\\{x_t,y_t\\}\\) covariance-stationary ergodic second moments sample moments (\\(\\mathbf{m}_1\\) \\(\\mathbf{m}_2\\)) converges probability associated population moments \\(\\mathbf{b} \\overset{p}{\\rightarrow} \\boldsymbol\\alpha\\) (\\(\\boldsymbol\\alpha\\) defined Eq. (4.2)).Example 4.1  (Forecasting MA(q) process) Consider MA(q) process:\n\\[\ny_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q},\n\\]\n\\(\\{\\varepsilon_t\\}\\) white noise sequence (Def. 1.1).:9\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}(y_{t+h}|\\varepsilon_{t},\\varepsilon_{t-1},\\dots) =\\\\\n&&\\left\\{\n\\begin{array}{lll}\n\\mu + \\theta_h \\varepsilon_{t} + \\dots + \\theta_q \\varepsilon_{t-q+h}  \\quad && \\quad h \\[1,q]\\\\\n\\mu \\quad && \\quad h > q\n\\end{array}\n\\right.\n\\end{eqnarray*}\\]\n\n\\[\\begin{eqnarray*}\n&&\\mathbb{V}ar(y_{t+h}|\\varepsilon_{t},\\varepsilon_{t-1},\\dots)= \\mathbb{E}\\left( [y_{t+h} - \\mathbb{E}(y_{t+h}|\\varepsilon_{t},\\varepsilon_{t-1},\\dots)]^2 \\right) =\\\\\n&&\\left\\{\n\\begin{array}{lll}\n\\sigma^2(1+\\theta_1^2+\\dots+\\theta_{h-1}^2) \\quad && \\quad h \\[1,q]\\\\\n\\sigma^2(1+\\theta_1^2+\\dots+\\theta_q^2) \\quad && \\quad h>q.\n\\end{array}\n\\right.\n\\end{eqnarray*}\\]Example 4.2  (Forecasting AR(p) process) (See web interface.) Consider AR(p) process:\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\varepsilon_t,\n\\]\n\\(\\{\\varepsilon_t\\}\\) white noise sequence (Def. 1.1).Using notation Eq. (2.4), :\n\\[\n\\mathbf{y}_t - \\boldsymbol\\mu = F (\\mathbf{y}_{t-1}- \\boldsymbol\\mu) + \\boldsymbol\\xi_t,\n\\]\n\\(\\boldsymbol\\mu = [\\mu,\\dots,\\mu]'\\) (\\(\\mu\\) defined Eq. (2.8)). Hence:\n\\[\n\\mathbf{y}_{t+h} - \\boldsymbol\\mu = \\boldsymbol\\xi_{t+h} + F \\boldsymbol\\xi_{t+h-1} + \\dots + F^{h-1} \\boldsymbol\\xi_{t+1} + F^h (\\mathbf{y}_{t}- \\mu).\n\\]\nTherefore:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\mathbf{y}_{t+h}|y_{t},y_{t-1},\\dots) &=& \\boldsymbol\\mu + F^{h}(\\mathbf{y}_t - \\boldsymbol\\mu)\\\\\n\\mathbb{V}ar\\left( [\\mathbf{y}_{t+h} - \\mathbb{E}(\\mathbf{y}_{t+h}|y_{t},y_{t-1},\\dots)] \\right) &=& \\Sigma + F\\Sigma F' + \\dots + F^{h-1}\\Sigma (F^{h-1})',\n\\end{eqnarray*}\\]\n:\n\\[\n\\Sigma = \\left[\n\\begin{array}{ccc}\n\\sigma^2  & 0& \\dots\\\\\n0  & 0 & \\\\\n\\vdots  & & \\ddots \\\\\n\\end{array}\n\\right].\n\\]Alternative approach: Taking (conditional) expectations sides \n\\[\ny_{t+h} - \\mu = \\phi_1 (y_{t+h-1} - \\mu) + \\phi_2 (y_{t+h-2} - \\mu) + \\dots + \\phi_p (y_{t-p} - \\mu) + \\varepsilon_{t+h},\n\\]\nobtain:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(y_{t+h}|y_{t},y_{t-1},\\dots) &=& \\mu + \\phi_1\\left(\\mathbb{E}[y_{t+h-1}|y_{t},y_{t-1},\\dots] - \\mu\\right)+\\\\\n&&\\phi_2\\left(\\mathbb{E}[y_{t+h-2}|y_{t},y_{t-1},\\dots] - \\mu\\right) + \\dots +\\\\\n&& \\phi_p\\left(\\mathbb{E}[y_{t+h-p}|y_{t},y_{t-1},\\dots] - \\mu\\right),\n\\end{eqnarray*}\\]\ncan exploited recursively.recursion begins \\(\\mathbb{E}(y_{t-k}|y_{t},y_{t-1},\\dots)=y_{t-k}\\) (\\(k \\ge 0\\)).Example 4.3  (Forecasting ARMA(p,q) process) Consider process:\n\\[\\begin{equation}\ny_t = c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q},\\tag{4.3}\n\\end{equation}\\]\n\\(\\{\\varepsilon_t\\}\\) white noise sequence (Def. 1.1). assume MA part process invertible (see Eq. (2.23)), implies information contained \\(\\{y_{t},y_{t-1},y_{t-2},\\dots\\}\\) identical \\(\\{\\varepsilon_{t},\\varepsilon_{t-1},\\varepsilon_{t-2},\\dots\\}\\).one use recursive algorithm compute conditional mean (Example 4.2), convenient employ Wold decomposition process (see Theorem 2.2 Prop. 2.7 computation \\(\\psi_i\\)’s context ARMA processes):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\ny_t = \\mu + \\sum_{=0}^{+\\infty} \\psi_i \\varepsilon_{t-}.\n\\]\nimplies:\n\\[\\begin{eqnarray*}\ny_{t+h} &=& \\mu + \\sum_{=0}^{h-1} \\psi_i \\varepsilon_{t+h-} + \\sum_{=h}^{+\\infty} \\psi_i \\varepsilon_{t+h-}\\\\\n&=& \\mu + \\sum_{=0}^{h-1} \\psi_i \\varepsilon_{t+h-} + \\sum_{=0}^{+\\infty} \\psi_{+h} \\varepsilon_{t-}.\n\\end{eqnarray*}\\]Since \\(\\mathbb{E}(y_{t+h}|y_t,y_{t-1},\\dots)=\\mu+\\sum_{=0}^{+\\infty} \\psi_{+h} \\varepsilon_{t-}\\), get:\n\\[\n\\mathbb{V}ar(y_{t+h}|y_t,y_{t-1},\\dots) =\\mathbb{V}ar\\left(\\sum_{=0}^{h-1} \\psi_i \\varepsilon_{t+h-}\\right)= \\sigma^2 \\sum_{=0}^{h-1} \\psi_i^2.\n\\]use previous formulas practice?One first select specification estimate model.\nTwo methods determine relevant specifications:Information criteria (see Definition 2.11).Box-Jenkins approach.Box Jenkins (1976) proposed approach now widely used.Data transformation. data transformed “make stationary”. , one can e.g. take logarithms, take changes considered series, remove (deterministic) trends.Select \\(p\\) \\(q\\). can based PACF approach (see Section 2.4), selection criteria (see Definition 2.11).Estimate model parameters. See Section 2.8.Check estimated model consistent data. See .Assessing performances forecasting modelOnce one fitted model given dataset (length \\(T\\), say), one compute MSE (mean square errors) evaluate performance model. MSE -sample one. easy reduce -sample MSE. Typically, model estimated OLS, adding covariates mechanically reduces MSE (see Props. ?? ??). , even additional data irrelevant, \\(R^2\\) regression increases. Adding irrelevant variables increases (-sample) \\(R^2\\) bound increase --sample MSE.Therefore, important analyse --sample performances forecasting model:Estimate model sample reduced size (\\(1,\\dots,T^*\\), \\(T^*<T\\))Use remaining available periods (\\(T^*+1,\\dots,T\\)) compute --sample forecasting errors (compute MSE). --sample exercise, important make sure data used produce forecasts (date \\(T^*\\)) indeed available date \\(T^*\\).Diebold-Mariano testHow compare different forecasting approaches? Diebold Mariano (1995) proposed simple test address question.Assume want compare approaches B. historical data sets implemented approaches past, providing two sets forecasting errors: \\(\\{e^{}_t\\}_{t=1,\\dots,T}\\) \\(\\{e^{B}_t\\}_{t=1,\\dots,T}\\).may case forecasts serve specific purpose , instance, dislike positive forecasting errors care less negative errors. assume able formalise means loss function \\(L(e)\\). instance:dislike large positive errors, may set \\(L(e)=\\exp(e)\\).concerned positive negative errors (indifferently), may set \\(L(e)=e^2\\) (standard approach).Let us define sequence \\(\\{d_t\\}_{t=1,\\dots,T} \\equiv \\{L(e^{}_t)-L(e^{B}_t)\\}_{t=1,\\dots,T}\\) assume sequence covariance stationary. consider following null hypothesis: \\(H_0:\\) \\(\\bar{d}=0\\), \\(\\bar{d}\\) denotes population mean \\(d_t\\)s. \\(H_0\\) assumption covariance-stationarity \\(d_t\\), (Theorem 1.1):\n\\[\n\\sqrt{T} \\bar{d}_T \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\sum_{j=-\\infty}^{+\\infty} \\gamma_j \\right),\n\\]\n\\(\\gamma_j\\)s autocovariances \\(d_t\\).Hence, assuming \\(\\hat{\\sigma}^2\\) consistent estimate \\(\\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\) (instance one given Newey-West formula, see Def. (1.6)), , \\(H_0\\):\n\\[\nDM_T := \\sqrt{T}\\frac{\\bar{d}_T}{\\sqrt{\\hat{\\sigma}^2}} \\overset{d}{\\rightarrow}  \\mathcal{N}(0,1).\n\\]\n\\(DM_T\\) test statistics. test size \\(\\alpha\\), critical region :10\n\\[\n]-\\infty,-\\Phi^{-1}(1-\\alpha/2)] \\cup [\\Phi^{-1}(1-\\alpha/2),+\\infty[,\n\\]\n\\(\\Phi\\) c.d.f. standard normal distribution.Example 4.4  (Forecasting Swiss GDP growth) use long historical time series Swiss GDP growth taken Jordà, Schularick, Taylor (2017) dataset (see Figure 1.4, Example 2.4).want forecast GDP growth. envision two specifications : AR(1) specification (one advocated AIC criteria, see Example 2.4), ARMA(2,2) specification. interested 2-year-ahead forecasts (.e., \\(h=2\\) since data yearly).alternative = \"greater\" alternative hypothesis method 2 accurate method 1. Since reject null (p-value 0.795), led use sophisticated model (ARMA(2,2)) keep simple AR(1) model.Assume now want compare AR(1) process VAR model (see Def. 3.1). consider bivariate VAR, GDP growth complemented CPI-based inflation rate., find alternative model (VAR(1) model) better AR(1) model forecast GDP growth.","code":"\nlibrary(AEC)\nlibrary(forecast)## Registered S3 method overwritten by 'quantmod':\n##   method            from\n##   as.zoo.data.frame zoo\ndata <- subset(JST,iso==\"CHE\")\nT <- dim(data)[1]\ny <- c(NaN,log(data$gdp[2:T]/data$gdp[1:(T-1)]))\nfirst.date <- T-50\ne1 <- NULL; e2 <- NULL;h<-2\nfor(T.star in first.date:(T-h)){\n  estim.model.1 <- arima(y[1:T.star],order=c(1,0,0))\n  estim.model.2 <- arima(y[1:T.star],order=c(2,0,2))\n  e1 <- c(e1,y[T.star+h] - predict(estim.model.1,n.ahead=h)$pred[h])\n  e2 <- c(e2,y[T.star+h] - predict(estim.model.2,n.ahead=h)$pred[h])\n}\nres.DM <- dm.test(e1,e2,h = h,alternative = \"greater\")\nres.DM## \n##  Diebold-Mariano Test\n## \n## data:  e1e2\n## DM = -0.82989, Forecast horizon = 2, Loss function power = 2, p-value =\n## 0.7946\n## alternative hypothesis: greater\nlibrary(vars)\ninfl <- c(NaN,log(data$cpi[2:T]/data$cpi[1:(T-1)]))\ny_var <- cbind(y,infl)\ne3 <- NULL\nfor(T.star in first.date:(T-h)){\n  estim.model.3 <- VAR(y_var[2:T.star,],p=1)\n  e3 <- c(e3,y[T.star+h] - predict(estim.model.3,n.ahead=h)$fcst$y[h,1])\n}\nres.DM <- dm.test(e1,e2,h = h,alternative = \"greater\")\nres.DM## \n##  Diebold-Mariano Test\n## \n## data:  e1e2\n## DM = -0.82989, Forecast horizon = 2, Loss function power = 2, p-value =\n## 0.7946\n## alternative hypothesis: greater"},{"path":"NonStat.html","id":"NonStat","chapter":"5 Non-stationary processes","heading":"5 Non-stationary processes","text":"time series analysis, nonstationarity several crucial implications. Indeed, various time-series regression procedures reliable anymore processes nonstationary.different reasons process can nonstationary. Two examples trends breaks. Generally speaking, trend persistent long-term movement variable time. linear trend simple example (deterministic) trend. say process \\(y_t\\) stationary around linear trend given :\n\\[\ny_t = + bt + x_t,\n\\]\n\\(x_t\\) stationary process. “trends” may also stochastic. typical example stochastic trend random walk:\n\\[\nw_t = w_{t-1} + \\varepsilon_t,\n\\]\n\\(\\varepsilon_t\\) sequence ..d. mean-zero shocks variance \\(\\sigma^2\\).see, \\(y_t = w_t + x_t\\), \\(w_t\\) random walk, use \\(y_t\\) econometric specifications operated carefully. Typically, shall see, standard inference linear regression models may longer valid since \\(y_t\\) features unconditional moments.\n\\[\n\\mathbb{V}ar_t(y_{t+h})=\\mathbb{V}ar_t(\\varepsilon_{t+h})+\\dots+\\mathbb{V}ar_t(\\varepsilon_{t+1})=h\\sigma^2.\n\\]\nUsing law total variance (assuming \\(\\mathbb{V}ar(y_t)\\) exists), , \\(h\\):\n\\[\\begin{equation}\n\\mathbb{V}ar(y_t) = \\mathbb{E}[\\mathbb{V}ar_{t-h}(y_{t})]+\\mathbb{V}ar[\\mathbb{E}_{t-h}(y_{t})].\\tag{5.1}\n\\end{equation}\\]\nalso \\(\\mathbb{E}_{t-h}(y_t)=y_{t-h}\\) \\(\\mathbb{V}ar_t(y_{t+h})=\\mathbb{V}ar_t(\\varepsilon_{t+h})+\\dots+\\mathbb{V}ar_t(\\varepsilon_{t+1})=h\\sigma^2\\). Therefore, Eq. (5.1) gives:\n\\[\n\\mathbb{V}ar(y_t) = h\\sigma^2 + \\mathbb{V}ar(y_{t-h}).\n\\]\nSince previous equation needs satisfied \\(h\\) (even infintely large ones), since \\(\\mathbb{V}ar(y_{t-h})>0\\), \\(\\mathbb{V}ar(y_t)\\) can finite. None population moments random walk actually defined.","code":""},{"path":"NonStat.html","id":"issues-when-working-with-nonstationary-time-series","chapter":"5 Non-stationary processes","heading":"5.1 Issues when working with nonstationary time series","text":"Let us discuss three issues associated nonstationary processes:\n1. Bias autoregressive coefficient towards 0 (context: autoregressions).\n2. Even large samples, \\(t\\)-statistics approximated normal distribution (context: OLS regressions).\n3. Spurious regressions: OLS-based regression analysis tends indicate relationships independent nonstationary (unit-root) series.","code":""},{"path":"NonStat.html","id":"the-bias-of-autoregressive-coefficient-towards-zero","chapter":"5 Non-stationary processes","heading":"5.1.1 The bias of autoregressive coefficient towards zero","text":"Consider non-stationary \\(y_t\\) process (e.g., \\(y_t\\) follows random walk). estimate \\(\\phi\\) (OLS) regression \\(y_t = c + \\phi y_{t-1} + \\varepsilon_t\\) biased toward zero. particular, approximation \\(\\mathbb{E}(\\phi)=1-5.3/T\\), \\(T\\) sample size (see, e.g., Abadir (1993)). 5th percentile distribution \\(\\phi\\) approximately \\(1 - 14.1/T\\), e.g., 0.824 \\(T=80\\). notably poses problems forecasts. Indeed, \\(\\mathbb{E}_t(y_{t+h})=y_t\\) \\(y_t\\) follows random walk, someone fit AR(1) process obtain instance \\(\\hat\\phi=0.824\\) obtain \\(\\mathbb{E}_t(y_{t+10})=0.144y_t\\).illustrated Figure 5.1. figure shows, black, distributions \\(\\hat\\phi\\), obtained OLS context linear model \\(y_t = c + \\phi y_{t-1} + \\varepsilon_t\\). distributions obtained simulations. consider two sample sizes (\\(T=50\\), left plot, \\(T=200\\), right plot). vertical red bars indicate means distributions, vertical blue bar gives approximated mean given \\(1-5.3/T\\).\nFigure 5.1: densities based 1000 simulations samples length \\(T\\), approximated kernel approach.\n","code":""},{"path":"NonStat.html","id":"spurious-regressions","chapter":"5 Non-stationary processes","heading":"5.1.2 Spurious regressions","text":"Consider two independent non-stationary (unit roots) variables. regress one , use standard OLS formulas compute standard deviation regression parameter, can shown tend underestimate standard deviation. result, use standard \\(t\\)-statistics test existence relationship two variables, often false positive, .e., often reject null hypothesis relationship (\\(H_0\\): regression coefficient = 0). phenomenon called spurious regressions (see, e.g., examples).situation illustrated Figure 5.2. simulate two independent random walks, \\(x_t\\) \\(y_t\\), regress \\(y_t\\) \\(x_t\\) OLS. (consider two different sample sizes, \\(T=50\\) \\(T=200\\).) black lines represent densities estimated \\(\\beta\\)’s, slope coefficients regressions. blue density represent asymptotic distribution \\(\\beta\\) based standard OLS formula (normal distribution mean zero variance \\(\\hat\\sigma^2/\\widehat{\\mathbb{V}ar}(x_t)\\)). fact latter distribution features smaller variance former implies using standard OLS inference misleading, distribution overestimates accuracy estimator.\nFigure 5.2: densities based 1000 simulations samples length \\(T\\), approximated kernel approach.\n","code":""},{"path":"NonStat.html","id":"nonstationarity-tests","chapter":"5 Non-stationary processes","heading":"5.2 Nonstationarity tests","text":"Hence, employing (OLS) regressions non-stationary variables may misleading. therefore important, employing techniques (OLS), check variables stationary. , specific tests used. tests called stationarity non-stationarity (unit-root) tests. context stationarity test, null hypothesis \\(y_t\\) stationary trend-stationary (.e. equal sum linear trend stationary component). context non-stationarity (unit root) tests, null hypothesis \\(y_t\\) (trend) stationary.illustrate, consider AR(1) case:\n\\[\\begin{equation}\ny_t = \\phi y_{t-1} + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0,1).\\tag{5.2}\n\\end{equation}\\]\\(y_t\\) stationary (.e. \\(|\\phi|<1\\), Prop. 2.3), can shown (Hamilton (1994), p.216) :\n\\[\n\\sqrt{T}(\\phi^{OLS}-\\phi) \\overset{d}{\\rightarrow}  \\mathcal{N}(0,1-\\phi^2).\n\\]\nprevious equation make sense \\(\\phi=1\\).Even standard OLS results valid non-stationary case, many (non)stationary tests make use statistics used standard OLS analysis. Even \\(t\\)-statistic admit Student-\\(t\\) distribution , one can still compute statistic. idea behind severeal unit-root tests simply determine distribution OLS-based statistics null hypothesis non-stationarity. Let us define \\(H_0\\) follows:\n\\[\nH_0:\\quad \\phi = 1 \\quad \\quad H_1: \\quad |\\phi| < 1.\n\\]Test statistic:\n\\[\nt_{\\phi=1} = \\frac{\\phi^{OLS}-1}{\\sigma^{OLS}_{\\phi}},\n\\]\n\\(\\phi^{OLS}\\) OLS estimate \\(\\phi\\) \\(\\sigma^{OLS}_{\\phi}\\) usual OLS-based standard error estimate \\(\\phi^{OLS}\\).Importantly, noted , null hypothesis, \\(t_{\\phi=1}\\) distributed Student-t variable (see Def. 8.10).Theorem 5.1  (Convergence results) \\(y_t\\) follows Eq. (5.2) (\\(\\mathbb{V}ar(\\varepsilon_t)=\\sigma^2\\)) \\(\\phi=1\\), :\n\\[\\begin{eqnarray}\nT^{-3/2} \\sum_{t=1}^{T} y_{t} &\\overset{d}{\\rightarrow}& \\sigma \\int_{0}^{1}W(r)dr\\\\\nT^{-2} \\sum_{t=1}^{T} y_{t}^2 &\\overset{d}{\\rightarrow}& \\sigma^2 \\int_{0}^{1}W(r)^2dr\\\\\nT^{-1} \\sum_{t=1}^{T} y_{t-1}\\varepsilon_{t} &\\overset{d}{\\rightarrow}& \\sigma^2 \\int_{0}^{1}W(r)dW(r),\n\\end{eqnarray}\\]\n\\(W(r)\\) denotes standard Brownian motion (Wiener process) defined unit interval.Proof. See P. C. B. Phillips (1987) Hamilton (1994) (Subsection 17.4).Theorem 5.1 notably implies , \\(\\phi=1\\), :\n\\[\\begin{eqnarray}\nT(\\phi^{OLS}-1) &\\overset{d}{\\rightarrow}& \\frac{\\int_{0}^{1}W(r)dW(r)}{\\int_{0}^{1}W(r)^2dr} = \\frac{\\frac{1}{2}(W(1)^2-1)}{\\int_{0}^{1}W(r)^2dr}\\tag{5.3}\\\\\nt_{\\phi=1} &\\overset{d}{\\rightarrow}& \\frac{\\int_{0}^{1}W(r)dW(r)}{\\left(\\int_{0}^{1}W(r)^2dr\\right)^{1/2}}= \\frac{\\frac{1}{2}(W(1)^2-1)}{\\left(\\int_{0}^{1}W(r)^2dr\\right)^{1/2}}\\tag{5.4}\n\\end{eqnarray}\\]previous theorem notably implies convergence rate \\(\\phi^{OLS}\\) (\\(T\\)) faster \\(y_t\\) stationary (case \\(\\sqrt{T}\\)). also implies , asymptotically, () \\(\\phi^{OLS}\\) normally distributed (b) \\(t_{\\phi=1}\\) standard normal.limiting distribution \\(t_{\\phi=1}\\) closed form; called Dickey-Fuller distribution.Although available closed form (evaluated numerically), distribution \\(T(\\phi^{OLS}-1)\\) can used test null hypothesis \\(\\phi=1\\).Nonstationarity tests: trends againThe right consideration potential trends \\(y_t\\)’s specification crucial. focus two types null hypotheses:Constant . Specification:\n\\[\ny_t = c + \\phi y_{t-1} + \\varepsilon_t.\n\\]\n\\(|\\phi|<1\\): \\(y_t\\) stationary (note \\((0)\\)) non-zero mean.Constant + Linear trend. Specification:\n\\[\ny_t = c + \\delta t + \\phi y_{t-1} + \\varepsilon_t.\n\\]\n\\(|\\phi|<1\\): \\(y_t\\) stationary around deterministic time trend.far, focused AR(1) processes. many time series complicated dynamic structure.Dickey-Fuller (DF) testThe specification underlying Dickey Fuller (1979) test :\n\\[\\begin{equation}\ny_t = \\boldsymbol\\beta'D_t + \\phi y_{t-1} + \\sum_{=1}^{p}\\psi_i \\Delta y_{t-} + \\varepsilon_t,\\tag{5.5}\n\\end{equation}\\]\n\\(D_t\\) vector deterministic trends (\\(D_t = 1\\) \\(D_t = [1,t]'\\)) \\(p\\) estimated \\(\\varepsilon_t\\) serially uncorrelated.null hypothesis DF test : \\(\\phi=1\\). Test statistics:\n\\[\\begin{eqnarray*}\n\\mbox{ADF bias statistic: } && ADF_\\pi =T(\\phi^{OLS}-1)\\\\\n\\mbox{ADF $t$ statistic: } && ADF_t = t_{\\phi=1} = \\frac{\\phi^{OLS}-1}{\\sigma^{OLS}_{\\phi}}\n\\end{eqnarray*}\\]null hypothesis (\\(\\phi=1\\)) regression Eq. (5.2) limiting distributions statistics , respectively, Eq. (5.3) Eq. (5.4).Alternative formulation Eq. (5.5):\n\\[\n\\Delta y_t = \\boldsymbol\\beta' D_t + \\pi y_{t-1} + \\sum_{=1}^{p}\\psi_i \\Delta y_{t-} + \\varepsilon_t,\n\\]\n\\(\\pi = \\phi - 1\\). null hypothesis \\(\\pi = 0\\). case:\n\\[\\begin{eqnarray*}\n\\mbox{ADF bias statistic: } && ADF_\\pi = T\\pi^{OLS}\\\\\n\\mbox{ADF $t$ statistic: } && ADF_t = t_{\\pi=0} = \\frac{\\pi^{OLS}}{\\sigma^{OLS}_{\\pi}}\n\\end{eqnarray*}\\]null hypothesis (\\(\\phi=1\\)) regression Eq. (5.2) limiting distributions statistics , respectively, Eq. (5.3) Eq. (5.4).selection \\(p\\) can rely information criteria (see Def. 2.11). Alternatively, Schwert (1989) proposes use:\n\\[\np=\\left[12 \\times \\left( \\frac{T}{100} \\right)^{1/4}\\right].\n\\]Importantly, test one-sided left-tailed test: one rejects null test statistics sufficiently negative; therefore interested first quantiles limit distribution.Phillips-Perron (PP) testThe regression underyling P. Phillips Perron (1988) test follows:\n\\[\n\\Delta y_t = \\boldsymbol\\beta' D_t + \\pi y_{t-1} + \\varepsilon_t.\n\\]\nissue serial correlation (heteroskedasticity) residual handled adjusting test statistics \\(t_{\\pi=0}\\) \\(T\\pi\\):11\\[\\begin{eqnarray*}\n\\mbox{PP $t$ stat.: } && Z_t = \\sqrt{\\frac{\\hat\\gamma_{0,T}}{\\hat\\lambda_T^2}} t_{\\pi=0,T} - \\frac{\\hat\\lambda_T^2-\\hat\\gamma_{0,T}}{2\\hat\\lambda_T}\\left(\\frac{T\\sigma^{OLS}_{\\pi,T}}{s_T}\\right)\\\\\n\\mbox{PP bias stat.: } && Z_\\pi = T\\pi^{OLS}_T - \\frac{1}{2}(\\hat\\lambda^2-\\hat\\gamma_{0,T}^2)\\left(\\frac{T \\sigma^{OLS}_{\\pi,T}}{s^2_T}\\right)^2\n\\end{eqnarray*}\\]\n\n\\[\\begin{eqnarray*}\n\\hat\\gamma_{j,T} &=& \\frac{1}{T}\\Sigma_{t=j+1}^{T}\\hat{\\varepsilon}_t\\hat{\\varepsilon}_{t-j}\\\\\n\\hat{\\varepsilon}_t &=& \\mbox{OLS residuals}\\\\\n\\hat\\lambda_T^2 &=& \\hat\\gamma_{0,T} + 2 \\Sigma_{j=1}^{q}\\left(1-\\frac{j}{q+1}\\right) \\hat\\gamma_{j,T} \\quad \\mbox{(Newey-West formula)}\\\\\ns_T^2 &=& \\frac{1}{T-k} \\Sigma_{t=1}^{T} \\hat{\\varepsilon}^2_t \\quad \\mbox{($k$: number param. estim. regression)}\\\\\n\\sigma^{OLS}_{\\pi,T} &=& \\mbox{OLS standard error $\\pi$}.\n\\end{eqnarray*}\\]underlying regression : \\(y_t = \\alpha + \\phi y_{t-1} + \\varepsilon_t\\), null \\(\\alpha=0\\) \\(\\phi=1\\), :limiting distribution \\(Z_\\pi\\) (see, e.g., Hamilton (1994) 17.6.8):\n\\[\n\\frac{\\frac{1}{2}\\{W(1)^2 - 1\\} - W(1)\\int_0^1 W(r)dr}{\\int_0^1 W(r)^2dr-\\left[\\int_0^1 W(r)dr\\right]^2};\n\\]limiting distribution \\(Z_t\\) (see, e.g., Hamilton (1994) 17.6.12):\n\\[\n\\frac{\\frac{1}{2}\\{W(1)^2 - 1\\} - W(1)\\int_0^1 W(r)dr}{\\left(\\int_0^1 W(r)^2dr-\\left[\\int_0^1 W(r)dr\\right]^2\\right)^2}.\n\\]\\(|\\phi|<1\\), OLS estimate \\(\\phi^{OLS}_T\\) consistent \\(\\varepsilon_t\\)s serially correlated (true residuals regressors correlated). \\(\\phi=1\\), rate convergence \\(\\phi^{OLS}_T\\) \\(T\\) (super-consistency), ensures \\(\\phi^{OLS}_T \\overset{p}{\\rightarrow} 1\\) even \\(\\varepsilon_t\\)s serially correlated.ADF test, test one-sided left-tailed (reject null test statistics sufficiently negative). critical values obtained simulation; can instance found .","code":""},{"path":"NonStat.html","id":"stationarity-test-kpss","chapter":"5 Non-stationary processes","heading":"5.3 Stationarity test: KPSS","text":"test proposed Kwiatkowski et al. (1992) stationarity test, .e., null hypothesis, process stationary. underlying specificaiton following:\n\\[\ny_t = \\boldsymbol\\beta' D_t + \\mu_t + \\varepsilon_t,\n\\]\n\\(\\mu_t = \\mu_{t-1} + \\eta_t\\), \\(\\mathbb{V}ar(\\eta_t)=\\sigma_\\eta^2\\), \\(D_t = 1\\) \\(D_t = [1,t]'\\) \\(\\{\\varepsilon_t\\}\\) covariance-stationary sequence.KPSS statistic corresponds Lagrange Multiplier test statistic associated hypothesis \\(\\sigma_\\eta^2=0\\):\n\\[\n\\boxed{\\xi^{KPSS}_T = \\left(\\frac{1}{\\hat\\lambda_T^2 T^2}\\sum_{t=1}^T\\hat{S}_t^2\\right),}\n\\]\n\\(\\hat{S}_t=\\Sigma_{=1}^t \\hat{\\varepsilon}_i\\), \\(\\hat{\\varepsilon}_t\\)s residuals OLS regression \\(y_t\\) \\(D_t\\), \\(\\hat\\lambda^2\\) consistent estimate long-run variance \\(\\hat{\\varepsilon}_t\\) (see Def. 1.7 Newey-West approach, see Eq. (1.6)).KPSS show , null hypothesis, \\(\\xi^{KPSS}_T\\) converges distribution towards distribution depend \\(\\boldsymbol\\beta\\) form \\(D_t\\). Specifically:\\(D_t = 1\\):\n\\[\n\\xi^{KPSS}_T \\overset{d}{\\rightarrow} \\int_{0}^1 (W(r)-rW(1))dr.\n\\]\\(D_t = [1,t]'\\):\n\\[\n\\xi^{KPSS}_T \\overset{d}{\\rightarrow} \\int_{0}^1 \\left\\{W(r) + r(2-3r)W(1) + 6r(r^2-1)\\int_{0}^{1}W(s)ds\\right\\}dr.\n\\]test one-sided right-tailed test: one rejects null \\(\\xi^{KPSS}_T\\) \\((1-\\alpha)\\) quantile limit distribution. critical values car found, e.g., .Example 5.1  (Stationarity inflation interest rates) Let us use quarterly US data test stationarity inflation 3-month short-term rate. Let us first plot data:\nFigure 5.3: US Inflation (black) short-term nominal rates (red).\nLet us now run tests. Note default alternative hypothesis function adf.test package tseries process trend-stationary. Note kpss.test returns p-value 0.01, means true p-value lower .","code":"\nlibrary(tseries)\ntest.adf.infl <- adf.test(US3var$infl,k=4)\ntest.adf.r    <- adf.test(US3var$r,k=4)\ntest.pp.infl <- pp.test(US3var$infl)\ntest.pp.r    <- pp.test(US3var$r)\ntest.kpss.infl <- kpss.test(US3var$infl)\ntest.kpss.r    <- kpss.test(US3var$r)\nc(test.adf.infl$p.value,test.pp.infl$p.value,test.kpss.infl$p.value)## [1] 0.29230878 0.04978275 0.01000000\nc(test.adf.r$p.value,test.pp.r$p.value,test.kpss.r$p.value)## [1] 0.3837577 0.3614365 0.0100000"},{"path":"cointeg.html","id":"cointeg","chapter":"6 Introduction to cointegration","heading":"6 Introduction to cointegration","text":"","code":""},{"path":"cointeg.html","id":"intuition","chapter":"6 Introduction to cointegration","heading":"6.1 Intuition","text":"Many statistical procedures well-defined processes interest stationary. result, especially one wants investigate joint dynamics different variables, one often begins making data stationary (, e.g., taking first differences removing deterministic trends). However, may remove information data. Heuristically, removing trends amounts filtering long-run variations series. However, may case different variables interact short run long run.instance, left plot Figure 6.1 suggests trends \\(x_t\\) \\(y_t\\) positively correlated. However, right plot shows , low values \\(h\\), correlation \\(x_t - x_{t-h}\\) \\(y_t - y_{t-h}\\) negative. notably case \\(h=1\\), means first differences two variables (.e., \\(\\Delta x_t\\) \\(\\Delta y_t\\)) negatively correlated. Hence, focusing first differences lead researcher think relationship \\(x_t\\) \\(y_t\\) negative one (case one focuses high-frequency comovements two variables).\nFigure 6.1: Situation conditional uncondional correlation \\(x_t\\) \\(y_t\\) sign.\nDefinition 6.1  (Integrated variables) univariate process \\(\\{y_t\\}\\) said \\((d)\\) \\(d^{th}\\) difference stationary (\\((d-1)^{th}\\) difference).instance:\\(y_t\\) stationary \\(\\Delta y_t = y_t - y_{t-1}\\) , \\(y_t\\) \\((1)\\).\\(\\Delta y_t\\) stationary \\(\\Delta^2 y_t=\\Delta(\\Delta y_t)\\) , \\(y_t\\) \\((2)\\).\\(y_t\\) stationary \\(y_t\\) \\((0)\\).","code":""},{"path":"cointeg.html","id":"the-bivariate-case","chapter":"6 Introduction to cointegration","heading":"6.2 The bivariate case","text":"regress \\((1)\\) variable \\(y_t\\) another independent \\((1)\\) variable \\(x_t\\), usual (OLS-based) t-tests regression coefficients often (misleadingly) show statistically significant coefficients (speak spurious regressions, see Section 5). solution regress \\(\\Delta y_t\\) (\\((0)\\)) \\(\\Delta x_t\\) inference correct. However, stated , economic interpretation regression changes, amounts focusing high-frequency movements variables.Let us now consider case \\(y_t\\) \\(x_t\\) still \\((1)\\), satisfy:\n\\[\\begin{equation}\ny_t = \\beta x_t + \\varepsilon_t,\\tag{6.1}\n\\end{equation}\\]\n\\(\\varepsilon_t \\sim (0)\\). , linear combination two \\((1)\\) variables \\((0)\\).case, convergence \\(b\\), OLS estimate \\(\\beta\\), fast. Indeed, convergence rate \\(1/T\\) (versus \\(1/\\sqrt{T}\\) purely stationary case). stems properties non-statinary processes stated Prop. 6.1. :\n\\[\nb_T = \\frac{\\sum_t x_t y_t}{\\sum_t x_t^2} = \\frac{\\sum_t x_t (\\beta x_t + \\varepsilon_t)}{\\sum_t x_t^2}\n= \\beta + \\frac{\\sum_t x_t \\varepsilon_t}{\\sum_t x_t^2}.\n\\]\nparticular, \\(\\varepsilon_t\\) white noise, using properties (ii) (iv) Prop. 6.1, get:\n\\[\\begin{equation}\nb_T \\approx \\beta + \\frac{1}{T}\\frac{\\int_{0}^{1}x_{\\infty}(r)dW^{\\varepsilon}_r}{\\int_{0}^{1}x^2_{\\infty}(r)dr}.\\tag{6.2}\n\\end{equation}\\]\nrandom variables \\(x^2_{\\infty}\\) \\(W^{\\varepsilon}_r\\) defined Prop. 6.1.Proposition 6.1  (Properties (1) process) \\(\\{y_t\\}\\) \\((1)\\) \\(y_t - y_{t-1} = H(L)\\varepsilon_t\\) \\(H(L)\\varepsilon_t\\) \\((0)\\), :\\(\\dfrac{1}{\\sqrt{T}}\\bar{y}_T \\overset{d}{\\rightarrow} \\int_{0}^{1}y_{\\infty}(r)dr\\),\\(\\dfrac{1}{T^2}\\sum_{t=1}^T y_t^2 \\overset{d}{\\rightarrow} \\int_{0}^{1}y^2_{\\infty}(r)dr\\),\\(\\dfrac{1}{T}\\sum_{t=1}^T y_t(y_t-y_{t-1}) \\overset{d}{\\rightarrow} \\dfrac{1}{2}y^2_{\\infty}(1) + \\dfrac{1}{2}\\mathbb{V}ar(y_t - y_{t-1})\\),\\(\\dfrac{1}{T}\\sum_{t=1}^T y_t \\eta_t \\overset{d}{\\rightarrow} \\sigma_\\eta \\int_{0}^{1}y_{\\infty}(r)dW_r^{\\eta}\\) \\(\\eta_t\\) white noise variance \\(\\sigma_\\eta^2\\),\\(y_{\\infty}(r)\\) form \\(\\omega W_r\\) (\\(W_r\\) Brownian motion (see extra material), \\(\\omega = \\sum_{k=-\\infty}^{\\infty} \\gamma_k\\), \\(\\gamma_k\\)s autocovariances \\(H(L)\\varepsilon_t\\), \\(\\eta_{\\infty}(r)\\) form \\(\\sigma_\\eta W^{\\eta}_r\\) (\\(W^{\\eta}_r\\) Brownian motion “associated” \\(\\eta_t\\)).Proof. (1)\n:\n\\[\n\\dfrac{1}{\\sqrt{T}}\\bar{y}_T = \\frac{1}{T}\\sum_{t=1}^{T}\\frac{y_{t}}{\\sqrt{T}} =  \\frac{1}{T}\\sum_{t=1}^{T}\\tilde{y}_T(t/T),\n\\]\n\\(\\tilde{y}_T(r)=(1/\\sqrt{T})y_{[rT]}\\). Now \\(\\tilde{y}_T(r) = r \\sqrt{T} \\left(\\frac{1}{Tr}\\sum_{t=1}^{[Tr]} H(L)\\varepsilon_t\\right)\\). Eq. (1.4) Theorem 1.1, \\(\\sqrt{r}\\sqrt{Tr} \\left(\\frac{1}{Tr}\\sum_{t=1}^{[Tr]} H(L)\\varepsilon_t\\right) \\rightarrow \\omega W_r\\). Therefore, large \\(T\\), \\(\\frac{1}{T}\\sum_{t=1}^{T}\\tilde{y}_T(t/T)\\) approximates \\(\\frac{1}{T}\\sum_{t=1}^{T} \\omega W_{t/T}\\), Riemann sum converges \\(\\int_{0}^{1}y_{\\infty}(r)dr\\).","code":""},{"path":"cointeg.html","id":"multivariate-case-and-vecm","chapter":"6 Introduction to cointegration","heading":"6.3 Multivariate case and VECM","text":"following, consider \\(n\\)-dimensional vector \\(y_t\\). Moreover, \\(\\varepsilon_t\\) \\(n\\)-dimensional white noise process. notion integration (Def. 6.1) can also defined multivariate case:Definition 6.2  (Order integration (multivariate case)) \\(\\{y_t\\}\\) \\((d)\\) \n\\[\\begin{equation}\n(1-L)^dy_t = \\mu + H(L)\\varepsilon_t,\\tag{6.3}\n\\end{equation}\\]\n\\(H(L)\\varepsilon_t\\) stationary process (\\((1-L)^{d-1}y_t\\) ).Definition 6.3  (Cointegration) \\(y_t\\) integrated order \\(d\\), components said cointegrated exists linear combination components \\(y_t\\) integrated order equal , lower , \\(d-1\\).instance, Eq. (6.1) implies \\([1,-\\beta]'\\) cointegrating vector \\([y_t,x_t]'\\) cointegrated.Consider \\((1)\\) process, \\(y_t\\), Wold representation \\(\\Delta y_t\\) Eq. (6.3). :\n\\[\ny_t = \\mu + H(L)\\varepsilon_t + y_{t-1} = t \\mu + H(L)(\\varepsilon_t + \\varepsilon_{t-1} + \\dots + \\varepsilon_1) + y_0.\n\\]can shown :\n\\[\ny_t = t \\mu + H(1)(\\varepsilon_t + \\varepsilon_{t-1} + \\dots + \\varepsilon_1) + \\xi_t,\n\\]\n\\(\\xi_t\\) stationary process.Assume \\(y_t\\) possesses cointegrating vector \\(\\pi\\) \\(\\pi' y_t\\) (univariate) stationary process.Necessarily, must \\(\\pi' \\mu = 0\\) \\(\\pi' H(1)=0\\). Reciprocally, \\(\\pi' \\mu = 0\\) \\(\\pi' H(1)=0\\), \\(\\pi\\) cointegrating vector \\(y_t\\). proves following proposition:Proposition 6.2  (Necessary sufficient conditions cointegration) \\(y_t\\) \\((1)\\) admits Wold representation Eq. (6.3), \\(d=1\\), \\(\\pi\\) cointegrating vector (iff):\n\\[\n\\pi' \\mu = 0 \\mbox{ (scalar equation) }\\pi' H(1)=0\\mbox{ (vectorial equation)}.\n\\]Consider following VAR(p) model, \\(y_t\\) \\((1)\\):\n\\[\\begin{equation}\ny_t = c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\varepsilon_t,\\tag{6.4}\n\\end{equation}\\]\n\\(\\Phi(L)y_t = c + \\varepsilon_t\\) \\(\\Phi(L) = - \\Phi_1 L - \\dots - \\Phi_p L^p\\).Suppose Wold representation \\(\\Delta y_t\\) Eq. (6.3). Premultiplying Eq. (6.3) \\(\\Phi(L)\\) gives:\n\\[\n(1-L)\\Phi(L)y_t = \\Phi(1)\\mu + \\Phi(L)H(L)\\varepsilon_t,\n\\]\n\n\\[\n(1-L)\\varepsilon_t = \\Phi(1)\\mu + \\Phi(L)H(L)\\varepsilon_t.\n\\]\nTaking expectation sides gives \\(\\Phi(1)\\mu=0\\). Therefore, previous equation hold \\(\\varepsilon_t\\), must \n\\[\n(1-L) Id = \\Phi(L)H(L).\n\\]\nprevious equality implies \\((1-z) Id = \\Phi(z)H(z)\\) \\(z\\). particular, \\(z=1\\):\n\\[\n0 = \\Phi(1)H(1).\n\\]Take row \\(\\pi'\\) \\(\\Phi(1)\\). Since \\(\\Phi(1)\\mu=0\\), \\(\\pi'\\mu=0\\) (scalar equation). Since \\(\\Phi(1)H(1)=0\\), \\(\\pi' H(1)=0\\) (vectorial equation). Prop. 6.2 implies rows \\(\\Phi(1)\\) cointegrating vectors \\(y_t\\). Therefore, \\(\\{a_1,\\dots,a_h\\}\\) constitutes basis space cointegrating vectors, \\(\\pi\\) can expressed linear combination \\(a_i\\)’s. , must :\n\\[\n\\pi = [a_1 \\quad a_2 \\quad \\dots \\quad a_h]b = \\underbrace{}_{n\\times h}\\underbrace{b}_{h \\times 1},\n\\]\n\\(= [a_1 \\quad a_2 \\quad \\dots \\quad a_h]\\).Since true rows \\(\\Phi(1)\\), comes matrix form:\n\\[\\begin{equation}\n\\underbrace{\\Phi(1)}_{n\\times n} = \\underbrace{B}_{n\\times h}\\underbrace{'}_{h\\times n}.\\tag{6.5}\n\\end{equation}\\]\nshows number independent cointegrating vectors—order cointegration \\(y_t\\)—rank \\(\\Phi(1)\\). important implications dynamics \\(\\Delta y_t\\).Consider process \\(y_t\\) whose VAR representation Eq. (6.4). VAR representation can rewritten:\n\\[\\begin{equation}\ny_t = (c + \\rho y_{t-1}) + \\zeta_1 \\Delta y_{t-1} + \\dots + \\zeta_{p-1} \\Delta y_{t-p+1} + \\varepsilon_t,\\tag{6.6}\n\\end{equation}\\]\n\\(\\zeta_k = - \\Phi_{k+1} - \\dots - \\Phi_{p}\\) \\(\\rho = \\Phi_1 + \\dots + \\Phi_p\\).Example 6.1  (VAR(2)) VAR(2) \\(y_t = c + \\Phi_1 y_{t-1}+ \\Phi_2 y_{t-2} + \\varepsilon_t\\), :\n\\[\ny_t = c + \\{\\Phi_1 + \\Phi_2\\} y_{t-1} + \\{-\\Phi_2\\} \\Delta y_{t-1} + \\varepsilon_t,\n\\]Subtracting \\(y_{t-1}\\) sides Eq. (6.6) remarking \\(-\\Phi(1) = \\rho - Id\\) (recall \\(\\Phi(L) = - \\Phi_1 L - \\dots - \\Phi_p L^p\\)), get:\n\\[\n\\Delta y_t = \\{c - \\underbrace{\\Phi(1) y_{t-1}}_{=BA'y_{t-1}}\\} + \\zeta_1 \\Delta y_{t-1} + \\dots + \\zeta_{p-1} \\Delta y_{t-p+1} + \\varepsilon_t.\n\\]Using Eq. (6.5), denoting \\(z_t\\) \\(h\\)-dimensional vector \\('y_{t}\\), obtain error correction representation cointegrated variable \\(y_t\\):\n\\[\\begin{equation}\n\\boxed{\\Delta y_t = c - B z_{t-1} + \\zeta_1 \\Delta y_{t-1} + \\dots + \\zeta_{p-1} \\Delta y_{t-p+1} + \\varepsilon_t.}\\tag{6.7}\n\\end{equation}\\]\ntype model called Vector Error Correction Model (VECM). components \\(z_t\\) can considered errors , multiplied components \\(B\\) generate correction forces imply , long run, congregation relationships satisfied. Example 6.2 illustrates .Example 6.2  (VECM example) Consider VAR(1) process \\(y_t\\) follows:\n\\[\ny_t = \\Phi_1 y_{t-1} + \\varepsilon_t = \\left[\\begin{array}{cc}\n0.5 & 0.5 \\\\\n0.2 & 0.8 \\\\\n\\end{array}\n\\right] y_{t-1} + \\varepsilon_t, \\quad \\varepsilon_t \\sim ..d. \\mathcal{N}(0,\\Sigma)\n\\]1 eigenvalue \\(\\Phi_1\\) \\(y_t\\) \\((1)\\). :\n\\[\n\\Phi(1) = Id - \\left[\\begin{array}{cc}\n0.5 & 0.5 \\\\\n0.2 & 0.8 \\\\\n\\end{array}\n\\right] = \\left[\\begin{array}{cc}\n0.5 & -0.5 \\\\\n-0.2 & 0.2 \\\\\n\\end{array}\n\\right],\n\\]\nrank 1. Therefore \\(y_t\\) cointegrated order 1.process, Eq. (6.7) writes:\n\\[\n\\Delta y_{t} =\n- \\left[\\begin{array}{cc}\n0.5 & -0.5 \\\\\n-0.2 & 0.2\n\\end{array}\n\\right] y_{t-1} + \\varepsilon_t =\n\\left[\\begin{array}{c}\n-0.5\\\\\n0.2\n\\end{array}\n\\right] z_{t-1} + \\varepsilon_t,\n\\]\n\\(z_{t} = y_{1,t}-y_{2,t}\\).process \\(z_t\\) stationary (\\(z_t = 0.3 z_{t-1} + \\varepsilon_{1,t} - \\varepsilon_{2,t}\\)).say long-run relationship \\(y_{1,t}\\) \\(y_{2,t}\\):\\(y_{1,t}\\) substantially \\(y_{2,t}\\), \\(z_t\\) large, influence \\(-0.5 z_t\\) \\(\\Delta y_{1,t+1}\\) negative, tends “correct” \\(y_{1,t}\\) brings closer \\(y_{2,t}\\).\nFigure 6.2: Simulation \\(y_{1,t}\\) (blue) \\(y_{2,t}\\) (red).\n","code":""},{"path":"cointeg.html","id":"cointegration-in-practice","chapter":"6 Introduction to cointegration","heading":"6.4 Cointegration in practice","text":"Assume vector variables \\(y_t\\) want investigate joint dynamics components. \\(y_{,t}\\)s \\((1)\\), may need use VECM. Therefore, first place, one test stationarity \\(y_t\\)’s components (see Section 5).components \\(y_t\\) \\((1)\\), one determine existence cointegrating vectors. two general possibilities :theory provides us relevant cointegration relationships, instance:\nPurchasing Power Parity (PPP) suggests existence long-run relationship domestic prices, foreign prices, exchange rate.\nreal rates stationary, Fisher equation (\\(r=-\\pi\\)) implies cointegration relatinsip nominal interest rate (\\(\\)) inflation \\(\\pi\\).\nPurchasing Power Parity (PPP) suggests existence long-run relationship domestic prices, foreign prices, exchange rate.real rates stationary, Fisher equation (\\(r=-\\pi\\)) implies cointegration relatinsip nominal interest rate (\\(\\)) inflation \\(\\pi\\).priori regarding cointegration relationship. estimate (test) potential cointegration relationship.first case, proceed two steps:\n. \\(\\pi^*\\) suspected cointegrating vector, can use unit-root stationarity tests \\(z_t={\\pi^*}' y_t\\) (see Section 5).\nb. Estimate Eq. (6.7) OLS.following, focus second case, one cointegration relationship. (general case, one can instance implement Johansen (1991) approach.)Engle Granger (1987) propose two-step estimation procedure estimate error-correction models. method proceeds assumption single cointegration equation, .e. \\(' = [\\alpha_1,\\dots,\\alpha_n]\\). (Recall \\(\\Phi(1) = BA'\\), see Eq. (6.5).) Without loss generality, one can set \\(\\alpha_1=1\\). case, cointegration relationship, exists, form:\n\\[\ny_{1,t} = - \\alpha_2 y_{2,t} - \\dots - \\alpha_n y_{n,t} + z_t,\n\\]\n\\(z_t \\sim (0)\\).first step consists estimating previous equation OLS (regression \\(y_{1,t}\\) \\(y_{2,t},\\dots,y_{2,t}\\)). second step consists estimating Eq. (6.7) also OLS, replaced \\(z_{t-1}\\) (lagged) residuals first step OLS regression (\\(\\hat{z}_{t-1}\\)). high speed convergence first-step regression (convergence \\(1/T\\), see Eq. (6.2)), asymptotic properties second-step estimates standard ones. , one can use standard t-statistic assess statistical significativity parameters.remains explain test existence cointegration relationship. amounts testing whether \\(z_t\\) stationary. Note however observe “true” \\(z_t\\), OLS-based estimates \\(\\hat{z}_t\\). Therefore, critical values usual unit-root tests . appropriate critical values given P. C. B. Phillips Ouliaris (1990).Example 6.3  (VECM US inflation nominal interest rate) data Example 5.1. first step, compute \\(z_t\\) use P. C. B. Phillips Ouliaris (1990)’s test see whether two variables cointegrated:reject null hypothesis unit root residuals. , results favor cointegration. Let us esitmate VECM model; amounts running two OLS regressions:Table 6.1 reports estimated coefficients eq1 eq2, together p-values:Table 6.1: Results second-stage OLS regressions (Engle-Granger approach). first variable short-term nominal interest rate; second variable inflation.Imporantly, one parameters associated \\(z_{t-1}\\) (parameters called speeds adjustement) significant expected signs: short term nominal interest rate \\(i_t\\) high \\(z_t\\) becomes positive (since, intercept, \\(z_t = i_t - \\beta \\pi_t\\), \\(\\beta\\) comes first-step regression), since \\(B_{1,1}\\) negative (equal -0.104), positive \\(z_t\\) generate negative correction (.e., \\(B_{1,1}z_t\\)) \\(i_{t+1}\\) date \\(t+1\\), thereby “correcting” high level \\(i_t\\).","code":"\nlibrary(AEC);library(tseries)\nT <- dim(US3var)[1]\ninfl <- US3var$infl\ni    <- US3var$r\neq <- lm(i~infl)\nz <- eq$residuals\npo.test(cbind(i,infl))## \n##  Phillips-Ouliaris Cointegration Test\n## \n## data:  cbind(i, infl)\n## Phillips-Ouliaris demeaned = -29.484, Truncation lag parameter = 2,\n## p-value = 0.01\ninfl_1 <- c(NaN,infl[1:(T-1)])\ninfl_2 <- c(NaN,NaN,infl[1:(T-2)])\ninfl_3 <- c(NaN,NaN,NaN,infl[1:(T-3)])\ni_1 <- c(NaN,i[1:(T-1)])\ni_2 <- c(NaN,NaN,i[1:(T-2)])\ni_3 <- c(NaN,NaN,NaN,i[1:(T-3)])\nz_1 <- c(NaN,z[1:(T-1)])\ndinfl   <- infl   - infl_1\ndinfl_1 <- infl_1 - infl_2\ndinfl_2 <- infl_2 - infl_3\ndi   <- i   - i_1\ndi_1 <- i_1 - i_2\ndi_2 <- i_2 - i_3\neq1 <- lm(di    ~ z_1 + dinfl_1 + di_1 + dinfl_2 + di_2)\neq2 <- lm(dinfl ~ z_1 + dinfl_1 + di_1 + dinfl_2 + di_2)"},{"path":"ARCHGARCH.html","id":"ARCHGARCH","chapter":"7 ARCH and GARCH models","heading":"7 ARCH and GARCH models","text":"","code":""},{"path":"ARCHGARCH.html","id":"conditional-heteroskedasticity","chapter":"7 ARCH and GARCH models","heading":"7.1 Conditional heteroskedasticity","text":"Many financial macroeconomic variables hit shocks whose variance constant time, .e. heteroskedatic shocks. Often, conditional variance shocks features persistent behavior (volatility clustering). observation large shocks (absolute value) tend followed important shocks notably established Mandelbrot (1963). situation illustrated Figure 7.1.Autoregressive Conditional Heteroskedasticity (ARCH) generalized version (GARCH) constitute useful tools model time series.\nFigure 7.1: Upper plot: SMI index (daily Close prices); lower plot: daily log returns.\norder test whether times series shocks \\(\\{u_1,\\dots,u_T\\}\\) features persistent conditional heteroskedasticity, simple test designed Engle (1982). test consists regressing \\(u_t^2\\) last \\(m\\) lags (.e., \\(u_{t-1}^2,\\dots,u_{t-m}^2\\)) OLS.null hypothesis \\(u_t\\) ..d., :\n\\[\nT \\times R^2 \\overset{d}{\\rightarrow} \\chi^2(m),\n\\]\n\\(R^2\\) centered \\(R^2\\) OLS regression.Let us employ test return series plotted lower panel Figure 7.1:p-value extremely close zero. Hence strongly reject null ..d. SMI return.","code":"\nlibrary(AEC);data(smi)\nsmi <- smi[complete.cases(smi),] # remove NaNs\nT <- dim(smi)[1]\nsmi$r <- 100*c(NaN,log(smi$Close[2:T]/smi$Close[1:(T-1)]))\nu <- smi$r^2\nu_1 <- c(NaN,smi$r[1:(T-1)]^2)\nu_2 <- c(NaN,NaN,smi$r[1:(T-2)]^2)\neq <- lm(u^2 ~ u_1^2 + u_2^2)\ntest.stat <- length(u)*summary(eq)$r.squared\npvalue <- 1 - pchisq(q = test.stat,df=2)"},{"path":"ARCHGARCH.html","id":"ARCH","chapter":"7 ARCH and GARCH models","heading":"7.2 The ARCH model","text":"","code":""},{"path":"ARCHGARCH.html","id":"the-two-arch-specifications","chapter":"7 ARCH and GARCH models","heading":"7.2.1 The two ARCH specifications","text":"Consider auto-regressive process following:\n\\[\\begin{equation}\ny_t = c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + u_t,\\tag{7.1}\n\\end{equation}\\]\n\\(u_t\\) white noise (Def. 1.1), .e. \\(\\mathbb{E}(u_t)=0\\), \\(\\mathbb{E}(u_t^2)=\\sigma^2\\) \\(\\mathbb{E}(u_t u_s)=0\\) \\(s \\ne t\\). Importantly, note unconditional variance \\(u_t\\) \\(\\sigma^2\\), conditional variance can time-varying. case context (G)ARCH models.ARCH(m) model, \\(u_t\\) follows:\n\\[\\begin{equation}\nu_t^2 = \\zeta + \\alpha_1 u_{t-1}^2 + \\dots + \\alpha_m u_{t-m}^2 + w_t,\\tag{7.2}\n\\end{equation}\\]\n\\(w_t\\) non-autocorrelated white noise process exogenous \\(u_t\\), sense :\n\\[\n\\mathbb{E}(w_t|u_{t-1},u_{t-2},\\dots)=0.\n\\]\ncase:\n\\[\\begin{equation}\n\\mathbb{E}(u_t^2|u_{t-1},u_{t-2},\\dots) = \\zeta + \\alpha_1 u_{t-1}^2 + \\dots + \\alpha_m u_{t-m}^2.\\tag{7.3}\n\\end{equation}\\]Eq. (7.2) make sense, case \n\\[\n\\zeta + \\alpha_1 u_{t-1}^2 + \\dots + \\alpha_m u_{t-m}^2 + w_t \\ge 0\n\\]\nrealisations \\(\\{u_t\\}\\). case \\(w_t > -\\zeta\\), \\(\\zeta>0\\), \\(\\alpha_i \\ge 0\\) \\(\\\\{1,\\dots,m\\}\\). Assuming case, \\(u_t^2\\) covariance-stationary roots :\n\\[\ng(z) = 1 - \\alpha_1 z - \\dots - \\alpha_m z^m = 0\n\\]\nlie outside unit circle (Prop. 2.5). necessary condition root 0 1 \\(\\sum_i \\alpha_i<1\\).12 also sufficient condition.13If \\(w_t > -\\zeta\\), \\(\\zeta>0\\), \\(\\alpha_i\\ge0\\) \\(\\\\{1,\\dots,m\\}\\) \\(\\sum_i \\alpha_i<1\\), unconditional variance \\(u_t\\) :\n\\[\n\\sigma^2 = \\mathbb{E}(u_t^2) = \\frac{\\zeta}{1 - \\sum_{=1}^m \\alpha_i}.\n\\]alternative representation ARCH(m) process follows:\n\\[\\begin{equation}\nu_t = \\sqrt{h_t}v_t,\\tag{7.4}\n\\end{equation}\\]\n\\(v_t\\) ..d. sequence zero mean unit variance, .e.:\n\\[\n\\mathbb{E}(v_t) = 0, \\quad \\mathbb{E}(v_t^2) = 1.\n\\]\n\\(h_t\\) follows:\n\\[\\begin{equation}\nh_t = \\zeta + \\alpha_1 u_{t-1}^2 + \\dots + \\alpha_m u_{t-m}^2,\\tag{7.5}\n\\end{equation}\\]\nEq. (7.3) also true. Since \\(u_t^2 = h_t v_t^2\\), Eq. (7.2) holds \\(w_t = h_t (v_t^2-1)\\). alternative representation convenient \\(v_t\\) necessarily bounded.","code":""},{"path":"ARCHGARCH.html","id":"maximum-likelihood-estimation-of-an-arch-process","chapter":"7 ARCH and GARCH models","heading":"7.2.2 Maximum Likelihood Estimation of an ARCH process","text":"Assume complete model :\n\\[\ny_t = \\mathbf{x}_t'\\boldsymbol\\beta + u_t,\n\\]\n\\(\\mathbf{x}_t\\) \\(k \\times 1\\) vector explanatory variables \\(u_t\\) specified Eq. (7.4).write likelihood function, convenient condition first \\(m\\) observations. Let us denote \\(\\mathcal{}_t\\) following information set:\n\\[\n\\mathcal{}_t = (y_t,y_{t-1},\\dots,y_0,y_{-1},\\dots,y_{-m+1}, \\mathbf{x}_t,\\mathbf{x}_{t-1},\\dots,\\mathbf{x}_0,\\mathbf{x}_{-1},\\dots,\\mathbf{x}_{-m+1}).\n\\]\\(v_t \\sim \\mathcal{N}(0,1)\\), , \\(t\\ge1\\):\n\\[\\begin{equation}\nf(y_t|\\mathbf{x_t},\\mathcal{}_{t-1}) = \\frac{1}{\\sqrt{2 \\pi h_t}}\\exp\\left(\\frac{-(y_t-\\mathbf{x}_t'\\boldsymbol\\beta)^2}{2h_t}\\right),\\tag{7.6}\n\\end{equation}\\]\n\\(h_t\\) given :\n\\[\nh_t = \\zeta + \\alpha_1 (y_{t-1} - \\mathbf{x}_{t-1}'\\boldsymbol\\beta)^2 + \\dots + \\alpha_m (y_{t-m} - \\mathbf{x}_{t-m}'\\boldsymbol\\beta)^2.\n\\]log likelihood function given :\n\\[\n\\log \\mathcal{L}(\\theta) = \\sum_{t=1}^T \\log(f(y_t|\\mathbf{x_t},\\mathcal{}_{t-1})),\n\\]\n\\(\\theta\\), vector unknown parameters, \\([\\boldsymbol\\beta',\\zeta,\\boldsymbol\\alpha']'\\), \\(\\boldsymbol\\alpha=[\\alpha_1,\\dots,\\alpha_m]'\\). maximisation log likelihood performed numerically.Note one can also use non-Gaussian distributions \\(v_t\\). (, one replace normal distribution Eq. (7.6).)Let us fit ARCH(2) model SMI return data (lower plot Figure 7.1). , make use function compute.garch package AEC. function takes four arguments:vector theta contains model parameterization: first, \\(\\zeta\\), \\(\\alpha_i\\)’s, GARCH models (see Subsection 7.3 ), \\(\\delta_i\\)’s.vector x contains observations process.m number lags ARCH specification.r number lags GARCH specification (see Subsection 7.3 ).Function compute.garch returns list, one entry log-likelihood associated parameterization \\([\\zeta,\\boldsymbol\\alpha']'\\), second resulting sequence \\(h_t\\)’s (see Eq. (7.5)). Let us create function returns log-likelihood:Now, let us maximize log-likelihood:Table 7.1 reports estimated parameters standard deviation. Figure 7.2 displays resulting 95% confidence intervals (.e., \\(\\pm 2 \\sqrt{h_t}\\)).Table 7.1: ARCH(2), ML estimation results.\nData: SMI daily returns.\nFigure 7.2: SMI daily returns (black) , red, 95% confidence interval based ARCH(2) estimated model (\\(\\pm 2 \\sqrt{h_t}\\)).\n","code":"\nloglik <- function(theta,x,m,r){\n  # first parameter of theta: zeta\n  # next: alpha's (ARCH)\n  # next: delta's (GARCH)\n  Garch <- compute.garch(theta,x,m,r)\n  return(Garch$logf)\n}\nm <- 2\nr <- 0 # for ARCH models, r=0\nsmi <- smi[4000:dim(smi)[1],] # reduce sample\npar0 <- c(0.62,0.2,0.2)\nres.opt <- optim(par=par0,x=smi$r,m=m,r=r,loglik,\n                 method=\"BFGS\",hessian=TRUE,\n                 control = list(trace=TRUE,maxit = 10))\nestim.param <- res.opt$par\nstd.dev <- sqrt(diag(solve(res.opt$hessian)))\nt.stat <- estim.param/std.dev"},{"path":"ARCHGARCH.html","id":"GARCH","chapter":"7 ARCH and GARCH models","heading":"7.3 The GARCH model","text":"One can generalize model replace Eq. (7.5) :\n\\[\\begin{eqnarray}\nh_t &=& (1-\\delta_1 - \\delta_2 - \\dots - \\delta_r) \\zeta + \\nonumber \\\\\n&&\\delta_1 h_{t-1} + \\delta_2 h_{t-2} + \\dots + \\delta_r h_{t-r} + \\nonumber \\\\\n&&\\alpha_1 u_{t-1}^2 + \\dots + \\alpha_m u_{t-m}^2. \\tag{7.7}\n\\end{eqnarray}\\]\ngeneralised autoregressive conditional heteroskedasticity model denoted GARCH(r,m). Non-negativity satisfied soon \\(\\kappa>0\\), \\(\\alpha_j \\ge 0\\), \\(\\delta_j \\ge 0\\) \\(j \\le p\\).Denoting \\(u_t^2 - h_t\\) \\(w_t\\), \\((1-\\delta_1 - \\dots - \\delta_r) \\zeta\\) \\(\\kappa\\), can checked :14\n\\[\\begin{eqnarray}\nu_t^2 &=& \\kappa + (\\delta_1 + \\alpha_1)u_{t-1}^2 + (\\delta_2 + \\alpha_2)u_{t-2}^2 + \\dots \\nonumber \\\\\n&& + (\\delta_p + \\alpha_p)u_{t-p}^2 + w_t - \\delta_1 w_{t-1} - \\dots - \\delta_r w_{t-r},\n\\end{eqnarray}\\]\n\\(p=max(m,r)\\), \\(\\alpha_j=0\\) \\(j>m\\) \\(\\delta_j=0\\) \\(j>r\\).\\(w_t = h_t(v_t^2 - 1)\\). regularity assumptions, \\(\\{w_t\\}\\) white noise. Hence, \\(u_t^2\\) follows ARMA(p,r) process. Accordingly, comes \\(u_t^2\\) covariance stationary roots :\n\\[\n1 - (\\delta_1 + \\alpha_1)z - \\dots - (\\delta_1 + \\alpha_1)z^p = 0\n\\]\nlie outside unit circle (Prop. 2.5). \\(\\delta_i + \\alpha_i\\) nonnegative, using reasoning ARCH models, case iff:\n\\[\n\\sum_i \\delta_i + \\sum_i \\alpha_i < 1.\n\\]\ncase, unconditional variance \\(u_t\\), .e. unconditional mean \\(u_t^2\\), :\n\\[\n\\mathbb{E}(u_t^2) = \\sigma^2 = \\frac{\\kappa}{1 - \\sum_i \\delta_i - \\sum_i \\alpha_i}.\n\\]GARCH models can also estimated ML approach.Table 7.2 reports estimated parameters fitting GARCH(1,1) model SMI return dataset. FigureFigure 7.3 compare condiitonal standard deviations (\\(\\sqrt{h_t}\\)) resulting ARCH(2) GARCH(1,1) specifications.Table 7.2: ARCH(2), ML estimation results.\nData: SMI daily returns.\nFigure 7.3: Estimated conditional standard deviations (\\(\\sqrt{h_t}\\)) SMI daily returns: ARCH(2) [black line] GARCH(1,1) [dotted red line] specifications.\n","code":"## initial  value 5703.217726 \n## iter  10 value 5359.604659\n## final  value 5359.604659 \n## stopped after 10 iterations"},{"path":"ARCHGARCH.html","id":"arch-in-mean","chapter":"7 ARCH and GARCH models","heading":"7.4 ARCH-in-mean","text":"Another extension ARCH model ARCH--Mean, ARCH-M model. model close specified Eqs. (7.1), (7.4) (7.5), also allows potential effect \\(h_t\\) \\(\\mathbb{E}_{t-1}(y_{t})\\):\n\\[\\begin{eqnarray*}\ny_t &=& c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + {\\color{blue}\\delta h_t}+ u_t \\\\\nu_t &=& \\sqrt{h_t} v_t \\\\\nh_t &=& \\zeta + \\alpha_1 u_{t-1}^2 + \\dots + \\alpha_m u_{t-m}^2,\n\\end{eqnarray*}\\]\n\\(v_t\\) zero-mean unit-variance ..d. sequence.","code":""},{"path":"append.html","id":"append","chapter":"8 Appendix","heading":"8 Appendix","text":"","code":""},{"path":"append.html","id":"PCAapp","chapter":"8 Appendix","heading":"8.1 Principal component analysis (PCA)","text":"Principal component analysis (PCA) classical easy--use statistical method reduce dimension large datasets containing variables linearly driven relatively small number factors. approach widely used data analysis image compression.Suppose \\(T\\) observations \\(n\\)-dimensional random vector \\(x\\), denoted \\(x_{1},x_{2},\\ldots,x_{T}\\). suppose component \\(x\\) mean zero.Let denote \\(X\\) matrix given \\(\\left[\\begin{array}{cccc} x_{1} & x_{2} & \\ldots & x_{T}\\end{array}\\right]'\\). Denote \\(j^{th}\\) column \\(X\\) \\(X_{j}\\).want find linear combination \\(x_{}\\)’s (\\(x.u\\)), \\(\\left\\Vert u\\right\\Vert =1\\), “maximum variance.” , want solve:\n\\[\\begin{equation}\n\\begin{array}{clll}\n\\underset{u}{\\arg\\max} & u'X'Xu. \\\\\n\\mbox{s.t. } & \\left\\Vert u \\right\\Vert =1\n\\end{array}\\tag{8.1}\n\\end{equation}\\]Since \\(X'X\\) positive definite matrix, admits following decomposition:\n\\[\\begin{eqnarray*}\nX'X & = & PDP'\\\\\n& = & P\\left[\\begin{array}{ccc}\n\\lambda_{1}\\\\\n& \\ddots\\\\\n&  & \\lambda_{n}\n\\end{array}\\right]P',\n\\end{eqnarray*}\\]\n\\(P\\) orthogonal matrix whose columns eigenvectors \\(X'X\\).can order eigenvalues \\(\\lambda_{1}\\geq\\ldots\\geq\\lambda_{n}\\). (Since \\(X'X\\) positive definite, eigenvalues positive.)Since \\(P\\) orthogonal, \\(u'X'Xu=u'PDP'u=y'Dy\\) \\(\\left\\Vert y\\right\\Vert =1\\). Therefore, \\(y_{}^{2}\\leq 1\\) \\(\\leq n\\).consequence:\n\\[\ny'Dy=\\sum_{=1}^{n}y_{}^{2}\\lambda_{}\\leq\\lambda_{1}\\sum_{=1}^{n}y_{}^{2}=\\lambda_{1}.\n\\]easily seen maximum reached \\(y=\\left[1,0,\\cdots,0\\right]'\\). Therefore, maximum optimization program (Eq. (8.1)) obtained \\(u=P\\left[1,0,\\cdots,0\\right]'\\). , \\(u\\) eigenvector \\(X'X\\) associated larger eigenvalue (first column \\(P\\)).Let us denote \\(F\\) vector given matrix product \\(XP\\). columns \\(F\\), denoted \\(F_{j}\\), called factors. :\n\\[\nF'F=P'X'XP=D.\n\\]\nTherefore, particular, \\(F_{j}\\)’s orthogonal.Since \\(X=FP'\\), \\(X_{j}\\)’s linear combinations factors. Let us denote \\(\\hat{X}_{,j}\\) part \\(X_{}\\) explained factor \\(F_{j}\\), :\n\\[\\begin{eqnarray*}\n\\hat{X}_{,j} & = & p_{ij}F_{j}\\\\\nX_{} & = & \\sum_{j}\\hat{X}_{,j}=\\sum_{j}p_{ij}F_{j}.\n\\end{eqnarray*}\\]Consider share variance explained—\\(n\\) variables (\\(X_{1},\\ldots,X_{n}\\))—first factor \\(F_{1}\\):\n\\[\\begin{eqnarray*}\n\\frac{\\sum_{}\\hat{X}'_{,1}\\hat{X}_{,1}}{\\sum_{}X_{}'X_{}} & = & \\frac{\\sum_{}p_{i1}F'_{1}F_{1}p_{i1}}{tr(X'X)} = \\frac{\\sum_{}p_{i1}^{2}\\lambda_{1}}{tr(X'X)} = \\frac{\\lambda_{1}}{\\sum_{}\\lambda_{}}.\n\\end{eqnarray*}\\]Intuitively, first eigenvalue large, means first factor captures large share fluctutaions \\(n\\) \\(X_{}\\)’s.token, easily seen fraction variance \\(n\\) variables explained factor \\(j\\) given :\n\\[\\begin{eqnarray*}\n\\frac{\\sum_{}\\hat{X}'_{,j}\\hat{X}_{,j}}{\\sum_{}X_{}'X_{}} & = & \\frac{\\lambda_{j}}{\\sum_{}\\lambda_{}}.\n\\end{eqnarray*}\\]Let us illustrate PCA term structure yields. term strucutre yields (yield curve) know driven small number factors (e.g., Litterman Scheinkman (1991)). One can typically employ PCA recover factors. data used example taken Fred database (tickers: “DGS6MO”,“DGS1”, …). second plot shows factor loardings, indicate first factor level factor (loadings = black line), second factor slope factor (loadings = blue line), third factor curvature factor (loadings = red line).run PCA, one simply apply function prcomp matrix data:Let us know visualize results. first plot Figure 8.1 shows share total variance explained different principal components (PCs). second plot shows factor loadings. two bottom plots show yields (black) fitted linear combinations first two PCs .\nFigure 8.1: PCA results. dataset contains 8 time series U.S. interest rates different maturities.\n","code":"\nlibrary(AEC)\nUSyields <- USyields[complete.cases(USyields),]\nyds <- USyields[c(\"Y1\",\"Y2\",\"Y3\",\"Y5\",\"Y7\",\"Y10\",\"Y20\",\"Y30\")]\nPCA.yds <- prcomp(yds,center=TRUE,scale. = TRUE)\npar(mfrow=c(2,2))\npar(plt=c(.1,.95,.2,.8))\nbarplot(PCA.yds$sdev^2/sum(PCA.yds$sdev^2),\n        main=\"Share of variance expl. by PC's\")\naxis(1, at=1:dim(yds)[2], labels=colnames(PCA.yds$x))\nnb.PC <- 2\nplot(-PCA.yds$rotation[,1],type=\"l\",lwd=2,ylim=c(-1,1),\n     main=\"Factor loadings (1st 3 PCs)\",xaxt=\"n\",xlab=\"\")\naxis(1, at=1:dim(yds)[2], labels=colnames(yds))\nlines(PCA.yds$rotation[,2],type=\"l\",lwd=2,col=\"blue\")\nlines(PCA.yds$rotation[,3],type=\"l\",lwd=2,col=\"red\")\nY1.hat <- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[\"Y1\",1:2]\nY1.hat <- mean(USyields$Y1) + sd(USyields$Y1) * Y1.hat\nplot(USyields$date,USyields$Y1,type=\"l\",lwd=2,\n     main=\"Fit of 1-year yields (2 PCs)\",\n     ylab=\"Obs (black) / Fitted by 2PCs (dashed blue)\")\nlines(USyields$date,Y1.hat,col=\"blue\",lty=2,lwd=2)\nY10.hat <- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[\"Y10\",1:2]\nY10.hat <- mean(USyields$Y10) + sd(USyields$Y10) * Y10.hat\nplot(USyields$date,USyields$Y10,type=\"l\",lwd=2,\n     main=\"Fit of 10-year yields (2 PCs)\",\n     ylab=\"Obs (black) / Fitted by 2PCs (dashed blue)\")\nlines(USyields$date,Y10.hat,col=\"blue\",lty=2,lwd=2)"},{"path":"append.html","id":"LinAlgebra","chapter":"8 Appendix","heading":"8.2 Linear algebra: definitions and results","text":"Definition 8.1  (Eigenvalues) eigenvalues matrix \\(M\\) numbers \\(\\lambda\\) :\n\\[\n|M - \\lambda | = 0,\n\\]\n\\(| \\bullet |\\) determinant operator.Proposition 8.1  (Properties determinant) :\\(|MN|=|M|\\times|N|\\).\\(|M^{-1}|=|M|^{-1}\\).\\(M\\) admits diagonal representation \\(M=TDT^{-1}\\), \\(D\\) diagonal matrix whose diagonal entries \\(\\{\\lambda_i\\}_{=1,\\dots,n}\\), :\n\\[\n|M - \\lambda |=\\prod_{=1}^n (\\lambda_i - \\lambda).\n\\]Definition 8.2  (Moore-Penrose inverse) \\(M \\\\mathbb{R}^{m \\times n}\\), Moore-Penrose pseudo inverse (exists ) unique matrix \\(M^* \\\\mathbb{R}^{n \\times m}\\) satisfies:\\(M M^* M = M\\)\\(M^* M M^* = M^*\\)\\((M M^*)'=M M^*\\)\n.iv \\((M^* M)'=M^* M\\).Proposition 8.2  (Properties Moore-Penrose inverse) \\(M\\) invertible \\(M^* = M^{-1}\\).pseudo-inverse zero matrix transpose.\n*\npseudo-inverse pseudo-inverse original matrix.Definition 8.3  (Idempotent matrix) Matrix \\(M\\) idempotent \\(M^2=M\\).\\(M\\) symmetric idempotent matrix, \\(M'M=M\\).Proposition 8.3  (Roots idempotent matrix) eigenvalues idempotent matrix either 1 0.Proof. \\(\\lambda\\) eigenvalue idempotent matrix \\(M\\) \\(\\exists x \\ne 0\\) s.t. \\(Mx=\\lambda x\\). Hence \\(M^2x=\\lambda M x \\Rightarrow (1-\\lambda)Mx=0\\). Either element \\(Mx\\) zero, case \\(\\lambda=0\\) least one element \\(Mx\\) nonzero, case \\(\\lambda=1\\).Proposition 8.4  (Idempotent matrix chi-square distribution) rank symmetric idempotent matrix equal trace.Proof. result follows Prop. 8.3, combined fact rank symmetric matrix equal number nonzero eigenvalues.Proposition 8.5  (Constrained least squares) solution following optimisation problem:\n\\[\\begin{eqnarray*}\n\\underset{\\boldsymbol\\beta}{\\min} && || \\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta ||^2 \\\\\n&& \\mbox{subject } \\mathbf{R}\\boldsymbol\\beta = \\mathbf{q}\n\\end{eqnarray*}\\]\ngiven :\n\\[\n\\boxed{\\boldsymbol\\beta^r = \\boldsymbol\\beta_0 - (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\{\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\}^{-1}(\\mathbf{R}\\boldsymbol\\beta_0 - \\mathbf{q}),}\n\\]\n\\(\\boldsymbol\\beta_0=(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\).Proof. See instance Jackman, 2007.Proposition 8.6  (Inverse partitioned matrix) :\n\\[\\begin{eqnarray*}\n&&\\left[ \\begin{array}{cc} \\mathbf{}_{11} & \\mathbf{}_{12} \\\\ \\mathbf{}_{21} & \\mathbf{}_{22} \\end{array}\\right]^{-1} = \\\\\n&&\\left[ \\begin{array}{cc} (\\mathbf{}_{11} - \\mathbf{}_{12}\\mathbf{}_{22}^{-1}\\mathbf{}_{21})^{-1} & - \\mathbf{}_{11}^{-1}\\mathbf{}_{12}(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\\\\n-(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1}\\mathbf{}_{21}\\mathbf{}_{11}^{-1} & (\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\end{array} \\right].\n\\end{eqnarray*}\\]Definition 8.4  (Matrix derivatives) Consider fonction \\(f: \\mathbb{R}^K \\rightarrow \\mathbb{R}\\). first-order derivative :\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) =\n\\left[\\begin{array}{c}\n\\frac{\\partial f}{\\partial b_1}(\\mathbf{b})\\\\\n\\vdots\\\\\n\\frac{\\partial f}{\\partial b_K}(\\mathbf{b})\n\\end{array}\n\\right].\n\\]\nuse notation:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}'}(\\mathbf{b}) = \\left(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b})\\right)'.\n\\]Proposition 8.7  :\\(f(\\mathbf{b}) = ' \\mathbf{b}\\) \\(\\) \\(K \\times 1\\) vector \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = \\).\\(f(\\mathbf{b}) = \\mathbf{b}'\\mathbf{b}\\) \\(\\) \\(K \\times K\\) matrix, \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = 2A\\mathbf{b}\\).Proposition 8.8  (Square absolute summability) :\n\\[\n\\underbrace{\\sum_{=0}^{\\infty}|\\theta_i| < + \\infty}_{\\mbox{Absolute summability}} \\Rightarrow \\underbrace{\\sum_{=0}^{\\infty} \\theta_i^2 < + \\infty}_{\\mbox{Square summability}}.\n\\]Proof. See Appendix 3.Hamilton. Idea: Absolute summability implies exist \\(N\\) , \\(j>N\\), \\(|\\theta_j| < 1\\) (deduced Cauchy criterion, Theorem 8.2 therefore \\(\\theta_j^2 < |\\theta_j|\\).","code":""},{"path":"append.html","id":"variousResults","chapter":"8 Appendix","heading":"8.3 Statistical analysis: definitions and results","text":"","code":""},{"path":"append.html","id":"moments-and-statistics","chapter":"8 Appendix","heading":"8.3.1 Moments and statistics","text":"Definition 8.5  (Partial correlation) partial correlation \\(y\\) \\(z\\), controlling variables \\(\\mathbf{X}\\) sample correlation \\(y^*\\) \\(z^*\\), latter two variables residuals regressions \\(y\\) \\(\\mathbf{X}\\) \\(z\\) \\(\\mathbf{X}\\), respectively.correlation denoted \\(r_{yz}^\\mathbf{X}\\). definition, :\n\\[\\begin{equation}\nr_{yz}^\\mathbf{X} = \\frac{\\mathbf{z^*}'\\mathbf{y^*}}{\\sqrt{(\\mathbf{z^*}'\\mathbf{z^*})(\\mathbf{y^*}'\\mathbf{y^*})}}.\\tag{8.2}\n\\end{equation}\\]Definition 8.6  (Skewness kurtosis) Let \\(Y\\) random variable whose fourth moment exists. expectation \\(Y\\) denoted \\(\\mu\\).skewness \\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^3]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{3/2}}.\n\\]kurtosis \\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^4]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{2}}.\n\\]Theorem 8.1  (Cauchy-Schwarz inequality) :\n\\[\n|\\mathbb{C}ov(X,Y)| \\le \\sqrt{\\mathbb{V}ar(X)\\mathbb{V}ar(Y)}\n\\]\n, \\(X \\ne =\\) \\(Y \\ne 0\\), equality holds iff \\(X\\) \\(Y\\) affine transformation.Proof. \\(\\mathbb{V}ar(X)=0\\), trivial. case, let’s define \\(Z\\) \\(Z = Y - \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\). easily seen \\(\\mathbb{C}ov(X,Z)=0\\). , variance \\(Y=Z+\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\) equal sum variance \\(Z\\) variance \\(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\), :\n\\[\n\\mathbb{V}ar(Y) = \\mathbb{V}ar(Z) + \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X) \\ge \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X).\n\\]\nequality holds iff \\(\\mathbb{V}ar(Z)=0\\), .e. iff \\(Y = \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X+cst\\).Definition 8.7  (Mean ergodicity) covariance-stationary process \\(y_t\\) ergodic mean :\n\\[\n\\mbox{plim}_{T \\rightarrow +\\infty} \\frac{1}{T}\\sum_{t=1}^T y_t = \\mathbb{E}(y_t).\n\\]Definition 8.8  (Second-moment ergodicity) covariance-stationary process \\(y_t\\) ergodic second moments , \\(j\\):\n\\[\n\\mbox{plim}_{T \\rightarrow +\\infty} \\frac{1}{T}\\sum_{t=1}^T (y_t-\\mu) (y_{t-j}-\\mu) = \\gamma_j.\n\\]noted ergodicity stationarity different properties. Typically process \\(\\{x_t\\}\\) , \\(\\forall t\\), \\(x_t \\equiv y\\), \\(y \\sim\\,\\mathcal{N}(0,1)\\) (say), \\(\\{x_t\\}\\) stationary ergodic.","code":""},{"path":"append.html","id":"standard-distributions","chapter":"8 Appendix","heading":"8.3.2 Standard distributions","text":"Definition 8.9  (F distribution) Consider \\(n=n_1+n_2\\) ..d. \\(\\mathcal{N}(0,1)\\) r.v. \\(X_i\\). r.v. \\(F\\) defined :\n\\[\nF = \\frac{\\sum_{=1}^{n_1} X_i^2}{\\sum_{j=n_1+1}^{n_1+n_2} X_j^2}\\frac{n_2}{n_1}\n\\]\n\\(F \\sim \\mathcal{F}(n_1,n_2)\\). (See Table 8.4 quantiles.)Definition 8.10  (Student-t distribution) \\(Z\\) follows Student-t (\\(t\\)) distribution \\(\\nu\\) degrees freedom (d.f.) :\n\\[\nZ = X_0 \\bigg/ \\sqrt{\\frac{\\sum_{=1}^{\\nu}X_i^2}{\\nu}}, \\quad X_i \\sim ..d. \\mathcal{N}(0,1).\n\\]\n\\(\\mathbb{E}(Z)=0\\), \\(\\mathbb{V}ar(Z)=\\frac{\\nu}{\\nu-2}\\) \\(\\nu>2\\). (See Table 8.2 quantiles.)Definition 8.11  (Chi-square distribution) \\(Z\\) follows \\(\\chi^2\\) distribution \\(\\nu\\) d.f. \\(Z = \\sum_{=1}^{\\nu}X_i^2\\) \\(X_i \\sim ..d. \\mathcal{N}(0,1)\\).\n\\(\\mathbb{E}(Z)=\\nu\\). (See Table 8.3 quantiles.)Definition 8.12  (Cauchy distribution) \nFigure 8.2: Pdf Cauchy distribution (\\(\\mu=0\\), \\(\\gamma=1\\)).\nProposition 8.9  (Inner product multivariate Gaussian variable) Let \\(X\\) \\(n\\)-dimensional multivariate Gaussian variable: \\(X \\sim \\mathcal{N}(0,\\Sigma)\\). :\n\\[\nX' \\Sigma^{-1}X \\sim \\chi^2(n).\n\\]Proof. \\(\\Sigma\\) symmetrical definite positive matrix, admits spectral decomposition \\(PDP'\\) \\(P\\) orthogonal matrix (.e. \\(PP'=Id\\)) D diagonal matrix non-negative entries. Denoting \\(\\sqrt{D^{-1}}\\) diagonal matrix whose diagonal entries inverse \\(D\\), easily checked covariance matrix \\(Y:=\\sqrt{D^{-1}}P'X\\) \\(Id\\). Therefore \\(Y\\) vector uncorrelated Gaussian variables. properties Gaussian variables imply components \\(Y\\) also independent. Hence \\(Y'Y=\\sum_i Y_i^2 \\sim \\chi^2(n)\\).remains note \\(Y'Y=X'PD^{-1}P'X=X'\\mathbb{V}ar(X)^{-1}X\\) conclude.","code":""},{"path":"append.html","id":"StochConvergences","chapter":"8 Appendix","heading":"8.3.3 Stochastic convergences","text":"Definition 8.13  (Convergence probability) random variable sequence \\(x_n\\) converges probability constant \\(c\\) \\(\\forall \\varepsilon\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|x_n - c|>\\varepsilon) = 0\\).denoted : \\(\\mbox{plim } x_n = c\\).Definition 8.14  (Convergence Lr norm) \\(x_n\\) converges \\(r\\)-th mean (\\(L^r\\)-norm) towards \\(x\\), \\(\\mathbb{E}(|x_n|^r)\\) \\(\\mathbb{E}(|x|^r)\\) exist \n\\[\n\\lim_{n \\rightarrow \\infty} \\mathbb{E}(|x_n - x|^r) = 0.\n\\]\ndenoted : \\(x_n \\overset{L^r}{\\rightarrow} c\\).\\(r=2\\), convergence called mean square convergence.Definition 8.15  (Almost sure convergence) random variable sequence \\(x_n\\) converges almost surely \\(c\\) \\(\\mathbb{P}(\\lim_{n \\rightarrow \\infty} x_n = c) = 1\\).denoted : \\(x_n \\overset{.s.}{\\rightarrow} c\\).Definition 8.16  (Convergence distribution) \\(x_n\\) said converge distribution (law) \\(x\\) \n\\[\n\\lim_{n \\rightarrow \\infty} F_{x_n}(s) = F_{x}(s)\n\\]\n\\(s\\) \\(F_X\\) –cumulative distribution \\(X\\)– continuous.denoted : \\(x_n \\overset{d}{\\rightarrow} x\\).Theorem 8.2  (Cauchy criterion (non-stochastic case)) \\(\\sum_{=0}^{T} a_i\\) converges (\\(T \\rightarrow \\infty\\)) iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\left|\\sum_{=N+1}^{M} a_i\\right| < \\eta.\n\\]Theorem 8.3  (Cauchy criterion (stochastic case)) \\(\\sum_{=0}^{T} \\theta_i \\varepsilon_{t-}\\) converges mean square (\\(T \\rightarrow \\infty\\)) random variable iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\mathbb{E}\\left[\\left(\\sum_{=N+1}^{M} \\theta_i \\varepsilon_{t-}\\right)^2\\right] < \\eta.\n\\]","code":""},{"path":"append.html","id":"multivariate-gaussian-distribution","chapter":"8 Appendix","heading":"8.3.4 Multivariate Gaussian distribution","text":"Proposition 8.10  (p.d.f. multivariate Gaussian variable) \\(Y \\sim \\mathcal{N}(\\mu,\\Omega)\\) \\(Y\\) \\(n\\)-dimensional vector, density function \\(Y\\) :\n\\[\n\\frac{1}{(2 \\pi)^{n/2}|\\Omega|^{1/2}}\\exp\\left[-\\frac{1}{2}\\left(Y-\\mu\\right)'\\Omega^{-1}\\left(Y-\\mu\\right)\\right].\n\\]","code":""},{"path":"append.html","id":"CLTappend","chapter":"8 Appendix","heading":"8.3.5 Central limit theorem","text":"Theorem 8.4  (Law large numbers) sample mean consistent estimator population mean.Proof. Let’s denote \\(\\phi_{X_i}\\) characteristic function r.v. \\(X_i\\). mean \\(X_i\\) \\(\\mu\\) Talyor expansion characteristic function :\n\\[\n\\phi_{X_i}(u) = \\mathbb{E}(\\exp(iuX)) = 1 + iu\\mu + o(u).\n\\]\nproperties characteristic function (see Def. ??) imply :\n\\[\n\\phi_{\\frac{1}{n}(X_1+\\dots+X_n)}(u) = \\prod_{=1}^{n} \\left(1 + \\frac{u}{n}\\mu + o\\left(\\frac{u}{n}\\right) \\right) \\rightarrow e^{iu\\mu}.\n\\]\nfacts () \\(e^{iu\\mu}\\) characteristic function constant \\(\\mu\\) (b) characteristic function uniquely characterises distribution imply sample mean converges distribution constant \\(\\mu\\), implies converges probability \\(\\mu\\).Theorem 8.5  (Lindberg-Levy Central limit theorem, CLT) \\(x_n\\) ..d. sequence random variables mean \\(\\mu\\) variance \\(\\sigma^2\\) (\\(\\]0,+\\infty[\\)), :\n\\[\n\\boxed{\\sqrt{n} (\\bar{x}_n - \\mu) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2), \\quad \\mbox{} \\quad \\bar{x}_n = \\frac{1}{n} \\sum_{=1}^{n} x_i.}\n\\]Proof. Let us introduce r.v. \\(Y_n:= \\sqrt{n}(\\bar{X}_n - \\mu)\\). \\(\\phi_{Y_n}(u) = \\left[ \\mathbb{E}\\left( \\exp(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)) \\right) \\right]^n\\). :\n\\[\\begin{eqnarray*}\n&&\\left[ \\mathbb{E}\\left( \\exp\\left(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)\\right) \\right) \\right]^n\\\\\n&=& \\left[ \\mathbb{E}\\left( 1 + \\frac{1}{\\sqrt{n}} u (X_1 - \\mu) - \\frac{1}{2n} u^2 (X_1 - \\mu)^2 + o(u^2) \\right) \\right]^n \\\\\n&=& \\left( 1 - \\frac{1}{2n}u^2\\sigma^2 + o(u^2)\\right)^n.\n\\end{eqnarray*}\\]\nTherefore \\(\\phi_{Y_n}(u) \\underset{n \\rightarrow \\infty}{\\rightarrow} \\exp \\left( - \\frac{1}{2}u^2\\sigma^2 \\right)\\), characteristic function \\(\\mathcal{N}(0,\\sigma^2)\\).","code":""},{"path":"append.html","id":"AppendixProof","chapter":"8 Appendix","heading":"8.4 Proofs","text":"Proof Eq. (1.3)Proof. :\n\\[\\begin{eqnarray*}\n&&T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right]\\\\\n&=& T\\mathbb{E}\\left[\\left(\\frac{1}{T}\\sum_{t=1}^T(y_t - \\mu)\\right)^2\\right] = \\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^T(y_t - \\mu)^2+2\\sum_{s<t\\le T}(y_t - \\mu)(y_s - \\mu)\\right]\\\\\n&=& \\gamma_0 +\\frac{2}{T}\\left(\\sum_{t=2}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-1} - \\mu)\\right]\\right) +\\frac{2}{T}\\left(\\sum_{t=3}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-2} - \\mu)\\right]\\right) + \\dots \\\\\n&&+ \\frac{2}{T}\\left(\\sum_{t=T-1}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-2)} - \\mu)\\right]\\right) + \\frac{2}{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-1)} - \\mu)\\right]\\\\\n&=&  \\gamma_0 + 2 \\frac{T-1}{T}\\gamma_1 + \\dots + 2 \\frac{1}{T}\\gamma_{T-1} .\n\\end{eqnarray*}\\]\nTherefore:\n\\[\\begin{eqnarray*}\n&& T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j \\\\\n&=& - 2\\frac{1}{T}\\gamma_1 - 2\\frac{2}{T}\\gamma_2 - \\dots - 2\\frac{T-1}{T}\\gamma_{T-1} - 2\\gamma_T - 2 \\gamma_{T+1} + \\dots\n\\end{eqnarray*}\\]\n:\n\\[\\begin{eqnarray*}\n&& \\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\\\\n&\\le& 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\end{eqnarray*}\\]\\(q \\le T\\), :\n\\[\\begin{eqnarray*}\n\\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right| &\\le& 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{q-1}{T}|\\gamma_{q-1}| +2\\frac{q}{T}|\\gamma_q| +\\\\\n&&2\\frac{q+1}{T}|\\gamma_{q+1}| + \\dots  + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\\\\\n&\\le& \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q-1)|\\gamma_{q-1}| +q|\\gamma_q|\\right) +\\\\\n&&2|\\gamma_{q+1}| + \\dots  + 2|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\end{eqnarray*}\\]Consider \\(\\varepsilon > 0\\). fact autocovariances absolutely summable implies exists \\(q_0\\) (Cauchy criterion, Theorem 8.2):\n\\[\n2|\\gamma_{q_0+1}|+2|\\gamma_{q_0+2}|+2|\\gamma_{q_0+3}|+\\dots < \\varepsilon/2.\n\\]\n, \\(T > q_0\\), comes :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) + \\varepsilon/2.\n\\]\n\\(T \\ge 2\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right)/(\\varepsilon/2)\\) (\\(= f(q_0)\\), say) \n\\[\n\\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) \\le \\varepsilon/2.\n\\]\n, \\(T>f(q_0)\\) \\(T>q_0\\), .e. \\(T>\\max(f(q_0),q_0)\\), :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\varepsilon.\n\\]Proof Proposition 4.1Proof. :\n\\[\\begin{eqnarray}\n\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) &=& \\mathbb{E}\\left([\\color{blue}{\\{y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)\\}} + \\color{red}{\\{\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}\\}}]^2\\right)\\nonumber\\\\\n&=&  \\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right) + \\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)\\nonumber\\\\\n&& + 2\\mathbb{E}\\left( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right). \\tag{8.3}\n\\end{eqnarray}\\]\nLet us focus last term. :\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right)\\\\\n&=& \\mathbb{E}( \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ \\underbrace{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}_{\\mbox{function $x_t$}}}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\color{blue}{\\underbrace{[\\mathbb{E}(y_{t+1}|x_t) - \\mathbb{E}(y_{t+1}|x_t)]}_{=0}})=0.\n\\end{eqnarray*}\\]Therefore, Eq. (8.3) becomes:\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) \\\\\n&=&  \\underbrace{\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right)}_{\\mbox{$\\ge 0$ depend $y^*_{t+1}$}} + \\underbrace{\\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)}_{\\mbox{$\\ge 0$ depends $y^*_{t+1}$}}.\n\\end{eqnarray*}\\]\nimplies \\(\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)\\) always larger \\(\\color{blue}{\\mathbb{E}([y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]^2)}\\), therefore minimized second term equal zero, \\(\\mathbb{E}(y_{t+1}|x_t) = y^*_{t+1}\\).Proof Proposition 3.1Proof. Using Proposition ??, obtain , conditionally \\(x_1\\), log-likelihood given \n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\theta) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right|\\\\\n&  & -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right].\n\\end{eqnarray*}\\]\nLet’s rewrite last term log-likelihood:\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)'\\Omega^{-1}\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\right],\n\\end{eqnarray*}\\]\n\\(j^{th}\\) element \\((n\\times1)\\) vector \\(\\hat{\\varepsilon}_{t}\\) sample residual, observation \\(t\\), OLS regression \\(y_{j,t}\\) \\(x_{t}\\). Expanding previous equation, get:\n\\[\\begin{eqnarray*}\n&&\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right]  = \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\\\\n&&+2\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}+\\sum_{t=1}^{T}x'_{t}(\\hat{\\Pi}-\\Pi)\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}.\n\\end{eqnarray*}\\]\nLet’s apply trace operator second term (scalar):\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t} & = & Tr\\left(\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\\\\n=  Tr\\left(\\sum_{t=1}^{T}\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\hat{\\varepsilon}_{t}'\\right) & = & Tr\\left(\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'\\sum_{t=1}^{T}x_{t}\\hat{\\varepsilon}_{t}'\\right).\n\\end{eqnarray*}\\]\nGiven , construction (property OLS estimates), sample residuals orthogonal explanatory variables, term zero. Introducing \\(\\tilde{x}_{t}=(\\hat{\\Pi}-\\Pi)'x_{t}\\), \n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] =\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}+\\sum_{t=1}^{T}\\tilde{x}'_{t}\\Omega^{-1}\\tilde{x}_{t}.\n\\end{eqnarray*}\\]\nSince \\(\\Omega\\) positive definite matrix, \\(\\Omega^{-1}\\) well. Consequently, smallest value last term can take obtained \\(\\tilde{x}_{t}=0\\), .e. \\(\\Pi=\\hat{\\Pi}.\\)MLE \\(\\Omega\\) matrix \\(\\hat{\\Omega}\\) maximizes \\(\\Omega\\overset{\\ell}{\\rightarrow}L(Y_{T};\\hat{\\Pi},\\Omega)\\). :\n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\end{eqnarray*}\\]Matrix \\(\\hat{\\Omega}\\) symmetric positive definite. easily checked (unrestricted) matrix maximizes latter expression symmetric positive definite matrix. Indeed:\n\\[\n\\frac{\\partial \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega)}{\\partial\\Omega}=\\frac{T}{2}\\Omega'-\\frac{1}{2}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t}\\Rightarrow\\hat{\\Omega}'=\\frac{1}{T}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t},\n\\]\nleads result.Proof Proposition 3.2Proof. Let us drop \\(\\) subscript. Rearranging Eq. (2.19), :\n\\[\n\\sqrt{T}(\\mathbf{b}-\\boldsymbol{\\beta}) =  (X'X/T)^{-1}\\sqrt{T}(X'\\boldsymbol\\varepsilon/T).\n\\]\nLet us consider autocovariances \\(\\mathbf{v}_t = x_t \\varepsilon_t\\), denoted \\(\\gamma^v_j\\). Using fact \\(x_t\\) linear combination past \\(\\varepsilon_t\\)s \\(\\varepsilon_t\\) white noise, get \\(\\mathbb{E}(\\varepsilon_t x_t)=0\\). Therefore\n\\[\n\\gamma^v_j = \\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}').\n\\]\n\\(j>0\\), \\(\\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}')=\\mathbb{E}(\\mathbb{E}[\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}'|\\varepsilon_{t-j},x_t,x_{t-j}])=\\) \\(\\mathbb{E}(\\varepsilon_{t-j}x_tx_{t-j}'\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}])=0\\). Note \\(\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}]=0\\) \\(\\{\\varepsilon_t\\}\\) ..d. white noise sequence. \\(j=0\\), :\n\\[\n\\gamma^v_0 = \\mathbb{E}(\\varepsilon_t^2x_tx_{t}')= \\mathbb{E}(\\varepsilon_t^2) \\mathbb{E}(x_tx_{t}')=\\sigma^2\\mathbf{Q}.\n\\]\nconvergence distribution \\(\\sqrt{T}(X'\\boldsymbol\\varepsilon/T)=\\sqrt{T}\\frac{1}{T}\\sum_{t=1}^Tv_t\\) results Central Limit Theorem covariance-stationary processes, using \\(\\gamma_j^v\\) computed .","code":""},{"path":"append.html","id":"Inference","chapter":"8 Appendix","heading":"8.5 Inference","text":"","code":""},{"path":"append.html","id":"MonteCarlo","chapter":"8 Appendix","heading":"8.5.1 Monte Carlo method","text":"use Monte Carlo need approximate distribution variable whose distribution unknown function another variable whose distribution known. instance, suppose know distribution random variable \\(X\\), takes values \\(\\mathbb{R}\\), density function \\(p\\). Assume want compute mean \\(\\varphi(X)\\). :\n\\[\n\\mathbb{E}(\\varphi(X))=\\int_{-\\infty}^{+\\infty}\\varphi(x)p(x)dx\n\\]\nSuppose integral simple expression. compute \\(\\mathbb{E}(\\varphi(X))\\) , virtue law large numbers, can approximate follows:\n\\[\n\\mathbb{E}(\\varphi(X))\\approx\\frac{1}{N}\\sum_{=1}^N\\varphi(X^{()}),\n\\]\n\\(\\{X^{()}\\}_{=1,...,N}\\) \\(N\\) independent draws \\(X\\). generally, distribution \\(\\varphi(X)\\) can approximated empirical distribution \\(\\varphi(X^{()})\\)’s. Typically, 10’000 values \\(\\varphi(X^{()})\\) drawn, \\(5^{th}\\) percentile p.d.f. \\(\\varphi(X)\\) can approximated \\(500^{th}\\) value 10’000 draws \\(\\varphi(X^{()})\\) (arranging values ascending order).instance, regards computation confidence intervals around IRFs, one think \\(\\{\\widehat{\\Phi}_j\\}_{j=1,...,p}\\), \\(\\widehat{\\Omega}\\) \\(X\\) \\(\\{\\widehat{\\Psi}_j\\}_{j=1,...}\\) \\(\\varphi(X)\\). (Proposition 3.3 provides us asymptotic distribution “\\(X\\).”)summarize, steps one can implement derive confidence intervals IRFs using Monte-Carlo approach: iteration \\(k\\),Draw \\(\\{\\widehat{\\Phi}_j^{(k)}\\}_{j=1,...,p}\\) \\(\\widehat{\\Omega}^{(k)}\\) asymptotic distribution (using Proposition 3.3).Compute matrix \\(B^{(k)}\\) \\(\\widehat{\\Omega}^{(k)}=B^{(k)}B^{(k)'}\\), according identification strategy.Compute associated IRFs \\(\\{\\widehat{\\Psi}_j\\}^{(k)}\\).Perform \\(N\\) replications report median impulse response (confidence intervals).","code":""},{"path":"append.html","id":"Delta","chapter":"8 Appendix","heading":"8.5.2 Delta method","text":"Suppose \\(\\beta\\) vector parameters \\(\\beta\\) estimator \n\\[\n\\sqrt{T}(\\hat\\beta-\\beta)\\overset{d}{\\rightarrow}\\mathcal{N}(0,\\Sigma_\\beta),\n\\]\n\\(d\\) denotes convergence distribution, \\(N(0,\\Sigma_\\beta)\\) denotes multivariate normal distribution mean vector 0 covariance matrix \\(\\Sigma_\\beta\\) \\(T\\) sample size used estimation.Let \\(g(\\beta) = (g_l(\\beta),..., g_m(\\beta))'\\) continuously differentiable function values \\(\\mathbb{R}^m\\), assume \\(\\partial g_i/\\partial \\beta' = (\\partial g_i/\\partial \\beta_j)\\) nonzero \\(\\beta\\) \\(= 1,\\dots, m\\). \n\\[\n\\sqrt{T}(g(\\hat\\beta)-g(\\beta))\\overset{d}{\\rightarrow}\\mathcal{N}\\left(0,\\frac{\\partial g}{\\partial \\beta'}\\Sigma_\\beta\\frac{\\partial g'}{\\partial \\beta}\\right).\n\\]\nUsing property, Lütkepohl (1990) provides asymptotic distributions \\(\\Psi_j\\)’s. following lines code can used get approximate confidence intervals IRFs.limit last two approaches (Monte Carlo Delta method) rely asymptotic results. Boostrapping approaches robust small-sample situations.","code":"\nirf.function <- function(THETA){\n  c <- THETA[1]\n  phi <- THETA[2:(p+1)]\n  if(q>0){\n    theta <- c(1,THETA[(1+p+1):(1+p+q)])\n  }else{theta <- 1}\n  sigma <- THETA[1+p+q+1]\n  r <- dim(Matrix.of.Exog)[2] - 1\n  beta <- THETA[(1+p+q+1+1):(1+p+q+1+(r+1))]\n  \n  irf <- sim.arma(0,phi,beta,sigma=sd(Ramey$ED3_TC,na.rm=TRUE),T=60,\n                  y.0=rep(0,length(x$phi)),nb.sim=1,make.IRF=1,\n                  X=NaN,beta=NaN)\n  return(irf)}\nIRF.0 <- 100*irf.function(x$THETA)\neps <- .00000001\nd.IRF <- NULL\nfor(i in 1:length(x$THETA)){\n  THETA.i <- x$THETA\n  THETA.i[i] <- THETA.i[i] + eps\n  IRF.i <- 100*irf.function(THETA.i)\n  d.IRF <- cbind(d.IRF,\n                 (IRF.i - IRF.0)/eps)}\nmat.var.cov.IRF <- d.IRF %*% x$I %*% t(d.IRF)"},{"path":"append.html","id":"Bootstrap","chapter":"8 Appendix","heading":"8.5.3 Bootstrap","text":"IRFs’ confidence intervals intervals 90% (95%, 75%, …) IRFs lie, repeat estimation large number times similar conditions (\\(T\\) observations). obviously , one sample: \\(\\{y_t\\}_{t=1,..,T}\\). can try construct samples.Bootstrapping consists :re-sampling \\(N\\) times, .e., constructing \\(N\\) samples \\(T\\) observations, using estimated\nVAR coefficients anda sample residuals distribution \\(N(0,BB')\\) (parametric approach), ora sample residuals drawn randomly set actual estimated residuals \\(\\{\\hat\\varepsilon_t\\}_{t=1,..,T}\\). (non-parametric approach).re-estimating SVAR \\(N\\) times.algorithm:Construct sample\n\\[\ny_t^{(k)}=\\widehat{\\Phi}_1 y_{t-1}^{(k)} + \\dots + \\widehat{\\Phi}_p y_{t-p}^{(k)} + \\hat\\varepsilon_t^{(k)},\n\\]\n\\(\\hat\\varepsilon_{t}^{(k)}=\\hat\\varepsilon_{s_t^{(k)}}\\), \\(\\{s_1^{(k)},..,s_T^{(k)}\\}\\) random set \\(\\{1,..,T\\}^T\\).Re-estimate SVAR compute IRFs \\(\\{\\widehat{\\Psi}_j\\}^{(k)}\\).Perform \\(N\\) replications report median impulse response (confidence intervals).","code":""},{"path":"append.html","id":"statistical-tables","chapter":"8 Appendix","heading":"8.6 Statistical Tables","text":"Table 8.1: Quantiles \\(\\mathcal{N}(0,1)\\) distribution. \\(\\) \\(b\\) respectively row column number; corresponding cell gives \\(\\mathbb{P}(0<X\\le +b)\\), \\(X \\sim \\mathcal{N}(0,1)\\).Table 8.2: Quantiles Student-\\(t\\) distribution. rows correspond different degrees freedom (\\(\\nu\\), say); columns correspond different probabilities (\\(z\\), say). cell gives \\(q\\) s.t. \\(\\mathbb{P}(-q<X<q)=z\\), \\(X \\sim t(\\nu)\\).Table 8.3: Quantiles \\(\\chi^2\\) distribution. rows correspond different degrees freedom; columns correspond different probabilities.Table 8.4: Quantiles \\(\\mathcal{F}\\) distribution. columns rows correspond different degrees freedom (resp. \\(n_1\\) \\(n_2\\)). different panels correspond different probabilities (\\(\\alpha\\)) corresponding cell gives \\(z\\) s.t. \\(\\mathbb{P}(X \\le z)=\\alpha\\), \\(X \\sim \\mathcal{F}(n_1,n_2)\\).","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
