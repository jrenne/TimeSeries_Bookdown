<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 1 Basics | Introduction to Time Series</title>
<meta name="author" content="Jean-Paul Renne">
<meta name="description" content="1.1 Shocks and lag operator A time series is an infinite sequence of random variables indexed by time: \(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\), \(y_i...">
<meta name="generator" content="bookdown 0.34 with bs4_book()">
<meta property="og:title" content="Chapter 1 Basics | Introduction to Time Series">
<meta property="og:type" content="book">
<meta property="og:description" content="1.1 Shocks and lag operator A time series is an infinite sequence of random variables indexed by time: \(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\), \(y_i...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 1 Basics | Introduction to Time Series">
<meta name="twitter:description" content="1.1 Shocks and lag operator A time series is an infinite sequence of random variables indexed by time: \(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\), \(y_i...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.0/transition.js"></script><script src="libs/bs3compat-0.5.0/tabs.js"></script><script src="libs/bs3compat-0.5.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="my-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Introduction to Time Series</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Introduction to Time Series</a></li>
<li><a class="active" href="Intro.html"><span class="header-section-number">1</span> Basics</a></li>
<li><a class="" href="Univariate.html"><span class="header-section-number">2</span> Univariate processes</a></li>
<li><a class="" href="VAR.html"><span class="header-section-number">3</span> Multivariate models</a></li>
<li><a class="" href="forecasting.html"><span class="header-section-number">4</span> Forecasting</a></li>
<li><a class="" href="NonStat.html"><span class="header-section-number">5</span> Non-stationary processes</a></li>
<li><a class="" href="cointeg.html"><span class="header-section-number">6</span> Introduction to cointegration</a></li>
<li><a class="" href="ARCHGARCH.html"><span class="header-section-number">7</span> ARCH and GARCH models</a></li>
<li><a class="" href="append.html"><span class="header-section-number">8</span> Appendix</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="Intro" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> Basics<a class="anchor" aria-label="anchor" href="#Intro"><i class="fas fa-link"></i></a>
</h1>
<div id="shocks-and-lag-operator" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> Shocks and lag operator<a class="anchor" aria-label="anchor" href="#shocks-and-lag-operator"><i class="fas fa-link"></i></a>
</h2>
<p>A time series is an infinite sequence of random variables indexed by time: <span class="math inline">\(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\)</span>, <span class="math inline">\(y_i \in \mathbb{R}^k\)</span>. In practice, we only observe samples, typically: <span class="math inline">\(\{y_{1},\dots,y_T\}\)</span>.</p>
<p>Standard time series models are built using <strong>shocks</strong> that we will often denote by <span class="math inline">\(\varepsilon_t\)</span>. Typically, <span class="math inline">\(\mathbb{E}(\varepsilon_t)=0\)</span>. In many models, the shocks are supposed to be i.i.d., but there exist other (less restrictive) notions of shocks. In particular, the definition of many processes is based on white noises:</p>
<div class="definition">
<p><span id="def:whitenoise" class="definition"><strong>Definition 1.1  (White noise) </strong></span>The process <span class="math inline">\(\{\varepsilon_t\}_{t \in] -\infty,+\infty[}\)</span> is a white noise if, for all <span class="math inline">\(t\)</span>:</p>
<ol style="list-style-type: lower-alpha">
<li>
<span class="math inline">\(\mathbb{E}(\varepsilon_t)=0\)</span>,</li>
<li>
<span class="math inline">\(\mathbb{E}(\varepsilon_t^2)=\sigma^2&lt;\infty\)</span>,</li>
<li>for all <span class="math inline">\(s\ne t\)</span>, <span class="math inline">\(\mathbb{E}(\varepsilon_t \varepsilon_s)=0\)</span>.</li>
</ol>
</div>
<p>Another type of shocks that are commonly used are Martingale Difference Sequences:</p>
<div class="definition">
<p><span id="def:MDS" class="definition"><strong>Definition 1.2  (Martingale Difference Sequence) </strong></span>The process <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> is a martingale difference sequence (MDS) if <span class="math inline">\(\mathbb{E}(|\varepsilon_{t}|)&lt;\infty\)</span> and if, for all <span class="math inline">\(t\)</span>,
<span class="math display">\[
\underbrace{\mathbb{E}_{t-1}(\varepsilon_{t})}_{\mbox{conditional on the past}}=0.
\]</span></p>
</div>
<p>By definition, if <span class="math inline">\(y_t\)</span> is a martingale, then <span class="math inline">\(y_{t}-y_{t-1}\)</span> is a MDS.</p>
<div class="example">
<p><span id="exm:ARCH" class="example"><strong>Example 1.1  (ARCH process) </strong></span>The Autoregressive conditional heteroskedasticity process—studied in Section <a href="ARCHGARCH.html#ARCHGARCH">7</a>—is an example of shock that satisfies the white noise and MDS definitions, but that is not i.i.d.:
<span class="math display">\[
\varepsilon_{t} = \sigma_t \times z_{t},
\]</span>
where <span class="math inline">\(z_t \sim i.i.d.\,\mathcal{N}(0,1)\)</span> and <span class="math inline">\(\sigma_t^2 = w + \alpha \varepsilon_{t-1}^2\)</span> (with <span class="math inline">\(w&gt;0\)</span> and <span class="math inline">\(0 \leq \alpha &lt;1\)</span>).</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fl">500</span></span>
<span><span class="va">w</span> <span class="op">&lt;-</span> <span class="fl">1</span>; <span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fl">.98</span></span>
<span><span class="va">all_epsilon</span> <span class="op">&lt;-</span> <span class="cn">NULL</span>; <span class="va">epsilon_1</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">t</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="cn">T</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">w</span> <span class="op">+</span> <span class="va">alpha</span> <span class="op">*</span> <span class="va">epsilon_1</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="va">epsilon</span> <span class="op">&lt;-</span> <span class="va">sigma</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="va">all_epsilon</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">all_epsilon</span>,<span class="va">epsilon</span><span class="op">)</span></span>
<span>  <span class="va">epsilon_1</span> <span class="op">&lt;-</span> <span class="va">epsilon</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.15</span>,<span class="fl">.95</span>,<span class="fl">.1</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">all_epsilon</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>     ylab<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">epsilon</span><span class="op">[</span><span class="va">t</span><span class="op">]</span><span class="op">)</span>,xlab<span class="op">=</span><span class="st">""</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:simulGARCHointro"></span>
<img src="TimeSeries_files/figure-html/simulGARCHointro-1.png" alt="Simulation of $\varepsilon_t$, where $\varepsilon_{t} = \sigma_t \times z_{t}$, with $z_t \sim i.i.d.\,\mathcal{N}(0,1)$." width="95%"><p class="caption">
Figure 1.1: Simulation of <span class="math inline">\(\varepsilon_t\)</span>, where <span class="math inline">\(\varepsilon_{t} = \sigma_t \times z_{t}\)</span>, with <span class="math inline">\(z_t \sim i.i.d.\,\mathcal{N}(0,1)\)</span>.
</p>
</div>
</div>
<div class="example">
<p><span id="exm:whiteNotMDS" class="example"><strong>Example 1.2  </strong></span>A white noise process is not necessarily a MDS. This is for instance the case for following process:
<span class="math display">\[
\varepsilon_{t} = z_t + z_{t-1}z_{t-2},
\]</span>
where <span class="math inline">\(z_t \sim i.i.d.\mathcal{N}(0,1)\)</span>.</p>
<p>Indeed, it can be shown that the <span class="math inline">\(\varepsilon_t\)</span>’s are not correlated through time, we have <span class="math inline">\(\mathbb{E}_{t-1}(\varepsilon_t)=z_{t-1}z_{t-2} \ne 0\)</span>.</p>
</div>
<p>In the following, to simplify the exposition, we will essentially consider <strong>strong white noises</strong>. A strong white noise is a particular case of white noise where the <span class="math inline">\(\varepsilon_{t}\)</span>’s are serially independent. Using the law of iterated expectation, it can be shown that a strong white noise is a martingale difference sequence. Indeed, when the <span class="math inline">\(\varepsilon_{t}\)</span>’s are serially independent, we have that <span class="math inline">\(\mathbb{E}(\varepsilon_{t}|\varepsilon_{t-1},\varepsilon_{t-2},\dots)=\mathbb{E}(\varepsilon_{t})=0\)</span>, as <span class="math inline">\(f_{\mathcal{E}_t}(\varepsilon_t|\varepsilon_{t-1},\varepsilon_{t-2},\dots)=f_{\mathcal{E}_t}(\varepsilon_t)\)</span>.</p>
<p>Let us now introduce the lag operator. The lag operator, denoted by <span class="math inline">\(L\)</span>, is defined on the time series space and is defined by:
<span class="math display" id="eq:lagOp">\[\begin{equation}
L: \{y_t\}_{t=-\infty}^{+\infty} \rightarrow \{w_t\}_{t=-\infty}^{+\infty} \quad \mbox{with} \quad w_t = y_{t-1}.\tag{1.1}
\end{equation}\]</span></p>
<p>It is easily seen that we have <span class="math inline">\(L^2 y_t =L(L y_t) = y_{t-2}\)</span> and, more generally, <span class="math inline">\(L^k y_t = y_{t-k}\)</span>.</p>
<p>Consider a process <span class="math inline">\(y_t\)</span> whose law of motion is <span class="math inline">\(y_t = \mu + \phi y_{t-1} + \varepsilon_t\)</span>, where the <span class="math inline">\(\varepsilon_t\)</span>’s are i.i.d. <span class="math inline">\(\mathcal{N}(0,\sigma^2)\)</span> (this is an AR(1) process, as we will see in Section <a href="Univariate.html#Univariate">2</a>). Using the lag operator, the dynamics of <span class="math inline">\(y_t\)</span> can be expressed as follows:
<span class="math display">\[
(1-\phi L) y_t = \mu + \varepsilon_t.
\]</span></p>
</div>
<div id="conditional-and-unconditional-moments" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> Conditional and unconditional moments<a class="anchor" aria-label="anchor" href="#conditional-and-unconditional-moments"><i class="fas fa-link"></i></a>
</h2>
<p>If it exists, the <strong>unconditional (or marginal) mean</strong> of the random variable <span class="math inline">\(y_t\)</span> is given by:
<span class="math display">\[
\mu_t := \mathbb{E}(y_t) = \int_{-\infty}^{\infty} y_t f_{Y_t}(y_t) dy_t,
\]</span>
where <span class="math inline">\(f_{Y_t}\)</span> is the unconditional, or marginal, density (p.d.f.) of <span class="math inline">\(y_t\)</span>. Note that, in the general case, <span class="math inline">\(Y_t\)</span> and <span class="math inline">\(Y_{t-1}\)</span>, may have different densities; that is, in general, <span class="math inline">\(f_{Y_t} \ne f_{Y_{t-1}}\)</span> (and, in particular, <span class="math inline">\(\mu_t \ne \mu_{t-1}\)</span>).</p>
<p>Similarly, if it exists, the <strong>unconditional (or marginal) variance</strong> of the random variable <span class="math inline">\(y_t\)</span> is:
<span class="math display">\[
\mathbb{V}ar(y_t) = \int_{-\infty}^{\infty} (y_t - \mathbb{E}(y_t))^2 f_{Y_t}(y_t) dy_t.
\]</span></p>
<div class="definition">
<p><span id="def:autocov" class="definition"><strong>Definition 1.3  (Autocovariance) </strong></span>The <span class="math inline">\(j^{th}\)</span> autocovariance of <span class="math inline">\(y_t\)</span> is given by:
<span class="math display">\[\begin{eqnarray*}
\gamma_{j,t} &amp;:=&amp; \mathbb{E}([y_t - \mathbb{E}(y_t)][y_{t-j} - \mathbb{E}(y_{t-j})])\\
&amp;=&amp; \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \dots \int_{-\infty}^{\infty} [y_t - \mathbb{E}(y_t)][y_{t-j} - \mathbb{E}(y_{t-j})] \times\\
&amp;&amp; f_{Y_t,Y_{t-1},\dots,Y_{t-j}}(y_t,y_{t-1},\dots,y_{t-j}) dy_t dy_{t-1} \dots dy_{t-j},
\end{eqnarray*}\]</span>
where <span class="math inline">\(f_{Y_t,Y_{t-1},\dots,Y_{t-j}}(y_t,y_{t-1},\dots,y_{t-j})\)</span> is the joint distribution of <span class="math inline">\(y_t,y_{t-1},\dots,y_{t-j}\)</span>.</p>
</div>
<p>In particular, note that <span class="math inline">\(\gamma_{0,t} = \mathbb{V}ar(y_t)\)</span>.</p>
<div class="definition">
<p><span id="def:covstat" class="definition"><strong>Definition 1.4  (Covariance stationarity) </strong></span>The process <span class="math inline">\(y_t\)</span> is covariance stationary —or weakly stationary— if, for all <span class="math inline">\(t\)</span> and <span class="math inline">\(j\)</span>,
<span class="math display">\[
\mathbb{E}(y_t) = \mu \quad \mbox{and} \quad \mathbb{E}\{(y_t - \mu)(y_{t-j} - \mu)\} = \gamma_j.
\]</span></p>
</div>
<p>Figure <a href="Intro.html#fig:NONNstat1">1.2</a> displays the simulation of a process that is not covariance stationary. This process follows <span class="math inline">\(y_t = 0.1t + \varepsilon_t\)</span>, where <span class="math inline">\(\varepsilon_t \sim\,i.i.d.\,\mathcal{N}(0,1)\)</span>. Indeed, for such a process, we have: <span class="math inline">\(\mathbb{E}(y_t)=0.1t\)</span>, which depends on <span class="math inline">\(t\)</span>.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fl">0.1</span><span class="op">*</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="cn">T</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="cn">T</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.95</span>,<span class="fl">.1</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,lwd<span class="op">=</span><span class="fl">2</span>,type<span class="op">=</span><span class="st">"l"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:NONNstat1"></span>
<img src="TimeSeries_files/figure-html/NONNstat1-1.png" alt="Example of a process that is not covariance stationary. it follows: $y_t = 0.1t + \varepsilon_t$, where $\varepsilon_t \sim \mathcal{N}(0,1)$." width="95%"><p class="caption">
Figure 1.2: Example of a process that is not covariance stationary. it follows: <span class="math inline">\(y_t = 0.1t + \varepsilon_t\)</span>, where <span class="math inline">\(\varepsilon_t \sim \mathcal{N}(0,1)\)</span>.
</p>
</div>
<div class="definition">
<p><span id="def:strictstat" class="definition"><strong>Definition 1.5  (Strict stationarity) </strong></span>The process <span class="math inline">\(y_t\)</span> is strictly stationary if, for all <span class="math inline">\(t\)</span> and all sets of integers <span class="math inline">\(J=\{j_1,\dots,j_n\}\)</span>, the distribution of <span class="math inline">\((y_{t},y_{t+j_1},\dots,y_{t+j_n})\)</span> depends on <span class="math inline">\(J\)</span> but not on <span class="math inline">\(t\)</span>.</p>
</div>
<p>The following process is covariance stationary but not strictly stationary:
<span class="math display">\[
y_t = \mathbb{I}_{\{t&lt;1000\}}\varepsilon_{1,t}+\mathbb{I}_{\{t\ge1000\}}\varepsilon_{2,t},
\]</span>
where <span class="math inline">\(\varepsilon_{1,t} \sim \mathcal{N}(0,1)\)</span> and <span class="math inline">\(\varepsilon_{2,t} \sim \sqrt{\frac{\nu - 2}{\nu}} t(\nu)\)</span> and <span class="math inline">\(\nu = 4\)</span><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Indeed, &lt;span class="math inline"&gt;\(\mathbb{E}[y_t]=\mathbb{I}_{\{t&amp;lt;1000\}}\times \underbrace{\mathbb{E}[\varepsilon_{1,t}]}_{=0}+\mathbb{I}_{\{t\ge1000\}} \times \underbrace{\mathbb{E}[\varepsilon_{2,t}]}_{=0}=0\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\mathbb{V}ar(y_t)=\mathbb{I}_{\{t&amp;lt;1000\}}\times \underbrace{\mathbb{V}ar(\varepsilon_{1,t})}_{=1}+\mathbb{I}_{\{t\ge1000\}} \times \underbrace{\mathbb{V}ar(\varepsilon_{2,t})}_{=\left(\sqrt{\frac{\nu - 2}{\nu}}\right)^2 \times \frac{\nu}{\nu - 2}=1}=1\)&lt;/span&gt; and for &lt;span class="math inline"&gt;\(\forall j \geq 1\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\gamma_{j,t}=\gamma_j=0\)&lt;/span&gt; as the &lt;span class="math inline"&gt;\(\varepsilon_t\)&lt;/span&gt;’s are serially uncorrelated.&lt;/p&gt;'><sup>1</sup></a>. A simulated path is displayed in Figure <a href="Intro.html#fig:NONNstat2">1.3</a>.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fl">2000</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="cn">T</span><span class="op">)</span></span>
<span><span class="va">y</span><span class="op">[</span><span class="op">(</span><span class="cn">T</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">rt</a></span><span class="op">(</span>n <span class="op">=</span> <span class="cn">T</span><span class="op">/</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">1</span>,df<span class="op">=</span><span class="fl">4</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.95</span>,<span class="fl">.15</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,type<span class="op">=</span><span class="st">"l"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="op">-</span><span class="fl">2.58</span>,col<span class="op">=</span><span class="st">"red"</span>,lty<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">2.58</span>,col<span class="op">=</span><span class="st">"red"</span>,lty<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:NONNstat2"></span>
<img src="TimeSeries_files/figure-html/NONNstat2-1.png" alt="Example of a process that is covariance stationary but not strictly stationary. The red lines delineate the 99\% confidence interval of the standard normal distribution ($\pm 2.58$)." width="95%"><p class="caption">
Figure 1.3: Example of a process that is covariance stationary but not strictly stationary. The red lines delineate the 99% confidence interval of the standard normal distribution (<span class="math inline">\(\pm 2.58\)</span>).
</p>
</div>
<div class="proposition">
<p><span id="prp:gammaMinus" class="proposition"><strong>Proposition 1.1  </strong></span>If <span class="math inline">\(y_t\)</span> is covariance stationary, then <span class="math inline">\(\gamma_j = \gamma_{-j}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>Since <span class="math inline">\(y_t\)</span> is covariance stationary, the covariance between <span class="math inline">\(y_t\)</span> and <span class="math inline">\(y_{t-j}\)</span> (i.e., <span class="math inline">\(\gamma_j\)</span>) is the same as that between <span class="math inline">\(y_{t+j}\)</span> and <span class="math inline">\(y_{t+j-j}\)</span> (i.e. <span class="math inline">\(\gamma_{-j}\)</span>).</p>
</div>
<div class="definition">
<p><span id="def:autocor" class="definition"><strong>Definition 1.6  (Auto-correlation) </strong></span>The <span class="math inline">\(j^{th}\)</span> auto-correlation of a covariance-stationary process is:
<span class="math display">\[
\rho_j = \frac{\gamma_j}{\gamma_0}.
\]</span></p>
</div>
<p>Consider a long historical time series of the Swiss GDP growth, taken from the <span class="citation">Jordà, Schularick, and Taylor (<a href="references.html#ref-JST_2017">2017</a>)</span> dataset.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Version 6 of the dataset, available on &lt;a href="https://www.macrohistory.net"&gt;this website&lt;/a&gt;.&lt;/p&gt;'><sup>2</sup></a></p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">JST</span><span class="op">)</span>;<span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">JST</span>,<span class="va">iso</span><span class="op">==</span><span class="st">"CHE"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.95</span>,<span class="fl">.1</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">data</span><span class="op">$</span><span class="va">growth</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">gdp</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">data</span><span class="op">$</span><span class="va">gdp</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">year</span>,<span class="va">data</span><span class="op">$</span><span class="va">growth</span>,type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">growth</span>,na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,col<span class="op">=</span><span class="st">"blue"</span>,lty<span class="op">=</span><span class="fl">2</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:autocov"></span>
<img src="TimeSeries_files/figure-html/autocov-1.png" alt="Annual growth rate of Swiss GDP, based on the Jorda-Schularick-Taylor Macrohistory Database." width="95%"><p class="caption">
Figure 1.4: Annual growth rate of Swiss GDP, based on the Jorda-Schularick-Taylor Macrohistory Database.
</p>
</div>
<p>Figure <a href="Intro.html#fig:autocov2">1.5</a> shows two scatter plots. In the first one, the coordinates of each point are of the form <span class="math inline">\((y_{t-1},y_t)\)</span>. Hence, the slope of the OLS regression line (in blue) is <span class="math inline">\(\widehat{\mathbb{C}ov}(y_t,y_{t-1})/\widehat{\mathbb{V}ar}(y_{t-1})\)</span> (where the hats indicate that these moments are the sample ones). Assuming that the process is covariance stationary, the slope therefore is an estimate of the auto-correlation of order one. By the same logic, the slope of the blue line in the second plot is an estimate of the auto-correlation of order three.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:autocov2"></span>
<img src="TimeSeries_files/figure-html/autocov2-1.png" alt="For order $j$, the slope of the blue line is, approximately, $\hat{\gamma}_j/\widehat{\mathbb{V}ar}(y_t)$, where hats indicate sample moments." width="95%"><p class="caption">
Figure 1.5: For order <span class="math inline">\(j\)</span>, the slope of the blue line is, approximately, <span class="math inline">\(\hat{\gamma}_j/\widehat{\mathbb{V}ar}(y_t)\)</span>, where hats indicate sample moments.
</p>
</div>
</div>
<div id="central-limit-theorem-clt-for-persistent-processes" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> Central Limit Theorem (CLT) for persistent processes<a class="anchor" aria-label="anchor" href="#central-limit-theorem-clt-for-persistent-processes"><i class="fas fa-link"></i></a>
</h2>
<p>This subsection shows that the Central Limit Theorem (CLT, see Theorem <a href="append.html#thm:LindbergLevyCLT">8.5</a>) can be extended to cases where the observations are auto-correlated.</p>
<div class="theorem">
<p><span id="thm:CLTcovstat" class="theorem"><strong>Theorem 1.1  (Central Limit Theorem for covariance-stationary processes) </strong></span>If process <span class="math inline">\(y_t\)</span> is covariance stationary and if the series of autocovariances is absolutely summable (<span class="math inline">\(\sum_{j=-\infty}^{+\infty} |\gamma_j| &lt;\infty\)</span>), then:
<span class="math display" id="eq:TCL4ts">\[\begin{eqnarray}
\bar{y}_T \overset{m.s.}{\rightarrow} \mu &amp;=&amp; \mathbb{E}(y_t) \tag{1.2}\\
\mbox{lim}_{T \rightarrow +\infty} T \mathbb{E}\left[(\bar{y}_T - \mu)^2\right] &amp;=&amp; \sum_{j=-\infty}^{+\infty} \gamma_j \tag{1.3}\\
\sqrt{T}(\bar{y}_T - \mu) &amp;\overset{d}{\rightarrow}&amp; \mathcal{N}\left(0,\sum_{j=-\infty}^{+\infty} \gamma_j \right) \tag{1.4}.
\end{eqnarray}\]</span></p>
<p>[Mean square (m.s.) and distribution (d.) convergences: see Definitions <a href="append.html#def:cvgceDistri">8.16</a> and <a href="append.html#def:convergenceLr">8.14</a>.]</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-2" class="proof"><em>Proof</em>. </span>The absolute value of the series of the autocovariances is finite as it is lower than the sum of the absolute values of the autocovariances which is finite by the absolute summability assumption. As a result, there exists a finite <span class="math inline">\(M \in \mathbb{R}\)</span> such that <span class="math inline">\(\sum_{j=-\infty}^{+\infty} \gamma_j=M\)</span> and Eq. <a href="Intro.html#eq:TCL2">(1.3)</a> implies Eq. <a href="Intro.html#eq:TCL20">(1.2)</a>. For Eq. <a href="Intro.html#eq:TCL2">(1.3)</a>, see Appendix <a href="append.html#AppendixProof">8.4</a>. For Eq. <a href="Intro.html#eq:TCL4ts">(1.4)</a>, see <span class="citation">Anderson (<a href="references.html#ref-Anderson_1971">1971</a>)</span>, p. 429.</p>
</div>
<div class="definition">
<p><span id="def:LRV" class="definition"><strong>Definition 1.7  (Long-run variance) </strong></span>Under the assumptions of Theorem <a href="Intro.html#thm:CLTcovstat">1.1</a>, the limit appearing in Eq. <a href="Intro.html#eq:TCL2">(1.3)</a> exists and is called <strong>long-run variance</strong>. It is denoted by <span class="math inline">\(S\)</span>, i.e.:
<span class="math display">\[
S = \Sigma_{j=-\infty}^{+\infty} \gamma_j  = \mbox{lim}_{T \rightarrow +\infty} T \mathbb{E}[(\bar{y}_T - \mu)^2].
\]</span></p>
</div>
<p>If <span class="math inline">\(y_t\)</span> is ergodic for second moments (see Def. <a href="append.html#def:ergod2nd">8.8</a>), a natural estimator of <span class="math inline">\(S\)</span> is:
<span class="math display" id="eq:covSmplMean">\[\begin{equation}
\hat\gamma_0 + 2 \sum_{\nu=1}^{q} \hat\gamma_\nu, \tag{1.5}
\end{equation}\]</span>
where <span class="math inline">\(\hat\gamma_\nu = \frac{1}{T}\sum_{\nu+1}^{T} (y_t - \bar{y})(y_{t-\nu} - \bar{y})\)</span>.</p>
<p>However, for small samples, Eq. <a href="Intro.html#eq:covSmplMean">(1.5)</a> does not necessarily result in a positive definite matrix. <span class="citation">Newey and West (<a href="references.html#ref-Newey_West_1987">1987</a>)</span> have proposed an estimator that does not have this defect. Their estimator is given by:
<span class="math display" id="eq:NWest">\[\begin{equation}
S^{NW}=\hat\gamma_0 + 2 \sum_{\nu=1}^{q}\left(1-\frac{\nu}{q+1}\right) \hat\gamma_\nu.\tag{1.6}
\end{equation}\]</span></p>
<p>Loosely speaking, Theorem <a href="Intro.html#thm:CLTcovstat">1.1</a> says that, for a given sample size, the higher the “persistency” of a process, the lower the accuracy of the sample mean as an estimate of the population mean. To illustrate, consider three processes that feature the same marginal variance (equal to one, say), but different autocorrelations: 0%, 70%, and 99.9%. Figure <a href="Intro.html#fig:TVTCL">1.6</a> displays simulated paths of such three processes. It indeed appears that, the larger the autocorrelation of the process, the further the sample mean (dashed red line) from the population mean (red solid line).</p>
<p>The same type of simulations can be performed using <a href="https://jrenne.shinyapps.io/MacroEc/">this web interface</a> (use panel “AR(1)”).</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:TVTCL"></span>
<img src="TimeSeries_files/figure-html/TVTCL-1.png" alt="The three samples have been simulated using the following data generating process: $x_t = \mu + \rho (x_{t-1}-\mu) + \sqrt{1-\rho^2}\varepsilon_t$, where $\varepsilon_t \sim \mathcal{N}(0,1)$. Case A: $\rho = 0$;  Case B: $\rho = 0.7$;  Case C: $\rho = 0.999$. In the three cases, $\mathbb{E}(x_t)=\mu=2$ and $\mathbb{V}ar(x_t)=1$. The dashed (respectively solid) red line indicate the sample (resp. unconditional) mean." width="100%"><p class="caption">
Figure 1.6: The three samples have been simulated using the following data generating process: <span class="math inline">\(x_t = \mu + \rho (x_{t-1}-\mu) + \sqrt{1-\rho^2}\varepsilon_t\)</span>, where <span class="math inline">\(\varepsilon_t \sim \mathcal{N}(0,1)\)</span>. Case A: <span class="math inline">\(\rho = 0\)</span>; Case B: <span class="math inline">\(\rho = 0.7\)</span>; Case C: <span class="math inline">\(\rho = 0.999\)</span>. In the three cases, <span class="math inline">\(\mathbb{E}(x_t)=\mu=2\)</span> and <span class="math inline">\(\mathbb{V}ar(x_t)=1\)</span>. The dashed (respectively solid) red line indicate the sample (resp. unconditional) mean.
</p>
</div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="index.html">Introduction to Time Series</a></div>
<div class="next"><a href="Univariate.html"><span class="header-section-number">2</span> Univariate processes</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#Intro"><span class="header-section-number">1</span> Basics</a></li>
<li><a class="nav-link" href="#shocks-and-lag-operator"><span class="header-section-number">1.1</span> Shocks and lag operator</a></li>
<li><a class="nav-link" href="#conditional-and-unconditional-moments"><span class="header-section-number">1.2</span> Conditional and unconditional moments</a></li>
<li><a class="nav-link" href="#central-limit-theorem-clt-for-persistent-processes"><span class="header-section-number">1.3</span> Central Limit Theorem (CLT) for persistent processes</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Introduction to Time Series</strong>" was written by Jean-Paul Renne. It was last built on 2025-02-19.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
