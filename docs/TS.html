<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 1 Time Series | Introduction to Time Series</title>
<meta name="author" content="Jean-Paul Renne">
<meta name="description" content="1.1 Introduction to time series A time series is an infinite sequence of random variables indexed by time: \(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\),...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 1 Time Series | Introduction to Time Series">
<meta property="og:type" content="book">
<meta property="og:description" content="1.1 Introduction to time series A time series is an infinite sequence of random variables indexed by time: \(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\),...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 1 Time Series | Introduction to Time Series">
<meta name="twitter:description" content="1.1 Introduction to time series A time series is an infinite sequence of random variables indexed by time: \(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\),...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="my-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Introduction to Time Series</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Introduction to Time Series</a></li>
<li><a class="active" href="TS.html"><span class="header-section-number">1</span> Time Series</a></li>
<li><a class="" href="append.html"><span class="header-section-number">2</span> Appendix</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="TS" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> Time Series<a class="anchor" aria-label="anchor" href="#TS"><i class="fas fa-link"></i></a>
</h1>
<div id="introduction-to-time-series" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> Introduction to time series<a class="anchor" aria-label="anchor" href="#introduction-to-time-series"><i class="fas fa-link"></i></a>
</h2>
<p>A time series is an infinite sequence of random variables indexed by time: <span class="math inline">\(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\)</span>, <span class="math inline">\(y_i \in \mathbb{R}^k\)</span>. In practice, we only observe samples, typically: <span class="math inline">\(\{y_{1},\dots,y_T\}\)</span>.</p>
<p>Standard time series models are built using <strong>shocks</strong> that we will often denote by <span class="math inline">\(\varepsilon_t\)</span>. Typically, <span class="math inline">\(\mathbb{E}(\varepsilon_t)=0\)</span>. In many models, the shocks are supposed to be i.i.d., but there exist other (less restrictive) notions of shocks. In particular, the definition of many processes is based on whote noises:</p>
<div class="definition">
<p><span id="def:whitenoise" class="definition"><strong>Definition 1.1  (White noise) </strong></span>The process <span class="math inline">\(\{\varepsilon_t\}_{t \in] -\infty,+\infty[}\)</span> is a white noise if, for all <span class="math inline">\(t\)</span>:</p>
<ol style="list-style-type: lower-alpha">
<li>
<span class="math inline">\(\mathbb{E}(\varepsilon_t)=0\)</span>,</li>
<li>
<span class="math inline">\(\mathbb{E}(\varepsilon_t^2)=\sigma^2&lt;\infty\)</span> and</li>
<li>for all <span class="math inline">\(s\ne t\)</span>, <span class="math inline">\(\mathbb{E}(\varepsilon_t \varepsilon_s)=0\)</span>.</li>
</ol>
</div>
<p>Another type of shocks that are commonly used are Martingale Difference Sequences:</p>
<div class="definition">
<p><span id="def:MDS" class="definition"><strong>Definition 1.2  (Martingale Difference Sequence) </strong></span>The process <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> is a martingale difference sequence (MDS) if <span class="math inline">\(\mathbb{E}(|\varepsilon_{t}|)&lt;\infty\)</span> and if, for all <span class="math inline">\(t\)</span>,
<span class="math display">\[
\underbrace{\mathbb{E}_{t-1}(\varepsilon_{t})}_{\mbox{Expectation conditional on the past}}=0.
\]</span></p>
</div>
<p>By definition, if <span class="math inline">\(y_t\)</span> is a martingale, then <span class="math inline">\(y_{t}-y_{t-1}\)</span> is a MDS.</p>
<div class="example">
<p><span id="exm:ARCH" class="example"><strong>Example 1.1  (ARCH process) </strong></span>The Autoregressive conditional heteroskedasticity (ARCH) process is an example of shock that satisfies the MDS definition but that is not i.i.d.:
<span class="math display">\[
\varepsilon_{t} = \sigma_t \times z_{t},
\]</span>
where <span class="math inline">\(z_t \sim i.i.d.\,\mathcal{N}(0,1)\)</span> and <span class="math inline">\(\sigma_t^2 = w + \alpha \varepsilon_{t-1}^2\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:whiteNotMDS" class="example"><strong>Example 1.2  </strong></span>A white noise process is not necessarily a MDS. This is for instance the following process:
<span class="math display">\[
\varepsilon_{t} = z_t + z_{t-1}z_{t-2},
\]</span>
where <span class="math inline">\(z_t \sim i.i.d.\mathcal{N}(0,1)\)</span>.</p>
</div>
<p>Let us now introduce the lag operator. The lag operator, denoted by <span class="math inline">\(L\)</span>, is defined on the time series space and is defined by:
<span class="math display" id="eq:lagOp">\[\begin{equation}
L: \{y_t\}_{t=-\infty}^{+\infty} \rightarrow \{w_t\}_{t=-\infty}^{+\infty} \quad \mbox{with} \quad w_t = y_{t-1}.\tag{1.1}
\end{equation}\]</span></p>
<p>We have: <span class="math inline">\(L^2 y_t = y_{t-2}\)</span> and, more generally, <span class="math inline">\(L^k y_t = y_{t-k}\)</span>.</p>
<p>Consider a time series <span class="math inline">\(y_t\)</span> defined by <span class="math inline">\(y_t = \mu + \phi y_{t-1} + \varepsilon_t\)</span>, where the <span class="math inline">\(\varepsilon_t\)</span>’s are i.i.d. <span class="math inline">\(\mathcal{N}(0,\sigma^2)\)</span>. Using the lag operator, the dynamics of <span class="math inline">\(y_t\)</span> can be expressed as follows:
<span class="math display">\[
(1-\phi L) y_t = \mu + \varepsilon_t.
\]</span></p>
<p>It is easily checked that we have <span class="math inline">\(L^2 y_t = y_{t-2}\)</span> and, generally, <span class="math inline">\(L^k y_t = y_{t-k}\)</span>.</p>
<p>If it exists, the <strong>unconditional (or marginal) mean</strong> of the random variable <span class="math inline">\(y_t\)</span> is given by:
<span class="math display">\[
\mu_t := \mathbb{E}(y_t) = \int_{-\infty}^{\infty} y_t f_{Y_t}(y_t) dy_t,
\]</span>
where <span class="math inline">\(f_{Y_t}\)</span> is the unconditional (or marginal) density of <span class="math inline">\(y_t\)</span>. Similarly, if it exists, the <strong>unconditional (or marginal) variance</strong> of the random variable <span class="math inline">\(y_t\)</span> is:
<span class="math display">\[
\mathbb{V}ar(y_t) = \int_{-\infty}^{\infty} (y_t - \mathbb{E}(y_t))^2 f_{Y_t}(y_t) dy_t.
\]</span></p>
<div class="definition">
<p><span id="def:autocov" class="definition"><strong>Definition 1.3  (Autocovariance) </strong></span>The <span class="math inline">\(j^{th}\)</span> autocovariance of <span class="math inline">\(y_t\)</span> is given by:
<span class="math display">\[\begin{eqnarray*}
\gamma_{j,t} &amp;:=&amp; \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \dots \int_{-\infty}^{\infty} [y_t - \mathbb{E}(y_t)][y_{t-j} - \mathbb{E}(y_{t-j})] \times\\
&amp;&amp; f_{Y_t,Y_{t-1},\dots,Y_{t-j}}(y_t,y_{t-1},\dots,y_{t-j}) dy_t dy_{t-1} \dots dy_{t-j} \\
&amp;=&amp; \mathbb{E}([y_t - \mathbb{E}(y_t)][y_{t-j} - \mathbb{E}(y_{t-j})]),
\end{eqnarray*}\]</span>
where <span class="math inline">\(f_{Y_t,Y_{t-1},\dots,Y_{t-j}}(y_t,y_{t-1},\dots,y_{t-j})\)</span> is the joint distribution of <span class="math inline">\(y_t,y_{t-1},\dots,y_{t-j}\)</span>.</p>
</div>
<p>In particular, <span class="math inline">\(\gamma_{0,t} = \mathbb{V}ar(y_t)\)</span>.</p>
<div class="definition">
<p><span id="def:covstat" class="definition"><strong>Definition 1.4  (Covariance stationarity) </strong></span>The process <span class="math inline">\(y_t\)</span> is covariance stationary —or weakly stationary— if, for all <span class="math inline">\(t\)</span> and <span class="math inline">\(j\)</span>,
<span class="math display">\[
\mathbb{E}(y_t) = \mu \quad \mbox{and} \quad \mathbb{E}\{(y_t - \mu)(y_{t-j} - \mu)\} = \gamma_j.
\]</span></p>
</div>
<p>Figure <a href="TS.html#fig:nonstat1">1.1</a> displays the simulation of a process that is not covariance stationary. This process follows <span class="math inline">\(y_t = 0.1t + \varepsilon_t\)</span>, where <span class="math inline">\(\varepsilon_t \sim\,i.i.d.\,\mathcal{N}(0,1)\)</span>. Indeed, for such a process, we have: <span class="math inline">\(\mathbb{E}(y_t)=0.1t\)</span>, which depends on <span class="math inline">\(t\)</span>.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:nonstat1"></span>
<img src="TimeSeries_files/figure-html/nonstat1-1.png" alt="Example of a process that is not covariance stationary ($y_t = 0.1t + \varepsilon_t$, where $\varepsilon_t \sim \mathcal{N}(0,1)$)." width="95%"><p class="caption">
Figure 1.1: Example of a process that is not covariance stationary (<span class="math inline">\(y_t = 0.1t + \varepsilon_t\)</span>, where <span class="math inline">\(\varepsilon_t \sim \mathcal{N}(0,1)\)</span>).
</p>
</div>
<div class="definition">
<p><span id="def:strictstat" class="definition"><strong>Definition 1.5  (Strict stationarity) </strong></span>The process <span class="math inline">\(y_t\)</span> is strictly stationary if, for all <span class="math inline">\(t\)</span> and all sets of integers <span class="math inline">\(J=\{j_1,\dots,j_n\}\)</span>, the distribution of <span class="math inline">\((y_{t},y_{t+j_1},\dots,y_{t+j_n})\)</span> depends on <span class="math inline">\(J\)</span> but not on <span class="math inline">\(t\)</span>.</p>
</div>
<p>The following process is covariance stationary but not strictly stationary:
<span class="math display">\[
y_t = \mathbb{I}_{\{t&lt;1000\}}\varepsilon_{1,t}+\mathbb{I}_{\{t\ge1000\}}\varepsilon_{2,t},
\]</span>
where <span class="math inline">\(\varepsilon_{1,t} \sim \mathcal{N}(0,1)\)</span> and <span class="math inline">\(\varepsilon_{2,t} \sim \sqrt{\frac{\nu - 2}{\nu}} t(\nu)\)</span> and <span class="math inline">\(\nu = 4\)</span>.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:nonstat2"></span>
<img src="TimeSeries_files/figure-html/nonstat2-1.png" alt="Example of a process that is covariance stationary but not strictly stationary. The red lines delineate the 99\% confidence interval of the standard normal distribution ($\pm 2.58$)." width="95%"><p class="caption">
Figure 1.2: Example of a process that is covariance stationary but not strictly stationary. The red lines delineate the 99% confidence interval of the standard normal distribution (<span class="math inline">\(\pm 2.58\)</span>).
</p>
</div>
<div class="proposition">
<p><span id="prp:gammaMinus" class="proposition"><strong>Proposition 1.1  </strong></span>If <span class="math inline">\(y_t\)</span> is covariance stationary, then <span class="math inline">\(\gamma_j = \gamma_{-j}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>Since <span class="math inline">\(y_t\)</span> is covariance stationary, the covariance between <span class="math inline">\(y_t\)</span> and <span class="math inline">\(y_{t-j}\)</span> (i.e <span class="math inline">\(\gamma_j\)</span>) is the same as that between <span class="math inline">\(y_{t+j}\)</span> and <span class="math inline">\(y_{t+j-j}\)</span> (i.e. <span class="math inline">\(\gamma_{-j}\)</span>).</p>
</div>
<div class="definition">
<p><span id="def:autocor" class="definition"><strong>Definition 1.6  (Auto-correlation) </strong></span>The <span class="math inline">\(j^{th}\)</span> auto-correlation of a covariance-stationary process is:
<span class="math display">\[
\rho_j = \frac{\gamma_j}{\gamma_0}.
\]</span></p>
</div>
<p>Consider a long historical time series of the Swiss GDP growth, taken from the <span class="citation">Jordà, Schularick, and Taylor (<a href="references.html#ref-JST_2017" role="doc-biblioref">2017</a>)</span> dataset.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Version 6 of the dataset, available on &lt;a href="https://www.macrohistory.net"&gt;this website&lt;/a&gt;.&lt;/p&gt;'><sup>1</sup></a></p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:autocov"></span>
<img src="TimeSeries_files/figure-html/autocov-1.png" alt="Annual growth rate of Swiss GDP, based on the Jorda-Schularick-Taylor Macrohistory Database." width="95%"><p class="caption">
Figure 1.3: Annual growth rate of Swiss GDP, based on the Jorda-Schularick-Taylor Macrohistory Database.
</p>
</div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:autocov2"></span>
<img src="TimeSeries_files/figure-html/autocov2-1.png" alt="For order $j$, the slope of the blue line is, approximately, $\hat{\gamma}_j/\widehat{\mathbb{V}ar}(y_t)$, where hats indicate sample moments." width="95%"><p class="caption">
Figure 1.4: For order <span class="math inline">\(j\)</span>, the slope of the blue line is, approximately, <span class="math inline">\(\hat{\gamma}_j/\widehat{\mathbb{V}ar}(y_t)\)</span>, where hats indicate sample moments.
</p>
</div>
<div class="definition">
<p><span id="def:ergodicity" class="definition"><strong>Definition 1.7  (Mean ergodicity) </strong></span>The covariance-stationary process <span class="math inline">\(y_t\)</span> is ergodic for the mean if:
<span class="math display">\[
\mbox{plim}_{T \rightarrow +\infty} \frac{1}{T}\sum_{t=1}^T y_t = \mathbb{E}(y_t).
\]</span></p>
</div>
<div class="definition">
<p><span id="def:ergod2nd" class="definition"><strong>Definition 1.8  (Second-moment ergodicity) </strong></span>The covariance-stationary process <span class="math inline">\(y_t\)</span> is ergodic for second moments if, for all <span class="math inline">\(j\)</span>:
<span class="math display">\[
\mbox{plim}_{T \rightarrow +\infty} \frac{1}{T}\sum_{t=1}^T (y_t-\mu) (y_{t-j}-\mu) = \gamma_j.
\]</span></p>
</div>
<p>It should be noted that ergodicity and stationarity are different properties. Typically if the process <span class="math inline">\(\{x_t\}\)</span> is such that, <span class="math inline">\(\forall t\)</span>, <span class="math inline">\(x_t \equiv y\)</span>, where <span class="math inline">\(y \sim\,\mathcal{N}(0,1)\)</span> (say), then <span class="math inline">\(\{x_t\}\)</span> is stationary but not ergodic.</p>
<div class="theorem">
<p><span id="thm:CLTcovstat" class="theorem"><strong>Theorem 1.1  (Central Limit Theorem for covariance-stationary processes) </strong></span>If process <span class="math inline">\(y_t\)</span> is covariance stationary and if the series of autocovariances is absolutely summable (<span class="math inline">\(\sum_{j=-\infty}^{+\infty} |\gamma_j| &lt;\infty\)</span>), then:
<span class="math display" id="eq:TCL4ts">\[\begin{eqnarray}
\bar{y}_T \overset{m.s.}{\rightarrow} \mu &amp;=&amp; \mathbb{E}(y_t) \tag{1.2}\\
\mbox{lim}_{T \rightarrow +\infty} T \mathbb{E}\left[(\bar{y}_T - \mu)^2\right] &amp;=&amp; \sum_{j=-\infty}^{+\infty} \gamma_j \tag{1.3}\\
\sqrt{T}(\bar{y}_T - \mu) &amp;\overset{d}{\rightarrow}&amp; \mathcal{N}\left(0,\sum_{j=-\infty}^{+\infty} \gamma_j \right) \tag{1.4}.
\end{eqnarray}\]</span></p>
<p>[Mean square (m.s.) and distribution (d.) convergences: see Definitions <a href="append.html#def:cvgceDistri">2.19</a> and <a href="append.html#def:convergenceLr">2.17</a>.]</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-2" class="proof"><em>Proof</em>. </span>By Proposition <a href="append.html#prp:absMs">2.8</a>, Eq. <a href="TS.html#eq:TCL2">(1.3)</a> implies Eq. <a href="TS.html#eq:TCL20">(1.2)</a>. For Eq. <a href="TS.html#eq:TCL2">(1.3)</a>, see Appendix <a href="append.html#AppendixProof">2.5</a>. For Eq. <a href="TS.html#eq:TCL4ts">(1.4)</a>, see <span class="citation">Anderson (<a href="references.html#ref-Anderson_1971" role="doc-biblioref">1971</a>)</span>, p. 429.</p>
</div>
<div class="definition">
<p><span id="def:LRV" class="definition"><strong>Definition 1.9  (Long-run variance) </strong></span>Under the assumptions of Theorem <a href="TS.html#thm:CLTcovstat">1.1</a>, the limit appearing in Eq. <a href="TS.html#eq:TCL2">(1.3)</a> exists and is called <strong>long-run variance</strong>. It is denoted by <span class="math inline">\(S\)</span>, i.e.:
<span class="math display">\[
S = \Sigma_{j=-\infty}^{+\infty} \gamma_j  = \mbox{lim}_{T \rightarrow +\infty} T \mathbb{E}[(\bar{y}_T - \mu)^2].
\]</span></p>
</div>
<p>If <span class="math inline">\(y_t\)</span> is ergodic for second moments (see Def. <a href="TS.html#def:ergod2nd">1.8</a>), a natural estimator of <span class="math inline">\(S\)</span> is:
<span class="math display" id="eq:covSmplMean">\[\begin{equation}
\hat\gamma_0 + 2 \sum_{\nu=1}^{q} \hat\gamma_\nu, \tag{1.5}
\end{equation}\]</span>
where <span class="math inline">\(\hat\gamma_\nu = \frac{1}{T}\sum_{\nu+1}^{T} (y_t - \bar{y})(y_{t-\nu} - \bar{y})\)</span>.</p>
<p>However, for small samples, Eq. <a href="TS.html#eq:covSmplMean">(1.5)</a> does not necessarily result in a positive definite matrix. <span class="citation">Newey and West (<a href="references.html#ref-Newey_West_1987" role="doc-biblioref">1987</a>)</span> have proposed an estimator that does not have this defect. Their estimator is given by:
<span class="math display" id="eq:NWest">\[\begin{equation}
S^{NW}=\hat\gamma_0 + 2 \sum_{\nu=1}^{q}\left(1-\frac{\nu}{q+1}\right) \hat\gamma_\nu.\tag{1.6}
\end{equation}\]</span></p>
<p>Loosely speaking, Theorem <a href="TS.html#thm:CLTcovstat">1.1</a> says that, for a given sample size, the higher the “persistency” of a proicess, the lower the accuracy of the sample mean as an estimate of the population mean. To illustrate, consider three processes that feature the same marginal variance (equal to one, say), but different autocorrelations: 0%, 70%, and 99.9%. Figure <a href="TS.html#fig:TVTCL">1.5</a> displays simulated paths of such three processes. It indeed appears that, the larger the autocorrelation of the process, the further the sample mean (dashed red line) from the population mean (red solid line).</p>
<p>The same type of simulations can be performed using <a href="https://jrenne.shinyapps.io/MacroEc/">this ShinyApp</a> (use panel “AR(1)”).</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:TVTCL"></span>
<img src="TimeSeries_files/figure-html/TVTCL-1.png" alt="The three samples have been simulated using the following data generating process: $x_t = \mu + \rho (x_{t-1}-\mu) + \sqrt{1-\rho^2}\varepsilon_t$, where $\varepsilon_t \sim \mathcal{N}(0,1)$. Case A: $\rho = 0$;  Case B: $\rho = 0.7$;  Case C: $\rho = 0.999$. In the three cases, $\mathbb{E}(x_t)=\mu=2$ and $\mathbb{V}ar(x_t)=1$." width="100%"><p class="caption">
Figure 1.5: The three samples have been simulated using the following data generating process: <span class="math inline">\(x_t = \mu + \rho (x_{t-1}-\mu) + \sqrt{1-\rho^2}\varepsilon_t\)</span>, where <span class="math inline">\(\varepsilon_t \sim \mathcal{N}(0,1)\)</span>. Case A: <span class="math inline">\(\rho = 0\)</span>; Case B: <span class="math inline">\(\rho = 0.7\)</span>; Case C: <span class="math inline">\(\rho = 0.999\)</span>. In the three cases, <span class="math inline">\(\mathbb{E}(x_t)=\mu=2\)</span> and <span class="math inline">\(\mathbb{V}ar(x_t)=1\)</span>.
</p>
</div>
</div>
<div id="univariate-processes" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> Univariate processes<a class="anchor" aria-label="anchor" href="#univariate-processes"><i class="fas fa-link"></i></a>
</h2>
<div id="moving-average-ma-processes" class="section level3" number="1.2.1">
<h3>
<span class="header-section-number">1.2.1</span> Moving Average (MA) processes<a class="anchor" aria-label="anchor" href="#moving-average-ma-processes"><i class="fas fa-link"></i></a>
</h3>
<div class="definition">
<p><span id="def:unlabeled-div-3" class="definition"><strong>Definition 1.10  </strong></span>Consider a white noise process <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> (Def. <a href="TS.html#def:whitenoise">1.1</a>). Then <span class="math inline">\(y_t\)</span> is a first-order moving average process if, for all <span class="math inline">\(t\)</span>:
<span class="math display">\[
y_t = \mu + \varepsilon_t + \theta \varepsilon_{t-1}.
\]</span></p>
</div>
<p>If <span class="math inline">\(\mathbb{E}(\varepsilon_t^2)=\sigma^2\)</span>, it is easily obtained that the unconditional mean and variances of <span class="math inline">\(y_t\)</span> are:
<span class="math display">\[
\mathbb{E}(y_t) = \mu, \quad \mathbb{V}ar(y_t) = (1+\theta^2)\sigma^2.
\]</span></p>
<p>The first auto-covariance is:
<span class="math display">\[
\gamma_1=\mathbb{E}\{(y_t - \mu)(y_{t-1} - \mu)\} = \theta \sigma^2.
\]</span></p>
<p>Higher-order auto-covariances are zero (<span class="math inline">\(\gamma_j=0\)</span> for <span class="math inline">\(j&gt;1\)</span>). Therefore: An MA(1) process is covariance-stationary (Def. <a href="TS.html#def:covstat">1.4</a>).</p>
<p>For a MA(1) process, the autocorrelation of order <span class="math inline">\(j\)</span> (see Def. <a href="TS.html#def:autocor">1.6</a>) is given by:
<span class="math display">\[
\rho_j =
\left\{
\begin{array}{lll}
1 &amp;\mbox{ if }&amp; j=0,\\
\theta / (1 + \theta^2) &amp;\mbox{ if }&amp; j = 1\\
0 &amp;\mbox{ if }&amp; j&gt;1.
\end{array}
\right.
\]</span></p>
<p>Notice that process <span class="math inline">\(y_t\)</span> defined through:
<span class="math display">\[
y_t = \mu + \varepsilon_t +\theta \varepsilon_{t-1},
\]</span>
where <span class="math inline">\(\mathbb{V}ar(\varepsilon_t)=\sigma^2\)</span>, has the same mean and autocovariances as
<span class="math display">\[
y_t = \mu + \varepsilon^*_t +\frac{1}{\theta}\varepsilon^*_{t-1},
\]</span>
where <span class="math inline">\(\mathbb{V}ar(\varepsilon^*_t)=\theta^2\sigma^2\)</span>. That is, even if we perfectly know the mean and auto-covariances of this process, it is not possible to identify which specification is the one that has been used to generate the data. Only one of these two specifications is said to be <em>fundamental</em>, that is the one that satisfies <span class="math inline">\(|\theta_1|&lt;1\)</span> (see Eq. <a href="TS.html#eq:invertible">(1.28)</a>).</p>
<div class="definition">
<p><span id="def:MAq" class="definition"><strong>Definition 1.11  (MA(q) process) </strong></span>A <span class="math inline">\(q^{th}\)</span> order Moving Average process is defined through:
<span class="math display">\[
y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q}.
\]</span>
where <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> is a white noise process (Def. <a href="TS.html#def:whitenoise">1.1</a>).</p>
</div>
<div class="proposition">
<p><span id="prp:covMAq" class="proposition"><strong>Proposition 1.2  (Covariance-stationarity of an MA(q) process) </strong></span>Finite-order Moving Average processes are covariance-stationary.</p>
<p>Moreover, the autocovariances of an MA(q) process (as defined in Def. <a href="TS.html#def:MAq">1.11</a>) are given by:
<span class="math display" id="eq:autocovMA">\[\begin{equation}
\gamma_j = \left\{ \begin{array}{ll} \sigma^2(\theta_j\theta_0 + \theta_{j+1}\theta_{1} +  \dots + \theta_{q}\theta_{q-j}) &amp;\mbox{for} \quad j \in \{0,\dots,q\} \\ 0 &amp;\mbox{for} \quad j&gt;q, \end{array} \right.\tag{1.7}
\end{equation}\]</span>
where we use the notation <span class="math inline">\(\theta_0=1\)</span>, and <span class="math inline">\(\mathbb{V}ar(\varepsilon_t)=\sigma^2\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span>The unconditional expectation of <span class="math inline">\(y_t\)</span> does not depend on time, since <span class="math inline">\(\mathbb{E}(y_t)=\mu\)</span>. Let’s turn to autocovariances. We can extend the series of the <span class="math inline">\(\theta_j\)</span>’s by setting <span class="math inline">\(\theta_j=0\)</span> for <span class="math inline">\(j&gt;q\)</span>. We then have:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}((y_t-\mu)(y_{t-j}-\mu)) &amp;=&amp; \mathbb{E}\left[(\theta_0 \varepsilon_t +\theta_1 \varepsilon_{t-1} + \dots +\theta_j \varepsilon_{t-j}+\theta_{j+1} \varepsilon_{t-j-1} + \dots) \right.\times \\
&amp;&amp;\left. (\theta_0 \varepsilon_{t-j} +\theta_1 \varepsilon_{t-j-1} + \dots)\right].
\end{eqnarray*}\]</span>
Then use the fact that <span class="math inline">\(\mathbb{E}(\varepsilon_t\varepsilon_s)=0\)</span> if <span class="math inline">\(t \ne s\)</span> (because <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> is a white noise process).</p>
</div>
<p>Figure <a href="TS.html#fig:simMA">1.6</a> displays simulated paths of two MA processes (an MA(1) and an MA(4)). Such simulations can be produced by using panel “ARMA(p,q)” of <a href="https://jrenne.shinyapps.io/MacroEc/">this web interface</a>.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fl">100</span>;<span class="va">nb.sim</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">y.0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">c</span> <span class="op">&lt;-</span> <span class="fl">1</span>;<span class="va">phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span>;<span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span> <span class="co"># MA(1) specification</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span><span class="va">c</span>,<span class="va">phi</span>,<span class="va">theta</span>,<span class="va">sigma</span>,<span class="cn">T</span>,<span class="va">y.0</span>,<span class="va">nb.sim</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.2</span>,<span class="fl">.9</span>,<span class="fl">.2</span>,<span class="fl">.85</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>     main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>,<span class="st">"=1, "</span>,<span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,<span class="st">"=1"</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="va">c</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span> <span class="co"># MA(4) specification</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span><span class="va">c</span>,<span class="va">phi</span>,<span class="va">theta</span>,<span class="va">sigma</span>,<span class="cn">T</span>,<span class="va">y.0</span>,<span class="va">nb.sim</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>     main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>,<span class="st">"=...="</span>,<span class="va">theta</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span>,<span class="st">"=1"</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="va">c</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:simMA"></span>
<img src="TimeSeries_files/figure-html/simMA-1.png" alt="Simulation of MA processes." width="95%"><p class="caption">
Figure 1.6: Simulation of MA processes.
</p>
</div>
<p>What if the order <span class="math inline">\(q\)</span> of an MA(q) process gets infinite? The notion of <strong>infinite-order Moving Average process</strong> exists and is important in time series analysis. The (infinite) sequence of <span class="math inline">\(\theta_j\)</span> has to satisfy some conditions for such a process to be well-defined (see Theorem <a href="TS.html#thm:infMA">1.2</a> below). These conditions relate to the “summability” of <span class="math inline">\(\{\theta_{i}\}_{i\in\mathbb{N}}\)</span> (see Definition <a href="TS.html#def:summability">1.12</a>).</p>
<div class="definition">
<p><span id="def:summability" class="definition"><strong>Definition 1.12  (Absolute and square summability) </strong></span>The sequence <span class="math inline">\(\{\theta_{i}\}_{i\in\mathbb{N}}\)</span> is absolutely summable if <span class="math inline">\(\sum_{i=0}^{\infty}|\theta_i| &lt; + \infty\)</span>, and it is square summable if <span class="math inline">\(\sum_{i=0}^{\infty} \theta_i^2 &lt; + \infty\)</span>.</p>
</div>
<p>According to Prop. <a href="append.html#prp:absMs">2.8</a>, absolute summability implies square summability.</p>
<div class="theorem">
<p><span id="thm:infMA" class="theorem"><strong>Theorem 1.2  (Existence condition for an infinite MA process) </strong></span>If <span class="math inline">\(\{\theta_{i}\}_{i\in\mathbb{N}}\)</span> is square summable (see Def. <a href="TS.html#def:summability">1.12</a>) and if <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> is a white noise process (see Def. <a href="TS.html#def:whitenoise">1.1</a>), then
<span class="math display">\[
\mu + \sum_{i=0}^{+\infty} \theta_{i} \varepsilon_{t-i}
\]</span>
defines a well-behaved [covariance-stationary] process, called infinite-order MA process (MA(<span class="math inline">\(\infty\)</span>)).</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-5" class="proof"><em>Proof</em>. </span>See Appendix 3.A in Hamilton. “Well behaved” means that <span class="math inline">\(\Sigma_{i=0}^{T} \theta_{t-i} \varepsilon_{t-i}\)</span> converges in mean square (Def. <a href="append.html#def:convergenceLr">2.17</a>) to some random variable <span class="math inline">\(Z_t\)</span>. The proof makes use of the fact that:
<span class="math display">\[
\mathbb{E}\left[\left(\sum_{i=N}^{M}\theta_{i} \varepsilon_{t-i}\right)^2\right] = \sum_{i=N}^{M}|\theta_{i}|^2 \sigma^2,
\]</span>
and that, when <span class="math inline">\(\{\theta_{i}\}\)</span> is square summable, <span class="math inline">\(\forall \eta&gt;0\)</span>, <span class="math inline">\(\exists N\)</span> s.t. the right-hand-side term in the last equation is lower than <span class="math inline">\(\eta\)</span> for all <span class="math inline">\(M \ge N\)</span> (static Cauchy criterion, Theorem <a href="append.html#thm:cauchycritstatic">2.2</a>). This implies that <span class="math inline">\(\Sigma_{i=0}^{T} \theta_{i} \varepsilon_{t-i}\)</span> converges in mean square (stochastic Cauchy criterion, see Theorem <a href="append.html#thm:cauchycritstochastic">2.3</a>).</p>
</div>
<div class="proposition">
<p><span id="prp:momentsMAinf" class="proposition"><strong>Proposition 1.3  (First two moments of an infinite MA process) </strong></span>If <span class="math inline">\(\{\theta_{i}\}_{i\in\mathbb{N}}\)</span> is absolutely summable, i.e. if <span class="math inline">\(\sum_{i=0}^{\infty}|\theta_i| &lt; + \infty\)</span>, then</p>
<ol style="list-style-type: lower-roman">
<li>
<span class="math inline">\(y_t = \mu + \sum_{i=0}^{+\infty} \theta_{i} \varepsilon_{t-i}\)</span> exists (Theorem <a href="TS.html#thm:infMA">1.2</a>) and is such that:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(y_t) &amp;=&amp; \mu\\
\gamma_0 = \mathbb{E}([y_t-\mu]^2) &amp;=&amp; \sigma^2(\theta_0^2 +\theta_1^2 + \dots)\\
\gamma_j = \mathbb{E}([y_t-\mu][y_{t-j}-\mu]) &amp;=&amp; \sigma^2(\theta_0\theta_j + \theta_{1}\theta_{j+1} + \dots).
\end{eqnarray*}\]</span>
</li>
<li>Process <span class="math inline">\(y_t\)</span> has absolutely summable auto-covariances, which implies that the results of Theorem <a href="TS.html#thm:CLTcovstat">1.1</a> (Central Limit) apply.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-6" class="proof"><em>Proof</em>. </span>The absolute summability of <span class="math inline">\(\{\theta_{i}\}\)</span> and the fact that <span class="math inline">\(\mathbb{E}(\varepsilon^2)&lt;\infty\)</span> imply that the order of integration and summation is interchangeable (see Hamilton, 1994, Footnote p. 52), which proves (i). For (ii), see end of Appendix 3.A in Hamilton (1994).</p>
</div>
</div>
<div id="ARsection" class="section level3" number="1.2.2">
<h3>
<span class="header-section-number">1.2.2</span> Auto-Regressive (AR) processes<a class="anchor" aria-label="anchor" href="#ARsection"><i class="fas fa-link"></i></a>
</h3>
<div class="definition">
<p><span id="def:AR1" class="definition"><strong>Definition 1.13  (First-order AR process (AR(1))) </strong></span>Consider a white noise process <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> (see Def. <a href="TS.html#def:whitenoise">1.1</a>). Process <span class="math inline">\(y_t\)</span> is an AR(1) process if it is defined by the following difference equation:
<span class="math display">\[
y_t = c + \phi y_{t-1} + \varepsilon_t.
\]</span></p>
</div>
<p>If <span class="math inline">\(|\phi|\ge1\)</span>, <span class="math inline">\(y_t\)</span> is not stationary. Indeed, we have:
<span class="math display">\[
y_{t+k} = c + \varepsilon_{t+k} + \phi  ( c + \varepsilon_{t+k-1})+ \phi^2  ( c + \varepsilon_{t+k-2})+ \dots + \phi^{k-1}  ( c + \varepsilon_{t+1}) + \phi^k y_t.
\]</span>
Therefore, the conditional variance
<span class="math display">\[
\mathbb{V}ar_t(y_{t+k}) = \sigma^2(1 + \phi^2 + \phi^4 + \dots + \phi^{2(k-1)})
\]</span>
does not converge for large <span class="math inline">\(k\)</span>’s. This implies that <span class="math inline">\(\mathbb{V}ar(y_{t})\)</span> does not exist.</p>
<p>By contrast, if <span class="math inline">\(|\phi| &lt; 1\)</span>, one can see that:
<span class="math display">\[
y_t = c + \varepsilon_t + \phi  ( c + \varepsilon_{t-1})+ \phi^2  ( c + \varepsilon_{t-2})+ \dots + \phi^k  ( c + \varepsilon_{t-k}) + \dots
\]</span>
Hence, if <span class="math inline">\(|\phi| &lt; 1\)</span>, the unconditional mean and variance of <span class="math inline">\(y_t\)</span> are:
<span class="math display">\[
\mathbb{E}(y_t) = \frac{c}{1-\phi} =: \mu \quad \mbox{and} \quad \mathbb{V}ar(y_t) = \frac{\sigma^2}{1-\phi^2}.
\]</span></p>
<p>Let us compute the <span class="math inline">\(j^{th}\)</span> autocovariance of the AR(1) process:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}([y_{t} - \mu][y_{t-j} - \mu]) &amp;=&amp; \mathbb{E}([\varepsilon_t + \phi  \varepsilon_{t-1}+ \phi^2 \varepsilon_{t-2} + \dots + \color{red}{\phi^j \varepsilon_{t-j}} + \color{blue}{\phi^{j+1} \varepsilon_{t-j-1}} \dots]\times \\
&amp;&amp;[\color{red}{\varepsilon_{t-j}} + \color{blue}{\phi \varepsilon_{t-j-1}} + \phi^2 \varepsilon_{t-j-2} + \dots + \phi^k \varepsilon_{t-j-k} + \dots])\\
&amp;=&amp; \mathbb{E}(\color{red}{\phi^j \varepsilon_{t-j}^2}+\color{blue}{\phi^{j+2} \varepsilon_{t-j-1}^2}+\phi^{j+4} \varepsilon_{t-j-2}^2+\dots)\\
&amp;=&amp; \frac{\phi^j \sigma^2}{1 - \phi^2}.
\end{eqnarray*}\]</span></p>
<p>Therefore <span class="math inline">\(\rho_j = \phi^j\)</span>.</p>
<p>By what precedes, we have:</p>
<div class="proposition">
<p><span id="prp:statioAR1" class="proposition"><strong>Proposition 1.4  (Covariance-stationarity of an AR(1) process) </strong></span>The AR(1) process, as defined in Def. <a href="TS.html#def:AR1">1.13</a>, is covariance-stationary iff <span class="math inline">\(|\phi|&lt;1\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:ARp" class="definition"><strong>Definition 1.14  (AR(p) process) </strong></span>Consider a white noise process <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> (see Def. <a href="TS.html#def:whitenoise">1.1</a>). Process <span class="math inline">\(y_t\)</span> is a <span class="math inline">\(p^{th}\)</span>-order autoregressive process (AR(p)) if its dynamics is defined by the following difference equation (with <span class="math inline">\(\phi_p \ne 0\)</span>):
<span class="math display" id="eq:AR">\[\begin{equation}
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \varepsilon_t.\tag{1.8}
\end{equation}\]</span></p>
</div>
<p>As we will see, the covariance-stationarity of process <span class="math inline">\(y_t\)</span> hinges on matrix <span class="math inline">\(F\)</span> defined as:
<span class="math display" id="eq:F">\[\begin{equation}
F = \left[
\begin{array}{ccccc}
\phi_1 &amp; \phi_2 &amp; \dots&amp; &amp; \phi_p \\
1 &amp; 0 &amp;\dots &amp;&amp; 0 \\
0 &amp; 1 &amp;\dots &amp;&amp; 0 \\
\vdots &amp;  &amp; \ddots &amp;&amp; \vdots \\
0 &amp; 0 &amp;\dots &amp;1&amp; 0 \\
\end{array}
\right].\tag{1.9}
\end{equation}\]</span></p>
<p>Note that this matrix <span class="math inline">\(F\)</span> is such that if <span class="math inline">\(y_t\)</span> follows Eq. <a href="TS.html#eq:AR">(1.8)</a>, then process <span class="math inline">\(\mathbf{y}_t\)</span> follows:
<span class="math display">\[
\mathbf{y}_t = \mathbf{c} + F \mathbf{y}_{t-1} + \boldsymbol\xi_t
\]</span>
with
<span class="math display">\[
\mathbf{c} =
\left[\begin{array}{c}
c\\
0\\
\vdots\\
0
\end{array}\right],
\quad
\boldsymbol\xi_t =
\left[\begin{array}{c}
\varepsilon_t\\
0\\
\vdots\\
0
\end{array}\right],
\quad
\mathbf{y}_t =
\left[\begin{array}{c}
y_t\\
y_{t-1}\\
\vdots\\
y_{t-p+1}
\end{array}\right].
\]</span></p>
<!-- :::{.definition #dynmult name="Dynamic multiplier"} -->
<!-- The **dynamic multiplier** of $y_{t+j}$ w.r.t. $\varepsilon_{t}$ is $\dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}}$. -->
<!-- ::: -->
<!-- Eq. \@ref(eq:Fyt) implies that we have: $\dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} = (F^j)_{[1,1]}$ -->
<!-- (for any $M,i,j$, $(M)_{[i,j]}$ denotes the $(i,j)$ element of matrix $M$). -->
<!-- Let us assume that the eigenvalues of $F$ (see Def. \@ref(def:determinant)), denoted by $\lambda_1,\dots,\lambda_p$, are distinct. -->
<!-- Then, there exists a nonsingular matrix $P$ such that: -->
<!-- $$ -->
<!-- F = P -->
<!-- \left[ -->
<!-- \begin{array}{cccc} -->
<!-- \lambda_1 & 0 & \dots& 0 \\ -->
<!-- 0 & \lambda_2 & 0 & \dots \\ -->
<!-- &&\ddots\\ -->
<!-- 0 & \dots & 0& \lambda_p\\ -->
<!-- \end{array} -->
<!-- \right] -->
<!-- P^{-1} = P D P^{-1}. -->
<!-- $$ -->
<!-- It can be seen that: -->
<!-- $$ -->
<!-- F^j = P -->
<!-- \left[ -->
<!-- \begin{array}{cccc} -->
<!-- \lambda_1^j & 0 & \dots& 0 \\ -->
<!-- 0 & \lambda_2^j & 0 & \dots \\ -->
<!-- &&\ddots\\ -->
<!-- 0 & \dots & 0& \lambda_p^j\\ -->
<!-- \end{array} -->
<!-- \right] -->
<!-- P^{-1}. -->
<!-- $$ -->
<!-- Hence, the dynamic multiplier of $y_{t+j}$ w.r.t. $\varepsilon_{t}$ is given by: -->
<!-- $$ -->
<!-- \dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} = \sum_i (P)_{[1,i]}(P^{-1})_{[i,1]}\lambda_i^j. -->
<!-- $$ -->
<!-- Denoting by $c_i$ the scalar $(P)_{[1,i]}(P^{-1})_{[i,1]}$, we have $\dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} = \sum_i c_i\lambda_i^j$. If the eigenvalues of $F$ are distinct and nonzero, then $c_i \ne 0$. Therefore, if $\exists i$ s.t. $|\lambda_i|>1$ then: -->
<!-- $$ -->
<!-- \left| \dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} \right| \underset{j \rightarrow \infty}{\rightarrow} \infty. -->
<!-- $$ -->
<div class="proposition">
<p><span id="prp:Feigen" class="proposition"><strong>Proposition 1.5  (The eigenvalues of matrix F) </strong></span>The eigenvalues of <span class="math inline">\(F\)</span> (defined by Eq. <a href="TS.html#eq:F">(1.9)</a>) are the solutions of:
<span class="math display" id="eq:Feigen">\[\begin{equation}
\lambda^p - \phi_1 \lambda^{p-1} - \dots - \phi_{p-1}\lambda - \phi_p = 0.\tag{1.10}
\end{equation}\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:stability" class="proposition"><strong>Proposition 1.6  (Covariance-stationarity of an AR(p) process) </strong></span>These four statements are equivalent:</p>
<ol style="list-style-type: lower-roman">
<li>Process <span class="math inline">\(\{y_t\}\)</span>, defined in Def. <a href="TS.html#def:ARp">1.14</a>, is covariance-stationary.</li>
<li>The eigenvalues of <span class="math inline">\(F\)</span> (as defined Eq. <a href="TS.html#eq:F">(1.9)</a>) lie strictly within the unit circle.</li>
<li>The roots of Eq. <a href="TS.html#eq:outside">(1.11)</a> (below) lie strictly outside the unit circle.
<span class="math display" id="eq:outside">\[\begin{equation}
1 - \phi_1 z - \dots - \phi_{p-1}z^{p-1} - \phi_p z^p = 0.\tag{1.11}
\end{equation}\]</span>
</li>
<li>The roots of Eq. <a href="TS.html#eq:inside">(1.12)</a> (below) lie strictly inside the unit circle.
<span class="math display" id="eq:inside">\[\begin{equation}
\lambda^p - \phi_1 \lambda^{p-1} - \dots - \phi_{p-1}\lambda - \phi_p = 0.\tag{1.12}
\end{equation}\]</span>
</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>Proof</em>. </span>We consider the case where the eigenvalues of <span class="math inline">\(F\)</span> are distinct; Jordan decomposition can be used in the general case. When the eigenvalues of <span class="math inline">\(F\)</span> are distinct, <span class="math inline">\(F\)</span> admits the following spectral decomposition: <span class="math inline">\(F = PDP^{-1}\)</span>, where <span class="math inline">\(D\)</span> is diagonal. Using the notations introduced in Eq. <a href="TS.html#eq:F">(1.9)</a>, we have:
<span class="math display">\[
\mathbf{y}_{t} = \mathbf{c} + F \mathbf{y}_{t-1} + \boldsymbol\xi_{t}.
\]</span>
Let’s introduce <span class="math inline">\(\mathbf{d} = P^{-1}\mathbf{c}\)</span>, <span class="math inline">\(\mathbf{z}_t = P^{-1}\mathbf{y}_t\)</span> and <span class="math inline">\(\boldsymbol\eta_t = P^{-1}\boldsymbol\xi_t\)</span>. We have:
<span class="math display">\[
\mathbf{z}_{t} = \mathbf{d} + D \mathbf{z}_{t-1} + \boldsymbol\eta_{t}.
\]</span>
Because <span class="math inline">\(D\)</span> is diagonal, the different component of <span class="math inline">\(\mathbf{z}_t\)</span>, denoted by <span class="math inline">\(z_{i,t}\)</span>, follow AR(1) processes. The (scalar) autoregressive parameters of these AR(1) processes are the diagonal entries of <span class="math inline">\(D\)</span> –which also are the eigenvalues of <span class="math inline">\(F\)</span>– that we denote by <span class="math inline">\(\lambda_i\)</span>.</p>
<p>Process <span class="math inline">\(y_t\)</span> is covariance-stationary iff <span class="math inline">\(\mathbf{y}_{t}\)</span> also is covariance-stationary, which is the case iff all <span class="math inline">\(z_{i,t}\)</span>, <span class="math inline">\(i \in [1,p]\)</span>, are covariance-stationary. By Prop. <a href="TS.html#prp:statioAR1">1.4</a>, process <span class="math inline">\(z_{i,t}\)</span> is covariance-stationary iff <span class="math inline">\(|\lambda_i|&lt;1\)</span>. This proves that (i) is equivalent to (ii). Prop. <a href="TS.html#prp:Feigen">1.5</a> further proves that (ii) is equivalent to (iv). Finally, it is easily seen that (iii) is equivalent to (iv) (as long as <span class="math inline">\(\phi_p \ne 0\)</span>).</p>
</div>
<!-- Note that we have: -->
<!-- \begin{equation} -->
<!-- \bv{y}_{t+j} = \boldsymbol\xi_{t+j} + F \boldsymbol\xi_{t+j-1} + F^2 \boldsymbol\xi_{t+j-2} + \dots + F^{j-1} \boldsymbol\xi_{t+1} + F^{j} \bv{y}_{t} -->
<!-- \end{equation} -->
<!-- or -->
<!-- \begin{equation} -->
<!-- \bv{y}_{t+j} = \boldsymbol\xi_{t+j} + F \boldsymbol\xi_{t+j-1} + F^2 \boldsymbol\xi_{t+j-2} + \dots + F^{j} \boldsymbol\xi_{t} + F^{j+1} \bv{y}_{t-1}.(\#eq:Fyt) -->
<!-- \end{equation} -->
<p>Using the lag operator (see Eq <a href="TS.html#eq:lagOp">(1.1)</a>), if <span class="math inline">\(y_t\)</span> is a covariance-stationary AR(p) process (Def. <a href="TS.html#def:ARp">1.14</a>), we can write:
<span class="math display">\[
y_t = \mu + \psi(L)\varepsilon_t,
\]</span>
where
<span class="math display">\[\begin{equation}
\psi(L) = (1 - \phi_1 L - \dots - \phi_p L^p)^{-1},
\end{equation}\]</span>
and
<span class="math display" id="eq:EAR">\[\begin{equation}
\mu = \mathbb{E}(y_t) = \dfrac{c}{1-\phi_1 -\dots - \phi_p}.\tag{1.13}
\end{equation}\]</span></p>
<p>In the following lines of codes, we compute the eigenvalues of the <span class="math inline">\(F\)</span> matrices associated with the following processes (where <span class="math inline">\(\varepsilon_t\)</span> is a white noise):
<span class="math display">\[\begin{eqnarray*}
x_t &amp;=&amp; 0.9 x_{t-1} -0.2 x_{t-2} + \varepsilon_t\\
y_t &amp;=&amp; 1.1 y_{t-1} -0.3 y_{t-2} + \varepsilon_t\\
w_t &amp;=&amp; 1.4 w_{t-1} -0.7 w_{t-2} + \varepsilon_t\\
z_t &amp;=&amp; 0.9 z_{t-1} +0.2 z_{t-2} + \varepsilon_t
\end{eqnarray*}\]</span></p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="cn">F</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.9</span>,<span class="fl">1</span>,<span class="op">-</span><span class="fl">.2</span>,<span class="fl">0</span><span class="op">)</span>,<span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">lambda_x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="cn">F</span><span class="op">)</span><span class="op">$</span><span class="va">values</span></span>
<span><span class="cn">F</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.1</span>,<span class="op">-</span><span class="fl">.3</span><span class="op">)</span></span>
<span><span class="va">lambda_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="cn">F</span><span class="op">)</span><span class="op">$</span><span class="va">values</span></span>
<span><span class="cn">F</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.4</span>,<span class="op">-</span><span class="fl">.7</span><span class="op">)</span></span>
<span><span class="va">lambda_w</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="cn">F</span><span class="op">)</span><span class="op">$</span><span class="va">values</span></span>
<span><span class="cn">F</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.9</span>,<span class="fl">.2</span><span class="op">)</span></span>
<span><span class="va">lambda_z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="cn">F</span><span class="op">)</span><span class="op">$</span><span class="va">values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">lambda_x</span>,<span class="va">lambda_y</span>,<span class="va">lambda_w</span>,<span class="va">lambda_z</span><span class="op">)</span></span></code></pre></div>
<pre><code>##                         [,1]                  [,2]
## lambda_x 0.500000+0.0000000i  0.4000000+0.0000000i
## lambda_y 0.600000+0.0000000i  0.5000000+0.0000000i
## lambda_w 0.700000+0.4582576i  0.7000000-0.4582576i
## lambda_z 1.084429+0.0000000i -0.1844289+0.0000000i</code></pre>
<p>The absolute values of the eigenvalues associated with process <span class="math inline">\(w_t\)</span> are both equal to 0.837. Therefore, according to Proposition <a href="TS.html#prp:stability">1.6</a>, processes <span class="math inline">\(x_t\)</span>, <span class="math inline">\(y_t\)</span>, and <span class="math inline">\(w_t\)</span> are covariance-stationary, but not <span class="math inline">\(z_t\)</span> (because the absolute value of one of the eigenvalues of the <span class="math inline">\(F\)</span> matrix associated with this process is larger than 1).</p>
<p>The computation of the autocovariances of <span class="math inline">\(y_t\)</span> is based on the so-called <strong>Yule-Walker equations</strong> (Eq. <a href="TS.html#eq:gammas">(1.14)</a>). Let’s rewrite Eq. <a href="TS.html#eq:AR">(1.8)</a>:
<span class="math display">\[
(y_t-\mu) = \phi_1 (y_{t-1}-\mu) + \phi_2 (y_{t-2}-\mu) + \dots + \phi_p (y_{t-p}-\mu) + \varepsilon_t.
\]</span>
Multiplying both sides by <span class="math inline">\(y_{t-j}-\mu\)</span> and taking expectations leads to the (Yule-Walker) equations:
<span class="math display" id="eq:gammas">\[\begin{equation}
\gamma_j = \left\{
\begin{array}{l}
\phi_1 \gamma_{j-1}+\phi_2 \gamma_{j-2}+ \dots + \phi_p \gamma_{j-p} \quad if \quad j&gt;0\\
\phi_1 \gamma_{1}+\phi_2 \gamma_{2}+ \dots + \phi_p \gamma_{p} + \sigma^2 \quad for \quad j=0.
\end{array}
\right.\tag{1.14}
\end{equation}\]</span>
Using <span class="math inline">\(\gamma_j = \gamma_{-j}\)</span> (Prop. <a href="TS.html#prp:gammaMinus">1.1</a>), one can express <span class="math inline">\((\gamma_0,\gamma_1,\dots,\gamma_{p})\)</span> as functions of <span class="math inline">\((\sigma^2,\phi_1,\dots,\phi_p)\)</span>.</p>
</div>
<div id="ar-ma-processes" class="section level3" number="1.2.3">
<h3>
<span class="header-section-number">1.2.3</span> AR-MA processes<a class="anchor" aria-label="anchor" href="#ar-ma-processes"><i class="fas fa-link"></i></a>
</h3>
<div class="definition">
<p><span id="def:ARMApq" class="definition"><strong>Definition 1.15  (ARMA(p,q) process) </strong></span><span class="math inline">\(\{y_t\}\)</span> is an ARMA(<span class="math inline">\(p\)</span>,<span class="math inline">\(q\)</span>) process if its dynamics is described by the following equation:
<span class="math display" id="eq:ARMApq">\[\begin{equation}
y_t = c + \underbrace{\phi_1 y_{t-1} + \dots + \phi_p y_{t-p}}_{\mbox{AR part}} + \underbrace{\varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q}}_{\mbox{MA part}},\tag{1.15}
\end{equation}\]</span>
where <span class="math inline">\(\{\varepsilon_t\}_{t \in ] -\infty,+\infty[}\)</span> is a white noise process (see Def. <a href="TS.html#def:whitenoise">1.1</a>).</p>
</div>
<div class="proposition">
<p><span id="prp:statioARMApq" class="proposition"><strong>Proposition 1.7  (Stationarity of an ARMA(p,q) process) </strong></span>The ARMA(<span class="math inline">\(p\)</span>,<span class="math inline">\(q\)</span>) process defined in <a href="TS.html#def:ARMApq">1.15</a> is covariance stationary iff the roots of
<span class="math display">\[
1 - \phi_1 z - \dots - \phi_p z^p=0
\]</span>
lie strictly outside the unit circle or, equivalently, iff those of
<span class="math display">\[
\lambda^p - \phi_1 \lambda^{p-1} - \dots - \phi_p=0
\]</span>
lie strictly within the unit circle.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-8" class="proof"><em>Proof</em>. </span>The proof of Prop. <a href="TS.html#prp:stability">1.6</a> can be adapted to the present case.</p>
</div>
<p>We can write:
<span class="math display">\[
(1 - \phi_1 L - \dots - \phi_p L^p)y_t = c + (1 + \theta_1 L + \dots + \theta_q L^q)\varepsilon_t.
\]</span></p>
<p>If the roots of <span class="math inline">\(1 - \phi_1 z - \dots - \phi_p z^p=0\)</span> lie outside the unit circle, we have:
<span class="math display" id="eq:ARMAwold">\[\begin{equation}
y_t = \mu + \psi(L)\varepsilon_t,\tag{1.16}
\end{equation}\]</span>
where
<span class="math display">\[
\psi(L) = \frac{1 + \theta_1 L + \dots + \theta_q L^q}{1 - \phi_1 L - \dots - \phi_p L^p} \quad and \quad \mu = \dfrac{c}{1-\phi_1 -\dots - \phi_p}.
\]</span></p>
<p>Eq. <a href="TS.html#eq:ARMAwold">(1.16)</a> is the <strong>Wold representation</strong> of this ARMA process (see Theorem <a href="TS.html#thm:Wold">1.3</a> below).</p>
<p>The stationarity of the process depends only on the AR specification (or on the eigenvalues of matrix <span class="math inline">\(F\)</span>, exactly as in Prop. <a href="TS.html#prp:stability">1.6</a>). If the process is stationary, the weights in <span class="math inline">\(\psi(L)\)</span> decay at a geometric rate.</p>
</div>
<div id="PACFapproach" class="section level3" number="1.2.4">
<h3>
<span class="header-section-number">1.2.4</span> PACF approach to identify AR/MA processes<a class="anchor" aria-label="anchor" href="#PACFapproach"><i class="fas fa-link"></i></a>
</h3>
<p>We have seen that the <span class="math inline">\(k^{th}\)</span>-order auto-correlation of a MA(q) process is null if <span class="math inline">\(k&gt;q\)</span>. This is exploited, in practice, to determine the order of a MA process. Moreover, since this is not the case for an AR process, this can be used to distinguish an AR from an MA process.</p>
<p>There exists an equivalent approach to determine whether a process can be modeled as an AR process; it is based on partial auto-correlations:</p>
<!-- :::{.definition #partialC name="Partial correlation"} -->
<!-- The partial correlation between $X$ and $Y$, given $Z$, is the correlation between: -->
<!-- a. the residuals of the linear regression of $X$ on $Z$ and -->
<!-- b. the residuals of the linear regression of $Y$ on $Z$. -->
<!-- ::: -->
<div class="definition">
<p><span id="def:partialAC" class="definition"><strong>Definition 1.16  (Partial auto-correlation) </strong></span>In a time series context, the partial auto-correlation (<span class="math inline">\(\phi_{h,h}\)</span>) of process <span class="math inline">\(\{y_t\}\)</span> is defined as the partial correlation of <span class="math inline">\(y_{t+h}\)</span> and <span class="math inline">\(y_t\)</span> given <span class="math inline">\(y_{t+h-1},\dots,y_{t+1}\)</span>. (see Def. <a href="append.html#def:partialcorrel">2.5</a> for the definition of partial correlation.)</p>
</div>
<p>If <span class="math inline">\(h&gt;p\)</span>, the regression of <span class="math inline">\(y_{t+h}\)</span> on <span class="math inline">\(y_{t+h-1},\dots,y_{t+1}\)</span> is:
<span class="math display">\[
y_{t+h} = c + \phi_1 y_{t+h-1}+\dots+ \phi_p  y_{t+h-p} + \varepsilon_{t+h}.
\]</span>
The residuals of the latter regressions (<span class="math inline">\(\varepsilon_{t+h}\)</span>) are uncorrelated to <span class="math inline">\(y_t\)</span>. Then the partial autocorrelation is zero for <span class="math inline">\(h&gt;p\)</span>.</p>
<p>Besides, it can be shown that <span class="math inline">\(\phi_{p,p}=\phi_p\)</span>. Hence <span class="math inline">\(\phi_{p,p}=\phi_p\)</span> but <span class="math inline">\(\phi_{h,h}=0\)</span> for <span class="math inline">\(h&gt;p\)</span>. This can be used to determine the order of an AR process. By contrast (importantly) if <span class="math inline">\(y_t\)</span> follows an MA(q) process, then <span class="math inline">\(\phi_{k,k}\)</span> asymptotically approaches zero instead of cutting off abruptly.</p>
<p>As illustrated below, functions <code>acf</code> and <code>pacf</code> can be conveniently used to employ the (P)ACF approach. (Note also the use of function <code>sim.arma</code> to simulate ARMA processes.)</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.2</span>,<span class="fl">.9</span>,<span class="fl">.2</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">1</span><span class="op">)</span>;<span class="va">phi</span><span class="op">=</span><span class="fl">0</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span>c<span class="op">=</span><span class="fl">0</span>,<span class="va">phi</span>,<span class="va">theta</span>,sigma<span class="op">=</span><span class="fl">1</span>,T<span class="op">=</span><span class="fl">1000</span>,y.0<span class="op">=</span><span class="fl">0</span>,nb.sim<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y.sim</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/stats/acf.html">acf</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/stats/acf.html">pacf</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>;<span class="va">phi</span><span class="op">=</span><span class="fl">0.9</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span>c<span class="op">=</span><span class="fl">0</span>,<span class="va">phi</span>,<span class="va">theta</span>,sigma<span class="op">=</span><span class="fl">1</span>,T<span class="op">=</span><span class="fl">1000</span>,y.0<span class="op">=</span><span class="fl">0</span>,nb.sim<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y.sim</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/stats/acf.html">acf</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/stats/acf.html">pacf</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:pacf"></span>
<img src="TimeSeries_files/figure-html/pacf-1.png" alt="ACF/PACF analysis of two processes (MA process on the left, AR on the right)." width="100%"><p class="caption">
Figure 1.7: ACF/PACF analysis of two processes (MA process on the left, AR on the right).
</p>
</div>
</div>
<div id="wold-decomposition" class="section level3" number="1.2.5">
<h3>
<span class="header-section-number">1.2.5</span> Wold decomposition<a class="anchor" aria-label="anchor" href="#wold-decomposition"><i class="fas fa-link"></i></a>
</h3>
<p>The Wold decomposition is an important result in time series analysis:</p>
<div class="theorem">
<p><span id="thm:Wold" class="theorem"><strong>Theorem 1.3  (Wold decomposition) </strong></span>Any covariance-stationary process admits the following representation:
<span class="math display">\[
y_t = \mu + \sum_{0}^{+\infty} \theta_i \varepsilon_{t-i} + \kappa_t,
\]</span>
where</p>
<ul>
<li>
<span class="math inline">\(\theta_0 = 1\)</span>, <span class="math inline">\(\sum_{i=0}^{\infty} \theta_i^2 &lt; +\infty\)</span> (square summability, see Def. <a href="TS.html#def:summability">1.12</a>).</li>
<li>
<span class="math inline">\(\{\varepsilon_t\}\)</span> is a white noise (see Def. <a href="TS.html#def:whitenoise">1.1</a>); <span class="math inline">\(\varepsilon_t\)</span> is the error made when forecasting <span class="math inline">\(y_t\)</span> based on a linear combination of lagged <span class="math inline">\(y_t\)</span>’s (<span class="math inline">\(\varepsilon_t = y_t - \hat{\mathbb{E}}[y_t|y_{t-1},y_{t-2},\dots]\)</span>).</li>
<li>For any <span class="math inline">\(j \ge 1\)</span>, <span class="math inline">\(\kappa_t\)</span> is not correlated with <span class="math inline">\(\varepsilon_{t-j}\)</span>; but <span class="math inline">\(\kappa_t\)</span> can be perfectly forecasted based on a linear combination of lagged <span class="math inline">\(y_t\)</span>’s (i.e. <span class="math inline">\(\kappa_t = \hat{\mathbb{E}}(\kappa_t|y_{t-1},y_{t-2},\dots)\)</span>). <span class="math inline">\(\kappa_t\)</span> is called the <strong>deterministic component</strong> of <span class="math inline">\(y_t\)</span>.</li>
</ul>
</div>
<div class="proof">
<p><span id="unlabeled-div-9" class="proof"><em>Proof</em>. </span>See <span class="citation">Anderson (<a href="references.html#ref-Anderson_1971" role="doc-biblioref">1971</a>)</span>. Partial proof in <a href="http://faculty.wcas.northwestern.edu/~lchrist/finc520/wold.pdf">L. Christiano</a>.</p>
</div>
<p>For an ARMA process, the Wold representation is given by Eq. <a href="TS.html#eq:ARMAwold">(1.16)</a>. As detailed in Prop. <a href="TS.html#prp:computPsi">1.8</a>, it can be computed by recursively replacing the lagged <span class="math inline">\(y_t\)</span>’s in Eq. <a href="TS.html#eq:ARMApq">(1.15)</a>. In this case, the deterministic component (<span class="math inline">\(\kappa\)</span>) is null.</p>
</div>
<div id="impulse-response-functions-irfs-in-arma-models" class="section level3" number="1.2.6">
<h3>
<span class="header-section-number">1.2.6</span> Impulse Response Functions (IRFs) in ARMA models<a class="anchor" aria-label="anchor" href="#impulse-response-functions-irfs-in-arma-models"><i class="fas fa-link"></i></a>
</h3>
<p>Consider the ARMA(p,q) process defined in Def. <a href="TS.html#def:ARMApq">1.15</a>, whose associated sequence of white noise is <span class="math inline">\(\{\varepsilon_t\}\)</span>. Let us construct a novel (counterfactual) sequence of shocks <span class="math inline">\(\{\tilde\varepsilon_t^{(s)}\}\)</span>:
<span class="math display">\[
\tilde\varepsilon_t^{(s)} = \left\{
\begin{array}{lcc}
\varepsilon_{t} &amp; if &amp; t \ne s,\\
\varepsilon_{t} + \delta &amp;if&amp; t=s.
\end{array}
\right.
\]</span>
We denote by <span class="math inline">\(\{\tilde{y}_t^{(s)}\}\)</span> the process following Eq. <a href="TS.html#eq:ARMApq">(1.15)</a> where <span class="math inline">\(\{\varepsilon_t\}\)</span> is replaced with <span class="math inline">\(\{\tilde\varepsilon_t^{(s)}\}\)</span>. The time series <span class="math inline">\(\{\tilde{y}_t^{(s)}\}\)</span> is the counterfactual series <span class="math inline">\(\{y_t\}\)</span> that would have prevailed if <span class="math inline">\(\varepsilon_t\)</span> had been shifted by <span class="math inline">\(\delta\)</span> on date <span class="math inline">\(s\)</span> (and that would be the only change).</p>
<p>The relationship between <span class="math inline">\(\{y_t\}\)</span> and <span class="math inline">\(\{\tilde{y}_t^{(s)}\}\)</span> defines the <strong>dynamics multiplier</strong>. The ltter is denoted by <span class="math inline">\(\frac{\partial y_t}{\partial \varepsilon_{s}}\)</span> and is such that:
<span class="math display">\[
\tilde{y}_t^{(s)} = y_t + \frac{\partial y_t}{\partial \varepsilon_{s}}\delta.
\]</span>
We will see that the dynamic multipliers are closely related to the infinite MA representation (or <strong>Wold decomposition</strong>, Theorem <a href="TS.html#thm:Wold">1.3</a>) of <span class="math inline">\(y_t\)</span>:
<span class="math display">\[
y_t = \mu + \sum_{i=0}^{+\infty} \psi_i \varepsilon_{t-i}.
\]</span>
For <span class="math inline">\(t&lt;s\)</span>, we have <span class="math inline">\(y_t = \tilde{y}_t^{(s)}\)</span> (because <span class="math inline">\(\tilde{\varepsilon}_{t-i}= \varepsilon_{t-i}\)</span> for all <span class="math inline">\(i \ge 0\)</span> if <span class="math inline">\(t&lt;s\)</span>).</p>
<p>For <span class="math inline">\(t \ge s\)</span>:
<span class="math display">\[
\tilde{y}_t^{(s)} = \mu + \left( \sum_{i=0}^{t-s-1} \psi_i \varepsilon_{t-i} \right) + \psi_{t-s}(\varepsilon_{s}+\delta) + \left( \sum_{i=t-s+1}^{+\infty} \psi_i \varepsilon_{t-i} \right)=y_t + \frac{\partial y_t}{\partial \varepsilon_{s}}\delta.
\]</span>
Therefore, for <span class="math inline">\(t \ge s\)</span>, we have:
<span class="math display">\[
\boxed{\dfrac{\partial y_t}{\partial \varepsilon_{s}}=\psi_{t-s}.}
\]</span>
That is, <span class="math inline">\(\{y_t\}\)</span>’s dynamic multiplier of order <span class="math inline">\(k\)</span> is the same object as the <span class="math inline">\(k^{th}\)</span> loading <span class="math inline">\(\psi_k\)</span> in the Wold decomposition of <span class="math inline">\(\{y_t\}\)</span>. The sequence <span class="math inline">\(\left\{\dfrac{\partial y_{t+h}}{\partial \varepsilon_{t}}\right\}_{h \ge 0} \equiv \left\{\psi_h\right\}_{h \ge 0}\)</span> defines the <strong>impulse response function (IRF)</strong> of <span class="math inline">\(y_t\)</span> to the shock <span class="math inline">\(\varepsilon_t\)</span>.</p>
<p>For ARMA processes, the computation of the IRFs is easy:</p>
<div class="proposition">
<p><span id="prp:computPsi" class="proposition"><strong>Proposition 1.8  (IRF of an ARMA(p,q) process) </strong></span>The coefficients <span class="math inline">\(\psi_h\)</span>, that define the IRF of process <span class="math inline">\(y_t\)</span> to <span class="math inline">\(\varepsilon_t\)</span>, can be computed recursively as follows:</p>
<ol style="list-style-type: decimal">
<li>Set <span class="math inline">\(\psi_{-1}=\dots=\psi_{-p}=0\)</span>.</li>
<li>For <span class="math inline">\(h \ge 0\)</span>, (recursively) apply:
<span class="math display">\[
\psi_h = \phi_1 \psi_{h-1} + \dots + \phi_p \psi_{h-p} + \theta_h,
\]</span>
where <span class="math inline">\(\theta_h = 0\)</span> for <span class="math inline">\(h&gt;q\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-10" class="proof"><em>Proof</em>. </span>This is obtained by applying the operator <span class="math inline">\(\frac{\partial}{\partial \varepsilon_{t}}\)</span> on both sides of Eq. <a href="TS.html#eq:ARMApq">(1.15)</a>:
<span class="math display">\[
y_{t+h} = c + \phi_1 y_{t+h-1} + \dots + \phi_p y_{t+h-p} + \varepsilon_{t+h} + \theta_1 \varepsilon_{t+h-1} + \dots + \theta_q \varepsilon_{t+h-q}.
\]</span></p>
</div>
<p>Note that Proposition <a href="TS.html#prp:computPsi">1.8</a> constitutes a simple way to compute the MA(<span class="math inline">\(\infty\)</span>) representation (or Wold representation) of an ARMA process.</p>
<p>One can use function <code>sim.arma</code> of package <code>AEC</code> to compute ARMA’s IRFs (with the argument <code>make.IRF = 1</code>):</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fl">21</span> <span class="co"># number of periods for IRF</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span>;<span class="va">phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span>;<span class="va">c</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span><span class="va">c</span>,<span class="va">phi</span>,<span class="va">theta</span>,sigma<span class="op">=</span><span class="fl">1</span>,<span class="cn">T</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">phi</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                  nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">3</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.25</span>,<span class="fl">.95</span>,<span class="fl">.2</span>,<span class="fl">.85</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>,<span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>     main<span class="op">=</span><span class="st">"(a) Process 1"</span>,xlab<span class="op">=</span><span class="st">"Time after shock on epsilon"</span>,</span>
<span>     ylab<span class="op">=</span><span class="st">"Dynamic multiplier (shock on epsilon at t=0)"</span>,col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">.5</span><span class="op">)</span>;<span class="va">phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.6</span><span class="op">)</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span><span class="va">c</span>,<span class="va">phi</span>,<span class="va">theta</span>,sigma<span class="op">=</span><span class="fl">1</span>,<span class="cn">T</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">phi</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                  nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>,<span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>     main<span class="op">=</span><span class="st">"(b) Process 2"</span>,xlab<span class="op">=</span><span class="st">"Time after shock on epsilon"</span>,</span>
<span>     ylab<span class="op">=</span><span class="st">""</span>,col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span>;<span class="va">phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">.5</span>,<span class="fl">.4</span><span class="op">)</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span><span class="va">c</span>,<span class="va">phi</span>,<span class="va">theta</span>,sigma<span class="op">=</span><span class="fl">1</span>,<span class="cn">T</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">phi</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                  nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>,<span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>     main<span class="op">=</span><span class="st">"(c) Process 3"</span>,xlab<span class="op">=</span><span class="st">"Time after shock on epsilon"</span>,</span>
<span>     ylab<span class="op">=</span><span class="st">""</span>,col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:IRFarma"></span>
<img src="TimeSeries_files/figure-html/IRFarma-1.png" alt="IRFs associated with the three processes. Process 1 (MA(2)): $y_t = \varepsilon_t + \varepsilon_{t-1} + \varepsilon_{t-2}$. Process 2 (ARMA(1,1)): $y_{t}=0.6y_{t-1} + \varepsilon_t + 0.5\varepsilon_{t-1}$. Process 3 (ARMA(4,2)): $y_{t}=0.5y_{t-3} + 0.4y_{t-4} + \varepsilon_t + \varepsilon_{t-1} + \varepsilon_{t-2}$." width="100%"><p class="caption">
Figure 1.8: IRFs associated with the three processes. Process 1 (MA(2)): <span class="math inline">\(y_t = \varepsilon_t + \varepsilon_{t-1} + \varepsilon_{t-2}\)</span>. Process 2 (ARMA(1,1)): <span class="math inline">\(y_{t}=0.6y_{t-1} + \varepsilon_t + 0.5\varepsilon_{t-1}\)</span>. Process 3 (ARMA(4,2)): <span class="math inline">\(y_{t}=0.5y_{t-3} + 0.4y_{t-4} + \varepsilon_t + \varepsilon_{t-1} + \varepsilon_{t-2}\)</span>.
</p>
</div>
<p>Consider the annual Swiss GDP growth from the JST macro-history database. Let us first determine relevant orders for AR and MA processes using the (P)ACF approach.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">JST</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">JST</span>,<span class="va">iso</span><span class="op">==</span><span class="st">"CHE"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.95</span>,<span class="fl">.1</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">growth</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">gdp</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">data</span><span class="op">$</span><span class="va">gdp</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.95</span>,<span class="fl">.15</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">year</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span>,<span class="va">growth</span>,type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>,lty<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html">acf</a></span><span class="op">(</span><span class="va">growth</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html">pacf</a></span><span class="op">(</span><span class="va">growth</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:IRFgdp1"></span>
<img src="TimeSeries_files/figure-html/IRFgdp1-1.png" alt="(P)ACF analysis of Swiss GDP growth." width="100%"><p class="caption">
Figure 1.9: (P)ACF analysis of Swiss GDP growth.
</p>
</div>
<p>The two bottom plots of Figure <a href="TS.html#fig:IRFgdp1">1.9</a> suggest that either an MA(2) or an AR(1) could be used to model the GDP growth rate series. Figure <a href="TS.html#fig:IRFgdp2">1.10</a> shows the IRFs based on these two respective specifications.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit an AR process:</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.html">arima</a></span><span class="op">(</span><span class="va">growth</span>,order<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">phi</span> <span class="op">&lt;-</span> <span class="va">res</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fl">11</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span>c<span class="op">=</span><span class="fl">0</span>,<span class="va">phi</span>,theta<span class="op">=</span><span class="fl">1</span>,sigma<span class="op">=</span><span class="fl">1</span>,<span class="cn">T</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">phi</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                  nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.15</span>,<span class="fl">.95</span>,<span class="fl">.25</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>,<span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">3</span>,</span>
<span>     xlab<span class="op">=</span><span class="st">"Time after shock on epsilon"</span>,</span>
<span>     ylab<span class="op">=</span><span class="st">"Dynamic multiplier (shock on epsilon at t=0)"</span>,col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span>
<span><span class="co"># Fit a MA process:</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.html">arima</a></span><span class="op">(</span><span class="va">growth</span>,order<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">phi</span> <span class="op">&lt;-</span> <span class="fl">0</span>;<span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="va">res</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span>c<span class="op">=</span><span class="fl">0</span>,<span class="va">phi</span>,<span class="va">theta</span>,sigma<span class="op">=</span><span class="fl">1</span>,<span class="cn">T</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">phi</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                  nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>,<span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,lwd<span class="op">=</span><span class="fl">3</span>,col<span class="op">=</span><span class="st">"red"</span>,lty<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:IRFgdp2"></span>
<img src="TimeSeries_files/figure-html/IRFgdp2-1.png" alt="Dynamic response of Swiss annual growth to a shock on the innovation $\varepsilon_t$ at date $t=0$. The solid line corresponds to an AR(1) specification; the dashed line corresponds to a MA(2) specification." width="100%"><p class="caption">
Figure 1.10: Dynamic response of Swiss annual growth to a shock on the innovation <span class="math inline">\(\varepsilon_t\)</span> at date <span class="math inline">\(t=0\)</span>. The solid line corresponds to an AR(1) specification; the dashed line corresponds to a MA(2) specification.
</p>
</div>
<p>The same kind of algorithm can be used to compute the impact of an increase in an exogenous variable <span class="math inline">\(x_t\)</span> within an ARMAX(p,q,r) model (see next section).</p>
</div>
<div id="ARMAIRF" class="section level3" number="1.2.7">
<h3>
<span class="header-section-number">1.2.7</span> ARMA processes with exogenous variables (ARMA-X)<a class="anchor" aria-label="anchor" href="#ARMAIRF"><i class="fas fa-link"></i></a>
</h3>
<!-- Recall that $\{y_t\}$ follows an ARMAX(p,q,r) model if its dynamics is of the form (see Def. \@ref(def:ARMAX)): -->
<!-- \begin{eqnarray} -->
<!-- y_t &=& \underbrace{c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p}}_{\mbox{AR(p) part}} + \underbrace{\beta_0 x_t + \dots + \beta_{r} x_{t-r}}_{\mbox{X(r) part}} + \nonumber \\ -->
<!-- &&\underbrace{\varepsilon_t + \theta_1\varepsilon_{t-1}+\dots +\theta_{q}\varepsilon_{t-q}.}_{\mbox{MA(q) part}}(\#eq:armaxirf) -->
<!-- \end{eqnarray} -->
<!-- where $\{\varepsilon_t\}$ is an i.i.d. white noise sequence and $\{x_t\}$ is an exogenous variable. -->
<!-- This algorithm was presented in Prop. \@ref(prop:computPsiARMAX). -->
<p>ARMA processes do not allow to investigate the influence of an exogenous variable (say <span class="math inline">\(x_t\)</span>) on the variable of interest (say <span class="math inline">\(y_t\)</span>). When <span class="math inline">\(x_t\)</span> and <span class="math inline">\(y_t\)</span> have reciprocal influences, the Vector Autoregressive (VAR) model may be used (this tools will be studied later, in Section <a href="TS.html#VAR">1.3</a>). However, when one suspects that <span class="math inline">\(x_t\)</span> has an “exogenous” influence on <span class="math inline">\(y_t\)</span>, then a simple extension of the ARMA processes may be considered. Loosely speaking, <span class="math inline">\(x_t\)</span> has an “exogenous” influence on <span class="math inline">\(y_t\)</span> if <span class="math inline">\(y_t\)</span> does not affect <span class="math inline">\(x_t\)</span>. This extension is called ARMAX(p,q,r).</p>
<p>To begin with, let us formalize this notion of exogeneity. Consider a white noise sequence <span class="math inline">\(\{\varepsilon_t\}\)</span> (Def. <a href="TS.html#def:whitenoise">1.1</a>).</p>
<div class="definition">
<p><span id="def:exogeneity" class="definition"><strong>Definition 1.17  (Exogeneity) </strong></span>We say that <span class="math inline">\(x_t\)</span> is (strictly) exogenous to <span class="math inline">\(\{\varepsilon_t\}\)</span> if
<span class="math display">\[
\mathbb{E}(\varepsilon_t|\underbrace{\dots,x_{t+1}}_{\mbox{future}},\underbrace{x_t,x_{t-1},\dots}_{\mbox{present and past}}) = 0.
\]</span></p>
</div>
<!-- If $x_t$ is exogenous to $\{\varepsilon_t\}$ then, in particular: -->
<!-- \begin{equation} -->
<!-- \mathbb{E}(\varepsilon_t|x_{t-k},x_{t-k-1},\dots)=0 \mbox{ for any } k\ge0,(\#eq:expepsx) -->
<!-- \end{equation} -->
<!-- which implies that: -->
<!-- \begin{equation} -->
<!-- \mathbb{E}(\varepsilon_tx_t|x_{t-k},x_{t-k-1},\dots)=0 \mbox{ for any } k\ge0, -->
<!-- \end{equation} -->
<!-- or -->
<!-- \begin{equation} -->
<!-- \mathbb{C}ov(\varepsilon_t,x_t|x_{t-k},x_{t-k-1},\dots)=0 \mbox{ for any } k\ge0, -->
<!-- \end{equation} -->
<!-- Past values of the exogenous variable do not allow to predict present and future values of $\varepsilon_t$ (Eq. \@ref(eq:expepsx)). -->
<p>Hence, if <span class="math inline">\(\{x_t\}\)</span> is strictly exogenous to <span class="math inline">\(\varepsilon_t\)</span>, then past, present and future values of <span class="math inline">\(x_t\)</span> do not allow to predict the <span class="math inline">\(\varepsilon_t\)</span>’s.</p>
<p>In the following, we assume that <span class="math inline">\(\{x_t\}\)</span> is a covariance stationary process.</p>
<div class="definition">
<p><span id="def:ARMAX" class="definition"><strong>Definition 1.18  (ARMAX(p,q,r) model) </strong></span>The process <span class="math inline">\(\{y_t\}\)</span> is an ARMAX(p,q,r) if it follows a difference equation:
<span class="math display" id="eq:DLM">\[\begin{eqnarray}
y_t &amp;=&amp; \underbrace{c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p}}_{\mbox{AR(p) part}} + \underbrace{\beta_0 x_t + \dots + \beta_{r} x_{t-r}}_{\mbox{X(r) part}} + \nonumber \\
&amp;&amp;\underbrace{\varepsilon_t + \theta_1\varepsilon_{t-1}+\dots +\theta_{q}\varepsilon_{t-q}.}_{\mbox{MA(q) part}} \tag{1.17}
\end{eqnarray}\]</span>
where <span class="math inline">\(\{\varepsilon_t\}\)</span> is an i.i.d. white noise sequence and <span class="math inline">\(\{x_t\}\)</span> is exogenous to <span class="math inline">\(y_t\)</span>.</p>
</div>
<!-- The Autoregressive Distributed Lag (ADL) Model ADL(p,r) is an ARMAX(p,0,r) model (see @Stock_Watson_2003, Chapter 16). -->
<p>What is the effect of a one-unit increase in <span class="math inline">\(x_t\)</span> on <span class="math inline">\(y_t\)</span>? To address this question, this notion of “effect” has to be formalized. Let us introduce two related sequences of values for <span class="math inline">\(\{x\}\)</span>. Denote the first by <span class="math inline">\(\{a\}\)</span> and the second by <span class="math inline">\(\{\tilde{a}^t\}\)</span>. Further, we posit <span class="math inline">\(a_s = \tilde{a}_s^t\)</span> for all <span class="math inline">\(s \ne t\)</span>, and <span class="math inline">\(\tilde{a}_t^t = a_t+1\)</span>.</p>
<p>With these notations, we define <span class="math inline">\(\frac{\partial y_{t+h}}{\partial x_t}\)</span> as follows:
<span class="math display" id="eq:dynmultX">\[\begin{equation}
\frac{\partial y_{t+h}}{\partial x_t} := \mathbb{E}(y_{t+h}|\{x\} = \{\tilde{a}^t\}) - \mathbb{E}(y_{t+h}|\{x\} = \{a\}).\tag{1.18}
\end{equation}\]</span>
Under the exogeneity assumption, it is easily seen that
<!-- \begin{eqnarray*} -->
<!-- y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \\ -->
<!-- &&\beta_0 x_t + \dots + \beta_{r} x_{t-r} +\\ -->
<!-- &&\varepsilon_t + \theta_1\varepsilon_{t-1}+\dots +\theta_{q}\varepsilon_{t-q}. -->
<!-- \end{eqnarray*} --></p>
<p><span class="math display">\[
\frac{\partial y_t}{\partial x_t} = \beta_0.
\]</span>
Now, since
<span class="math display">\[\begin{eqnarray*}
y_{t+1} &amp;=&amp; c + \phi_1 y_{t} + \dots + \phi_p y_{t+1-p} + \beta_0 x_{t+1} + \dots + \beta_{r} x_{t+1-r} +\\
&amp;&amp;\varepsilon_{t+1} + \theta_1\varepsilon_{t}+\dots +\theta_{q}\varepsilon_{t+1-q},
\end{eqnarray*}\]</span>
and using the exogeneity assumption, we obtain:
<span class="math display">\[
\frac{\partial y_{t+1}}{\partial x_t} := \phi_1 \frac{\partial y_{t}}{\partial x_t} + \beta_1 = \phi_1\beta_0 + \beta_1.
\]</span>
This can be applied recursively to give <span class="math inline">\(\dfrac{\partial y_{t+h}}{\partial x_t}\)</span> for any <span class="math inline">\(h \ge 0\)</span>:</p>
<div class="proposition">
<p><span id="prp:computPsiARMAX" class="proposition"><strong>Proposition 1.9  (Dynamic multipliers in ARMAX models) </strong></span>One can recursively compute the dynamic multipliers <span class="math inline">\(\frac{\partial y_{t+h}}{\partial x_t}\)</span> as follows:</p>
<ol style="list-style-type: lower-roman">
<li>Initialization: <span class="math inline">\(\dfrac{\partial y_{t+h}}{\partial x_t}=0\)</span> for <span class="math inline">\(h&lt;0\)</span>.</li>
<li>For <span class="math inline">\(h \ge 0\)</span> and assuming that the first <span class="math inline">\(h-1\)</span> multipliers have been computed, we have:
<span class="math display" id="eq:dynmultX">\[\begin{eqnarray}
\dfrac{\partial y_{t+h}}{\partial x_t} &amp;=&amp; \phi_1 \dfrac{\partial y_{t+h-1}}{\partial x_t} + \dots + \phi_p \dfrac{\partial y_{t+h-p}}{\partial x_t} + \beta_h,\tag{1.18}
\end{eqnarray}\]</span>
where we use the notation <span class="math inline">\(\beta_h=0\)</span> if <span class="math inline">\(h&gt;r\)</span>.</li>
</ol>
</div>
<p>Remark that the resulting dynamic multipliers are the same as those obtained for an ARMA(p,r) model where the <span class="math inline">\(\theta_i\)</span>’s are replaced with <span class="math inline">\(\beta_i\)</span>’s (see Proposition <a href="TS.html#prp:computPsi">1.8</a> in Section <a href="TS.html#ARMAIRF">1.2.7</a>).</p>
<p>It has to be stressed that the definition of the dynamic multipliers (Eq. <a href="TS.html#eq:dynmultX">(1.18)</a>) does not reflect a potential persistency of the shock occuring on date <span class="math inline">\(t\)</span> in process <span class="math inline">\(\{x\}\)</span> itself. Going in this direction would necessitate to model the joint dynamics of <span class="math inline">\(x_t\)</span> (for instance using a VAR model , see Section <a href="TS.html#VAR">1.3</a>).</p>
<div class="example">
<p><span id="exm:OrangeJuice" class="example"><strong>Example 1.3  (Influence of the number of freezing days on the price of orange juice) </strong></span>This example is based on data used in <span class="citation">J. Stock and Watson (<a href="references.html#ref-Stock_Watson_2003" role="doc-biblioref">2003</a>)</span> (Chapter 16). The objective is to study the influence of the number of freezing days on the price of orange juice. Let us first estimate a ARMAX(0,0,12) model:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AER</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"FrozenJuice"</span><span class="op">)</span></span>
<span><span class="va">FJ</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">FrozenJuice</span><span class="op">)</span></span>
<span><span class="va">date</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/time.html">time</a></span><span class="op">(</span><span class="va">FrozenJuice</span><span class="op">)</span></span>
<span><span class="va">price</span> <span class="op">&lt;-</span> <span class="va">FJ</span><span class="op">$</span><span class="va">price</span><span class="op">/</span><span class="va">FJ</span><span class="op">$</span><span class="va">ppi</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">price</span><span class="op">)</span></span>
<span><span class="va">k</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">dprice</span> <span class="op">&lt;-</span> <span class="fl">100</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">price</span><span class="op">[</span><span class="op">(</span><span class="va">k</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">price</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">k</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">fdd</span> <span class="op">&lt;-</span> <span class="va">FJ</span><span class="op">$</span><span class="va">fdd</span><span class="op">[</span><span class="op">(</span><span class="va">k</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.95</span>,<span class="fl">.15</span>,<span class="fl">.75</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">date</span>,<span class="va">price</span>,type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,</span>
<span>     main<span class="op">=</span><span class="st">"(a) Price of orange Juice"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">date</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="va">dprice</span><span class="op">)</span>,type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,</span>
<span>     main<span class="op">=</span><span class="st">"(b) Monthly pct Change (y)"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">date</span>,<span class="va">FJ</span><span class="op">$</span><span class="va">fdd</span>,type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,</span>
<span>     main<span class="op">=</span><span class="st">"(c) Number of freezing days (x)"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="TimeSeries_files/figure-html/freez-1.png" width="672"></div>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nb.lags</span> <span class="op">&lt;-</span> <span class="fl">12</span></span>
<span><span class="va">FDD</span> <span class="op">&lt;-</span> <span class="va">FJ</span><span class="op">$</span><span class="va">fdd</span><span class="op">[</span><span class="op">(</span><span class="va">nb.lags</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span></span>
<span><span class="va">names.FDD</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">nb.lags</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">FDD</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">FDD</span>,<span class="va">FJ</span><span class="op">$</span><span class="va">fdd</span><span class="op">[</span><span class="op">(</span><span class="va">nb.lags</span><span class="op">+</span><span class="fl">1</span><span class="op">-</span><span class="va">i</span><span class="op">)</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">i</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">names.FDD</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">names.FDD</span>,<span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">" Lag "</span>,<span class="fu"><a href="https://rdrr.io/r/base/toString.html">toString</a></span><span class="op">(</span><span class="va">i</span><span class="op">)</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">FDD</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">" Lag 0"</span>,<span class="va">names.FDD</span><span class="op">)</span></span>
<span><span class="va">dprice</span> <span class="op">&lt;-</span> <span class="va">dprice</span><span class="op">[</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">dprice</span><span class="op">)</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">FDD</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">dprice</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">eq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">dprice</span><span class="op">~</span><span class="va">FDD</span><span class="op">)</span></span>
<span><span class="co"># Compute the Newey-West std errors:</span></span>
<span><span class="va">var.cov.mat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/NeweyWest.html">NeweyWest</a></span><span class="op">(</span><span class="va">eq</span>,lag <span class="op">=</span> <span class="fl">7</span>, prewhite <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">robust_se</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">var.cov.mat</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Stargazer output (with and without Robust SE)</span></span>
<span><span class="fu">stargazer</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/stargazer/man/stargazer.html">stargazer</a></span><span class="op">(</span><span class="va">eq</span>, <span class="va">eq</span>, type <span class="op">=</span> <span class="st">"text"</span>,</span>
<span>                     column.labels<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"(no HAC)"</span>,<span class="st">"(HAC)"</span><span class="op">)</span>,keep.stat<span class="op">=</span><span class="st">"n"</span>,</span>
<span>                     se <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="cn">NULL</span>,<span class="va">robust_se</span><span class="op">)</span>,no.space <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## =========================================
##                  Dependent variable:     
##              ----------------------------
##                         dprice           
##                 (no HAC)        (HAC)    
##                   (1)            (2)     
## -----------------------------------------
## FDD Lag 0       0.496***      0.496***   
##                 (0.058)        (0.139)   
## FDD Lag 1       0.150***       0.150*    
##                 (0.058)        (0.087)   
## FDD Lag 2        0.046          0.046    
##                 (0.057)        (0.056)   
## FDD Lag 3        0.062          0.062    
##                 (0.057)        (0.046)   
## FDD Lag 4        0.024          0.024    
##                 (0.057)        (0.030)   
## FDD Lag 5        0.036          0.036    
##                 (0.057)        (0.030)   
## FDD Lag 6        0.037          0.037    
##                 (0.057)        (0.046)   
## FDD Lag 7        0.019          0.019    
##                 (0.057)        (0.015)   
## FDD Lag 8        -0.038        -0.038    
##                 (0.057)        (0.034)   
## FDD Lag 9        -0.006        -0.006    
##                 (0.057)        (0.050)   
## FDD Lag 10      -0.112*        -0.112    
##                 (0.057)        (0.069)   
## FDD Lag 11       -0.063        -0.063    
##                 (0.058)        (0.052)   
## FDD Lag 12      -0.140**       -0.140*   
##                 (0.058)        (0.078)   
## Constant        -0.426*        -0.426*   
##                 (0.238)        (0.243)   
## -----------------------------------------
## Observations      600            600     
## =========================================
## Note:         *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>Let us now use function <code>estim.armax</code>, from package <code>AEC</code>to estimate an ARMA-X(2,0,1) model:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nb.lags</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">FDD</span> <span class="op">&lt;-</span> <span class="va">FJ</span><span class="op">$</span><span class="va">fdd</span><span class="op">[</span><span class="op">(</span><span class="va">nb.lags</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span></span>
<span><span class="va">names.FDD</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">nb.lags</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">FDD</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">FDD</span>,<span class="va">FJ</span><span class="op">$</span><span class="va">fdd</span><span class="op">[</span><span class="op">(</span><span class="va">nb.lags</span><span class="op">+</span><span class="fl">1</span><span class="op">-</span><span class="va">i</span><span class="op">)</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">i</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">names.FDD</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">names.FDD</span>,<span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">" Lag "</span>,<span class="fu"><a href="https://rdrr.io/r/base/toString.html">toString</a></span><span class="op">(</span><span class="va">i</span><span class="op">)</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">FDD</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">" Lag 0"</span>,<span class="va">names.FDD</span><span class="op">)</span></span>
<span><span class="va">dprice</span> <span class="op">&lt;-</span> <span class="fl">100</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">price</span><span class="op">[</span><span class="op">(</span><span class="va">k</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">price</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">k</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">dprice</span> <span class="op">&lt;-</span> <span class="va">dprice</span><span class="op">[</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">dprice</span><span class="op">)</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">FDD</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">dprice</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">res.armax</span> <span class="op">&lt;-</span> <span class="fu">estim.armax</span><span class="op">(</span>Y <span class="op">=</span> <span class="va">dprice</span>,p<span class="op">=</span><span class="fl">3</span>,q<span class="op">=</span><span class="fl">0</span>,X<span class="op">=</span><span class="va">FDD</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] "=================================================="
## [1] "  ESTIMATING"
## [1] "=================================================="
## [1] "  END OF ESTIMATION"
## [1] "=================================================="
## [1] ""
## [1] "  RESULTS:"
## [1] "  -----------------------"
##                 THETA     st.dev   t.ratio
## c         -0.46556249 0.19554352 -2.380864
## phi   t-1  0.09788977 0.04025907  2.431496
## phi   t-2  0.05049849 0.03827488  1.319364
## phi   t-3  0.07155170 0.03764750  1.900570
## sigma      4.64917949 0.13300769 34.954215
## beta  t-0  0.47015552 0.05665344  8.298800
## beta  t-1  0.10015862 0.05972526  1.676989
## [1] "=================================================="</code></pre>
<p>Figure <a href="TS.html#fig:freez4">1.11</a> shows the IRF associated with each of the two models.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nb.periods</span> <span class="op">&lt;-</span> <span class="fl">20</span></span>
<span><span class="va">IRF1</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span>c<span class="op">=</span><span class="fl">0</span>,phi<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span>,theta<span class="op">=</span><span class="va">eq</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fl">13</span><span class="op">]</span>,sigma<span class="op">=</span><span class="fl">1</span>,</span>
<span>                 T<span class="op">=</span><span class="va">nb.periods</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span>,nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">IRF2</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span>c<span class="op">=</span><span class="fl">0</span>,phi<span class="op">=</span><span class="va">res.armax</span><span class="op">$</span><span class="va">phi</span>,theta<span class="op">=</span><span class="va">res.armax</span><span class="op">$</span><span class="va">beta</span>,sigma<span class="op">=</span><span class="fl">1</span>,</span>
<span>                 T<span class="op">=</span><span class="va">nb.periods</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">res.armax</span><span class="op">$</span><span class="va">phi</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                 nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.15</span>,<span class="fl">.95</span>,<span class="fl">.2</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">IRF1</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,col<span class="op">=</span><span class="st">"red"</span>,xlab<span class="op">=</span><span class="st">"months after shock"</span>,</span>
<span>     ylab<span class="op">=</span><span class="st">"Chge in price (percent)"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">IRF2</span>,lwd<span class="op">=</span><span class="fl">2</span>,col<span class="op">=</span><span class="st">"red"</span>,lty<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>,col<span class="op">=</span><span class="st">"grey"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:freez4"></span>
<img src="TimeSeries_files/figure-html/freez4-1.png" alt="Response of changes in orange juice price (in percent) to the number of freezing days. The solid (respectively dashed) line corresponds to the ARMAX(0,0,12) (resp. ARMAX(3,0,1)) model. The first model is estimated by OLS (see above), the second by MLE." width="95%"><p class="caption">
Figure 1.11: Response of changes in orange juice price (in percent) to the number of freezing days. The solid (respectively dashed) line corresponds to the ARMAX(0,0,12) (resp. ARMAX(3,0,1)) model. The first model is estimated by OLS (see above), the second by MLE.
</p>
</div>
</div>
<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->
<!-- \begin{exmpl}[Effect of monetary policy shocks] -->
<!-- \begin{itemize} -->
<!-- \item \href{https://www.aeaweb.org/articles?id=10.1257/mac.20130329}{Gertler and Karadi (2015)}'s type of shocks (high-frequency change in Euro-dollar futures). -->
<!-- \item Effect on 12-month growth rate of Industrial Production (IP)? -->
<!-- \item Data from \href{http://econweb.ucsd.edu/~vramey/research.html\#data}{Ramey's website}. -->
<!-- \end{itemize} -->
<!-- \begin{figure} -->
<!-- \caption{IP and MP shocks} -->
<!-- \begin{center} -->
<!-- \includegraphics[width=.8\linewidth]{../../figures/Figure_Ramey.pdf} -->
<!-- \end{center} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->
<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->
<!-- \addtocounter{exmpl}{-1} -->
<!-- \begin{exmpl}[Effect of monetary policy shocks (cont'd)] -->
<!-- \begin{figure} -->
<!-- \caption{IRF of IP growth rate to a 1 std-deviation MP shock} -->
<!-- \begin{center} -->
<!-- \includegraphics[width=.8\linewidth]{../../figures/Figure_Ramey2.pdf} -->
<!-- \end{center} -->
<!-- \begin{tiny} -->
<!-- ARMAX(1,1,1). estimated by MLE (Section \ref{section:MLE_ARMA}). The initial shock corresponds to one standard deviation of the series of monetary-policy shocks. The blue lines delineate the 95\% confidence interval. -->
<!-- \end{tiny} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->
<div class="example">
<p><span id="exm:Ramey1" class="example"><strong>Example 1.4  (Real effect of a monetary policy shock) </strong></span>In this example, we make use of monetary shocks identified through high-frequency data (see <span class="citation">Gertler and Karadi (<a href="references.html#ref-Gertler_Karadi_2015" role="doc-biblioref">2015</a>)</span>). This dataset comes from <a href="https://econweb.ucsd.edu/~vramey/research.html">Valerie Ramey’s website</a> (see <span class="citation">Ramey (<a href="references.html#ref-Ramey_2016_NBER" role="doc-biblioref">2016</a>)</span>).</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Ramey</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="co"># Construct growth series:</span></span>
<span><span class="va">Ramey</span><span class="op">$</span><span class="va">growth</span> <span class="op">&lt;-</span> <span class="va">Ramey</span><span class="op">$</span><span class="va">LIP</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fl">12</span><span class="op">)</span>,<span class="va">Ramey</span><span class="op">$</span><span class="va">LIP</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">Ramey</span><span class="op">$</span><span class="va">LIP</span><span class="op">)</span><span class="op">-</span><span class="fl">12</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co"># Prepare matrix of exogenous variables:</span></span>
<span><span class="va">vec.lags</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">9</span>,<span class="fl">12</span>,<span class="fl">18</span><span class="op">)</span></span>
<span><span class="va">Matrix.of.Exog</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="va">shocks</span> <span class="op">&lt;-</span> <span class="va">Ramey</span><span class="op">$</span><span class="va">ED2_TC</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">vec.lags</span><span class="op">)</span><span class="op">)</span><span class="op">{</span><span class="va">Matrix.of.Exog</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">Matrix.of.Exog</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="va">vec.lags</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span>,<span class="va">shocks</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">vec.lags</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">}</span></span>
<span><span class="co"># Look for dates where data are available:</span></span>
<span><span class="va">indic.good.dates</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html">complete.cases</a></span><span class="op">(</span><span class="va">Matrix.of.Exog</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:Ramey1fig"></span>
<img src="TimeSeries_files/figure-html/Ramey1fig-1.png" alt="The blue line corresponds to monetary-policy shocks identified by means of the Gertler and Karadi (2015)'s approach (high-frequency change in Euro-dollar futures). The black slid line is the year-on-year growth rate of industrial production." width="95%"><p class="caption">
Figure 1.12: The blue line corresponds to monetary-policy shocks identified by means of the Gertler and Karadi (2015)’s approach (high-frequency change in Euro-dollar futures). The black slid line is the year-on-year growth rate of industrial production.
</p>
</div>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Estimate ARMAX:</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">1</span>; <span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">estim.armax</span><span class="op">(</span><span class="va">Ramey</span><span class="op">$</span><span class="va">growth</span><span class="op">[</span><span class="va">indic.good.dates</span><span class="op">]</span>,<span class="va">p</span>,<span class="va">q</span>,</span>
<span>                 X<span class="op">=</span><span class="va">Matrix.of.Exog</span><span class="op">[</span><span class="va">indic.good.dates</span>,<span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] "=================================================="
## [1] "  ESTIMATING"
## [1] "=================================================="
## [1] "  END OF ESTIMATION"
## [1] "=================================================="
## [1] ""
## [1] "  RESULTS:"
## [1] "  -----------------------"
##                   THETA       st.dev    t.ratio
## c         -0.0001716198 0.0005845907 -0.2935726
## phi   t-1  0.9825608412 0.0120458531 81.5683897
## sigma      0.0087948724 0.0003211748 27.3834438
## beta  t-0 -0.0193570616 0.0087331529 -2.2165032
## beta  t-1 -0.0225707935 0.0086750938 -2.6017925
## beta  t-2 -0.0070131593 0.0086387440 -0.8118263
## [1] "=================================================="</code></pre>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute IRF:</span></span>
<span><span class="va">irf</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span><span class="fl">0</span>,<span class="va">x</span><span class="op">$</span><span class="va">phi</span>,<span class="va">x</span><span class="op">$</span><span class="va">beta</span>,<span class="va">x</span><span class="op">$</span><span class="va">sigma</span>,T<span class="op">=</span><span class="fl">60</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">phi</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF<span class="op">=</span><span class="fl">1</span>,X<span class="op">=</span><span class="cn">NaN</span>,beta<span class="op">=</span><span class="cn">NaN</span><span class="op">)</span></span></code></pre></div>
<p>Figure <a href="TS.html#fig:Ramey3">1.13</a> displays the resulting IRF, with a 95% confidence band. The code used to produce the confidence bands (i.e., to compute the standard deviation of the dynamic multipliers for the different horizons) is based on the Delta method (see Eq. <a href="#eq:DeltaMethod">(<strong>??</strong>)</a>). The codes are available in Appendix <a href="append.html#IRFDELTA">2.6.2</a>.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:Ramey3"></span>
<img src="TimeSeries_files/figure-html/Ramey3-1.png" alt="Response of industrial-production growth to monetary-policy shocks. Dashed lines correpsond to the $\pm$  2-standard-deviation bands." width="95%"><p class="caption">
Figure 1.13: Response of industrial-production growth to monetary-policy shocks. Dashed lines correpsond to the <span class="math inline">\(\pm\)</span> 2-standard-deviation bands.
</p>
</div>
</div>
</div>
<div id="estimARMA" class="section level3" number="1.2.8">
<h3>
<span class="header-section-number">1.2.8</span> Maximum Likelihood Estimation of ARMA processes<a class="anchor" aria-label="anchor" href="#estimARMA"><i class="fas fa-link"></i></a>
</h3>
<p>Consider the general case (of any time series); assume we observe a sample <span class="math inline">\(\mathbf{y}=[y_1,\dots,y_T]'\)</span>. In order to implement ML techniques (see Section <a href="#secMLE"><strong>??</strong></a>), we need to evaluate the joint p.d.f. (or “likelihood”) of <span class="math inline">\(\mathbf{y}\)</span>, i.e., <span class="math inline">\(\mathcal{L}(\boldsymbol\theta;\mathbf{y})\)</span>, where <span class="math inline">\(\boldsymbol\theta\)</span> is a vector of parameters that characterizes the dynamics of <span class="math inline">\(y_t\)</span>. The Maximum Likelihood (ML) estimate of <span class="math inline">\(\boldsymbol\theta\)</span> is then given by:
<span class="math display">\[
\boxed{\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \mathcal{L}(\boldsymbol\theta;\mathbf{y})  = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\mathbf{y}).}
\]</span></p>
<p>In the time series context, if process <span class="math inline">\(y_t\)</span> is Markovian, then there exists a useful way to rewrite the likelihood <span class="math inline">\(\mathcal{L}(\boldsymbol\theta;\mathbf{y})\)</span>. Let us first recall the definition of a Markovian process (see also Def. <a href="#def:MC"><strong>??</strong></a>):</p>
<div class="definition">
<p><span id="def:Markov" class="definition"><strong>Definition 1.19  (Markovian process) </strong></span>Process <span class="math inline">\(y_t\)</span> is Markovian of order one if <span class="math inline">\(f_{Y_t|Y_{t-1},Y_{t-2},\dots} = f_{Y_t|Y_{t-1}}\)</span>. More generally, it is Markovian of order <span class="math inline">\(k\)</span> if <span class="math inline">\(f_{Y_t|Y_{t-1},Y_{t-2},\dots} = f_{Y_t|Y_{t-1},\dots,Y_{t-k}}\)</span>.</p>
</div>
<p>Now, remember Bayes’ formula:
<span class="math display">\[
\mathbb{P}(X_2=x,X_1=y) = \mathbb{P}(X_2=x|X_1=y)\mathbb{P}(X_1=y).
\]</span>
Using it leads to the following decomposition of our likelihood function:
<span class="math display">\[\begin{eqnarray*}
f_{Y_T,\dots,Y_1}(y_T,\dots,y_1;\boldsymbol\theta) &amp;=&amp;f_{Y_T|Y_{T-1},\dots,Y_1}(y_T,\dots,y_1;\boldsymbol\theta) \times \\
&amp;&amp; f_{Y_{T-1},\dots,Y_1}(y_{T-1},\dots,y_1;\boldsymbol\theta).
\end{eqnarray*}\]</span>
Using the previous expression recursively, one obtains:
<span class="math display" id="eq:recursMLE">\[\begin{equation}
f_{Y_T,\dots,Y_1}(y_T,\dots,y_1;\boldsymbol\theta) = f_{Y_1}(y_1;\boldsymbol\theta) \prod_{t=2}^{T} f_{Y_t|Y_{t-1},\dots,Y_1}(y_t,\dots,y_1;\boldsymbol\theta).\tag{1.19}
\end{equation}\]</span></p>
<p>Let us start with the Gaussian AR(1) process (which is Markovian of order one):
<span class="math display">\[
y_t = c + \phi_1 y_{t-1} + \varepsilon_t, \quad \varepsilon_t \sim\,i.i.d.\, \mathcal{N}(0,\sigma^2).
\]</span>
For <span class="math inline">\(t&gt;1\)</span>:
<span class="math display">\[
f_{Y_t|Y_{t-1},\dots,Y_1}(y_t,\dots,y_1;\boldsymbol\theta) = f_{Y_t|Y_{t-1}}(y_t,y_{t-1};\boldsymbol\theta)
\]</span>
and
<span class="math display">\[
f_{Y_t|Y_{t-1}}(y_t,y_{t-1};\boldsymbol\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y_t - c - \phi_1 y_{t-1})^2}{2\sigma^2}\right).
\]</span></p>
<p>These expressions can be plugged into Eq. <a href="TS.html#eq:recursMLE">(1.19)</a>. But what about <span class="math inline">\(f_{Y_1}(y_1;\boldsymbol\theta)\)</span>? There exist two possibilities:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Case 1</strong>: We use the marginal distribution: <span class="math inline">\(y_1 \sim \mathcal{N}\left(\dfrac{c}{1-\phi_1},\dfrac{\sigma^2}{1-\phi_1^2}\right)\)</span>.</li>
<li>
<strong>Case 2</strong>: <span class="math inline">\(y_1\)</span> is considered to be deterministic. In a way, that means that the first observation is “sacrificed”.</li>
</ol>
<p>For a Gaussian AR(1) process, we have:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Case 1</strong>: The (exact) log-likelihood is:
<span class="math display">\[\begin{eqnarray}
\log \mathcal{L}(\boldsymbol\theta;\mathbf{y})  &amp;=&amp; - \frac{T}{2} \log(2\pi) - T\log(\sigma) + \frac{1}{2}\log(1-\phi_1^2)\nonumber \\
&amp;&amp; - \frac{(y_1 - c/(1-\phi_1))^2}{2\sigma^2/(1-\phi_1^2)} - \sum_{t=2}^T \left[\frac{(y_t - c - \phi_1 y_{t-1})^2}{2\sigma^2} \right].
\end{eqnarray}\]</span>
The Maximum Likelihood Estimator of <span class="math inline">\(\boldsymbol\theta= [c,\phi_1,\sigma^2]\)</span> is obtained by numerical optimization.</p></li>
<li><p><strong>Case 2</strong>: The (conditional) log-likelihood is:
<span class="math display" id="eq:Lstar">\[\begin{eqnarray}
\log \mathcal{L}^*(\boldsymbol\theta;\mathbf{y})  &amp;=&amp; - \frac{T-1}{2} \log(2\pi) - (T-1)\log(\sigma)\nonumber\\
&amp;&amp; - \sum_{t=2}^T \left[\frac{(y_t - c - \phi_1 y_{t-1})^2}{2\sigma^2} \right].\tag{1.20}
\end{eqnarray}\]</span></p></li>
</ol>
<p>Exact MLE and conditional MLE have the same asymptotic (i.e. large-sample) distribution. Indeed, when the process is stationary, <span class="math inline">\(f_{Y_1}(y_1;\boldsymbol\theta)\)</span> makes a relatively negligible contribution to <span class="math inline">\(\log \mathcal{L}(\boldsymbol\theta;\mathbf{y})\)</span>.</p>
<p>The conditional MLE has a substantial advantage: in the Gaussian case, the conditional MLE is simply obtained by OLS. Indeed, let us introduce the notations:
<span class="math display">\[
Y = \left[\begin{array}{c}
y_2\\
\vdots\\
y_T
\end{array}\right] \quad and \quad
X = \left[\begin{array}{cc}
1 &amp;y_1\\
\vdots&amp;\vdots\\
1&amp;y_{T-1}
\end{array}\right].
\]</span>
Eq. <a href="TS.html#eq:Lstar">(1.20)</a> then rewrites:
<span class="math display">\[\begin{eqnarray}
\log \mathcal{L}^*(\boldsymbol\theta;\mathbf{y})  &amp;=&amp; - \frac{T-1}{2} \log(2\pi) - (T-1)\log(\sigma) \nonumber \\
&amp;&amp; - \frac{1}{2\sigma^2} (Y-X[c,\phi_1]')'(Y-X[c,\phi_1]'),
\end{eqnarray}\]</span>
which is maximised for:
<span class="math display" id="eq:AROLSsigma">\[\begin{eqnarray}
[\hat{c},\hat\phi_1]' &amp;=&amp; (X'X)^{-1}X'Y \tag{1.21} \\
\hat{\sigma^2} &amp;=&amp; \frac{1}{T-1} \sum_{t=2}^T (y_t - \hat{c} - \hat{\phi_1}y_{t-1})^2 \nonumber \\
&amp;=&amp; \frac{1}{T-1} Y'(I - X(X'X)^{-1}X')Y. \tag{1.22}
\end{eqnarray}\]</span></p>
<p>Let us turn to the case of an AR(p) process. We have:
<span class="math display">\[\begin{eqnarray*}
\log \mathcal{L}(\boldsymbol\theta;\mathbf{y}) &amp;=&amp; \log f_{Y_p,\dots,Y_1}(y_p,\dots,y_1;\boldsymbol\theta) +\\
&amp;&amp; \underbrace{\sum_{t=p+1}^{T} \log f_{Y_t|Y_{t-1},\dots,Y_{t-p}}(y_t,\dots,y_{t-p};\boldsymbol\theta)}_{\log \mathcal{L}^*(\boldsymbol\theta;\mathbf{y})}.
\end{eqnarray*}\]</span>
where <span class="math inline">\(f_{Y_p,\dots,Y_{1}}(y_p,\dots,y_{1};\boldsymbol\theta)\)</span> is the marginal distribution of <span class="math inline">\(\mathbf{y}_{1:p} := [y_p,\dots,y_1]'\)</span>. The marginal distribution of <span class="math inline">\(\mathbf{y}_{1:p}\)</span> is Gaussian; it is therefore fully characterised by its mean and covariance matrix:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(\mathbf{y}_{1:p})&amp;=&amp;\frac{c}{1-\phi_1-\dots-\phi_p} \mathbf{1}_{p\times 1} \\
\mathbb{V}ar(\mathbf{y}_{1:p}) &amp;=&amp; \left[\begin{array}{cccc}
\gamma_0 &amp; \gamma_1 &amp; \dots &amp; \gamma_{p-1} \\
\gamma_1 &amp; \gamma_0 &amp; \dots &amp; \gamma_{p-2} \\
\vdots &amp;  &amp; \ddots &amp; \vdots \\
\gamma_{p-1} &amp; \gamma_{p-2} &amp; \dots &amp; \gamma_{0} \\
\end{array}\right],
\end{eqnarray*}\]</span>
where the <span class="math inline">\(\gamma_i\)</span>’s are computed using the Yule-Walker equations (Eq. <a href="TS.html#eq:gammas">(1.14)</a>). Note that they depend, in a non-linear way, on the model parameters. Hence, the maximization of the exact log-likelihood necessitates numerical oprimization procedures. By contrast, the maximization of the conditional log-likelihood <span class="math inline">\(\log \mathcal{L}^*(\boldsymbol\theta;\mathbf{y})\)</span> only requires OLS, using Eqs. <a href="TS.html#eq:AROLSmean">(1.21)</a> and <a href="TS.html#eq:AROLSsigma">(1.22)</a>, with:
<span class="math display">\[
Y = \left[\begin{array}{c}
y_{p+1}\\
\vdots\\
y_T
\end{array}\right] \quad and \quad
X = \left[\begin{array}{cccc}
1 &amp; y_p &amp; \dots &amp; y_1\\
\vdots&amp;\vdots&amp;&amp;\vdots\\
1&amp;y_{T-1}&amp;\dots&amp;y_{T-p}
\end{array}\right].
\]</span></p>
<p>Again, for stationary processes, conditional and exact MLE have the same asymptotic (large-sample) distribution. In small samples, the OLS formula is however biased. Indeed, consider the regression (where <span class="math inline">\(y_t\)</span> follows an AR(p) process):
<span class="math display" id="eq:OLSregARp">\[\begin{equation}
y_t = \boldsymbol\beta'\mathbf{x}_t + \varepsilon_t,\tag{1.23}
\end{equation}\]</span>
with <span class="math inline">\(\mathbf{x}_t = [1,y_{t-1},\dots,y_{t-p}]'\)</span> and <span class="math inline">\(\boldsymbol\beta = [c,\phi_1,\dots,\phi_p]'\)</span>.</p>
<p>The bias results from the fact that <span class="math inline">\(\mathbf{x}_t\)</span> correlates to the <span class="math inline">\(\varepsilon_s\)</span>’s for <span class="math inline">\(s&lt;t\)</span>. To be sure:
<span class="math display" id="eq:olsar1">\[\begin{equation}
\mathbf{b} = \boldsymbol{\beta} + (X'X)^{-1}X'\boldsymbol\varepsilon,\tag{1.24}
\end{equation}\]</span>
and because of the specific form of <span class="math inline">\(X\)</span>, we have non-zero correlation between <span class="math inline">\(\mathbf{x}_t\)</span> and <span class="math inline">\(\varepsilon_s\)</span> for <span class="math inline">\(s&lt;t\)</span>, therefore <span class="math inline">\(\mathbb{E}[(X'X)^{-1}X'\boldsymbol\varepsilon] \ne 0\)</span>. Again, asymptotically, the previous expectation goes to zero, and we have:</p>
<div class="proposition">
<p><span id="prp:cgceOLSARp" class="proposition"><strong>Proposition 1.10  (Large-sample porperties of the OLS estimator of AR(p) models) </strong></span>Assume <span class="math inline">\(\{y_t\}\)</span> follows the AR(p) process:
<span class="math display">\[
y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \varepsilon_t
\]</span>
where <span class="math inline">\(\{\varepsilon_{t}\}\)</span> is an i.i.d. white noise process. If <span class="math inline">\(\mathbf{b}\)</span> is the OLS estimator of <span class="math inline">\(\boldsymbol\beta\)</span> (Eq. <a href="TS.html#eq:OLSregARp">(1.23)</a>), we have:
<span class="math display">\[
\sqrt{T}(\mathbf{b}-\boldsymbol{\beta}) =  \underbrace{\left[\frac{1}{T}\sum_{t=p}^T \mathbf{x}_t\mathbf{x}_t' \right]^{-1}}_{\overset{p}{\rightarrow} \mathbf{Q}^{-1}}
\underbrace{\sqrt{T} \left[\frac{1}{T}\sum_{t=1}^T \mathbf{x}_t\varepsilon_t \right]}_{\overset{d}{\rightarrow} \mathcal{N}(0,\sigma^2\mathbf{Q})},
\]</span>
where <span class="math inline">\(\mathbf{Q} = \mbox{plim }\frac{1}{T}\sum_{t=p}^T \mathbf{x}_t\mathbf{x}_t'= \mbox{plim }\frac{1}{T}\sum_{t=1}^T \mathbf{x}_t\mathbf{x}_t'\)</span> is given by:
<span class="math display" id="eq:Qols">\[\begin{equation}
\mathbf{Q} = \left[
\begin{array}{ccccc}
1 &amp; \mu &amp;\mu &amp; \dots &amp; \mu \\
\mu &amp; \gamma_0 + \mu^2 &amp; \gamma_1 + \mu^2 &amp; \dots &amp; \gamma_{p-1} + \mu^2\\
\mu &amp; \gamma_1 + \mu^2 &amp; \gamma_0 + \mu^2 &amp; \dots &amp; \gamma_{p-2} + \mu^2\\
\vdots &amp;\vdots &amp;\vdots &amp;\dots &amp;\vdots \\
\mu &amp; \gamma_{p-1} + \mu^2 &amp; \gamma_{p-2} + \mu^2 &amp; \dots &amp; \gamma_{0} + \mu^2
\end{array}
\right].\tag{1.25}
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-11" class="proof"><em>Proof</em>. </span>Rearranging Eq. <a href="TS.html#eq:olsar1">(1.24)</a>, we have:
<span class="math display">\[
\sqrt{T}(\mathbf{b}-\boldsymbol{\beta}) =  (X'X/T)^{-1}\sqrt{T}(X'\boldsymbol\varepsilon/T).
\]</span>
Let us consider the autocovariances of <span class="math inline">\(\mathbf{v}_t = \mathbf{x}_t \varepsilon_t\)</span>, denoted by <span class="math inline">\(\gamma^v_j\)</span>. Using the fact that <span class="math inline">\(\mathbf{x}_t\)</span> is a linear combination of past <span class="math inline">\(\varepsilon_t\)</span>’s and that <span class="math inline">\(\varepsilon_t\)</span> is a white noise, we get that <span class="math inline">\(\mathbb{E}(\varepsilon_t\mathbf{x}_t)=0\)</span>. Therefore
<span class="math display">\[
\gamma^v_j = \mathbb{E}(\varepsilon_t\varepsilon_{t-j}\mathbf{x}_t\mathbf{x}_{t-j}').
\]</span>
If <span class="math inline">\(j&gt;0\)</span>, we have
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(\varepsilon_t\varepsilon_{t-j}\mathbf{x}_t\mathbf{x}_{t-j}')=\mathbb{E}(\mathbb{E}[\varepsilon_t\varepsilon_{t-j}\mathbf{x}_t\mathbf{x}_{t-j}'|\varepsilon_{t-j},\mathbf{x}_t,\mathbf{x}_{t-j}])\\
&amp;=&amp;\mathbb{E}(\varepsilon_{t-j}\mathbf{x}_t\mathbf{x}_{t-j}'\mathbb{E}[\varepsilon_t|\varepsilon_{t-j},\mathbf{x}_t,\mathbf{x}_{t-j}])=0.
\end{eqnarray*}\]</span>
Note that, for <span class="math inline">\(j&gt;0\)</span>, we have <span class="math inline">\(\mathbb{E}[\varepsilon_t|\varepsilon_{t-j},\mathbf{x}_t,\mathbf{x}_{t-j}]=0\)</span> because <span class="math inline">\(\{\varepsilon_t\}\)</span> is an i.i.d. white noise sequence. If <span class="math inline">\(j=0\)</span>, we have:
<span class="math display">\[
\gamma^v_0 = \mathbb{E}(\varepsilon_t^2\mathbf{x}_t\mathbf{x}_{t}')= \mathbb{E}(\varepsilon_t^2) \mathbb{E}(\mathbf{x}_t\mathbf{x}_{t}')=\sigma^2\mathbf{Q}.
\]</span>
The convergence in distribution of <span class="math inline">\(\sqrt{T}(X'\boldsymbol\varepsilon/T)=\sqrt{T}\frac{1}{T}\sum_{t=1}^Tv_t\)</span> results from Theorem <a href="TS.html#thm:CLTcovstat">1.1</a> (applied on <span class="math inline">\(\mathbf{v}_t=\mathbf{x}_t\varepsilon_t\)</span>), using the <span class="math inline">\(\gamma_j^v\)</span> computed above.</p>
</div>
<p>These two cases (exact or conditional log-likelihoods) can be implemented when asking R to fit an AR process by means of function <code>arima</code>. Let us for instance use the output gap of the <code>US3var</code> dataset (US quarterly data, covering the period 1959:2 to 2015:1, used in <span class="citation">Gouriéroux, Monfort, and Renne (<a href="references.html#ref-Gourieroux_Monfort_Renne_2017" role="doc-biblioref">2017</a>)</span>).</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">US3var</span><span class="op">$</span><span class="va">y.gdp.gap</span></span>
<span><span class="va">ar3.Case1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.html">arima</a></span><span class="op">(</span><span class="va">y</span>,order <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span>,method<span class="op">=</span><span class="st">"ML"</span><span class="op">)</span></span>
<span><span class="va">ar3.Case2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.html">arima</a></span><span class="op">(</span><span class="va">y</span>,order <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span>,method<span class="op">=</span><span class="st">"CSS"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">ar3.Case1</span><span class="op">$</span><span class="va">coef</span>,<span class="va">ar3.Case2</span><span class="op">$</span><span class="va">coef</span><span class="op">)</span></span></code></pre></div>
<pre><code>##           ar1         ar2        ar3  intercept
## [1,] 1.191267 -0.08934705 -0.1781163 -0.9226007
## [2,] 1.192003 -0.08811150 -0.1787662 -1.0341696</code></pre>
<p>The two sets of estimated coefficients appear to be very close to each other.</p>
<p>Let us now turn to Moving-Average processes. Start with the MA(1):
<span class="math display">\[
y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1},\quad \varepsilon_t \sim i.i.d.\mathcal{N}(0,\sigma^2).
\]</span>
The <span class="math inline">\(\varepsilon_t\)</span>’s are easily computed recursively, starting with <span class="math inline">\(\varepsilon_t = y_t - \mu - \theta_1 \varepsilon_{t-1}\)</span>. We obtain:
<span class="math display">\[
\varepsilon_t = y_t - \theta_1 y_{t-1} + \theta_1^2 y_{t-2}^2 + \dots + (-1)^{t-1} \theta_1^{t-1} y_{1} + (-1)^t\theta_1^{t}\varepsilon_{0}.
\]</span>
Assume that one wants to recover the sequence of <span class="math inline">\(\{\varepsilon_t\}\)</span>’s based on observed values of <span class="math inline">\(y_t\)</span> (from date 1 to date <span class="math inline">\(t\)</span>). One can use the previous expression, but what value should be used for <span class="math inline">\(\varepsilon_0\)</span>? If one does not use the true value of <span class="math inline">\(\varepsilon_0\)</span> but 0 (say), one does not obtain <span class="math inline">\(\varepsilon_t\)</span>, but only an estimate of it (<span class="math inline">\(\hat\varepsilon_t\)</span>, say), with:
<span class="math display">\[
\hat\varepsilon_t = \varepsilon_t - (-1)^t\theta_1^{t}\varepsilon_{0}.
\]</span>
Clearly, if <span class="math inline">\(|\theta_1|&lt;1\)</span>, then the error becomes small for large <span class="math inline">\(t\)</span>. Formally, when <span class="math inline">\(|\theta_1|&lt;1\)</span>, we have:
<span class="math display">\[
\hat\varepsilon_t \overset{p}{\rightarrow} \varepsilon_t.
\]</span>
Hence, when <span class="math inline">\(|\theta_1|&lt;1\)</span>, a consistent estimate of the conditional log-likelihood is given by:
<span class="math display" id="eq:MALstar">\[\begin{equation}
\log \hat{\mathcal{L}}^*(\boldsymbol\theta;\mathbf{y}) = -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \sum_{t=1}^T \frac{\hat\varepsilon_t^2}{2\sigma^2}.\tag{1.26}
\end{equation}\]</span>
Loosely speaking, if <span class="math inline">\(|\theta_1|&lt;1\)</span> and if <span class="math inline">\(T\)</span> is sufficiently large:
<span class="math display">\[
\mbox{approximate conditional MLE $\approx$ exact MLE.}
\]</span></p>
<p>Note that <span class="math inline">\(\hat{\mathcal{L}}^*(\boldsymbol\theta;\mathbf{y})\)</span> is a complicated nonlinear function of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\theta\)</span>. Its maximization therefore has to be based on numerical optimization procedures.</p>
<p>Let us not consider the case of a Gaussian MA(<span class="math inline">\(q\)</span>) process:
<span class="math display" id="eq:estimMAq">\[\begin{equation}
y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q} , \quad \varepsilon_t \sim i.i.d.\mathcal{N}(0,\sigma^2). \tag{1.27}
\end{equation}\]</span></p>
<p>Let us assume that this process is an <strong>invertible MA process</strong>. That is, assume that the roots of:
<span class="math display" id="eq:invertible">\[\begin{equation}
\lambda^q + \theta_1 \lambda^{q-1} + \dots + \theta_{q-1} \lambda + \theta_q = 0 \tag{1.28}
\end{equation}\]</span>
lie strictly inside of the unit circle. In this case, the polynomial form <span class="math inline">\(\Theta(L)=1 + \theta_1 L + \dots + \theta_q L^q\)</span> is <em>invertible</em> and Eq. <a href="TS.html#eq:estimMAq">(1.27)</a> writes:
<span class="math display">\[
\varepsilon_t = \Theta(L)^{-1}(y_t - \mu),
\]</span>
which implies that, if we knew all past values of <span class="math inline">\(y_t\)</span>, we would also know <span class="math inline">\(\varepsilon_t\)</span>. In this case, we can consistently estimate the <span class="math inline">\(\varepsilon_t\)</span>’s by recursively computing the <span class="math inline">\(\hat\varepsilon_t\)</span>’s as follows (for <span class="math inline">\(t&gt;0\)</span>):
<span class="math display" id="eq:condiVarepsiMABB">\[\begin{equation}
\hat\varepsilon_t = y_t - \mu - \theta_1 \hat\varepsilon_{t-1} - \dots  - \theta_q \hat\varepsilon_{t-q},\tag{1.29}
\end{equation}\]</span>
with
<span class="math display" id="eq:condiVarepsiMA">\[\begin{equation}
\hat\varepsilon_{0}=\dots=\hat\varepsilon_{-q+1}=0.\tag{1.30}
\end{equation}\]</span></p>
<p>In this context, a consistent estimate of the conditional log-likelihood is still given by Eq. <a href="TS.html#eq:MALstar">(1.26)</a>, using Eqs. <a href="TS.html#eq:condiVarepsiMABB">(1.29)</a> and <a href="TS.html#eq:condiVarepsiMA">(1.30)</a> to recursively compute the <span class="math inline">\(\hat\varepsilon_t\)</span>’s.</p>
<p>Note that we could determine the exact likelihood of an MA process. Indeed, vector <span class="math inline">\(\mathbf{y} = [y_1,\dots,y_T]'\)</span> is a Gaussian-distributed vector of mean <span class="math inline">\(\boldsymbol\mu = [\mu,\dots,\mu]'\)</span> and of variance:
<span class="math display">\[
\boldsymbol\Omega = \left[\begin{array}{ccccccc}
\gamma_0 &amp; \gamma_1&amp;\dots&amp;\gamma_q&amp;{\color{red}0}&amp;{\color{red}\dots}&amp;{\color{red}0}\\
\gamma_1 &amp; \gamma_0&amp;\gamma_1&amp;&amp;\ddots&amp;{\color{red}\ddots}&amp;{\color{red}\vdots}\\
\vdots &amp; \gamma_1&amp;\ddots&amp;\ddots&amp;&amp;\ddots&amp;{\color{red}0}\\
\gamma_q &amp;&amp;\ddots&amp;&amp;&amp;&amp;\gamma_q\\
{\color{red}0} &amp;&amp;&amp;\ddots&amp;\ddots&amp;\ddots&amp;\vdots\\
{\color{red}\vdots}&amp;{\color{red}\ddots}&amp;\ddots&amp;&amp;\gamma_1&amp;\gamma_0&amp;\gamma_1\\
{\color{red}0}&amp;{\color{red}\dots}&amp;{\color{red}0}&amp;\gamma_q&amp;\dots&amp;\gamma_1&amp;\gamma_0
\end{array}\right],
\]</span>
where the <span class="math inline">\(\gamma_j\)</span>’s are given by Eq. <a href="TS.html#eq:autocovMA">(1.7)</a>. The p.d.f. of <span class="math inline">\(\mathbf{y}\)</span> is then given by (see Prop. <a href="append.html#prp:pdfMultivarGaussian">2.18</a>):
<span class="math display">\[
(2\pi)^{-T/2}|\boldsymbol\Omega|^{-1/2}\exp\left( -\frac{1}{2} (\mathbf{y}-\boldsymbol\mu)' \boldsymbol\Omega^{-1} (\mathbf{y}-\boldsymbol\mu)\right).
\]</span>
For large samples, the computation of this likelihood however becomes numerically demanding.</p>
<p>Finally, let us consider the MLE of an ARMA(<span class="math inline">\(p\)</span>,<span class="math inline">\(q\)</span>) processes:
<span class="math display">\[
y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} +
\dots + \theta_q \varepsilon_{t-q} , \; \varepsilon_t \sim i.i.d.\,\mathcal{N}(0,\sigma^2).
\]</span>
If the MA part of this process is invertible, the log-likelihood function can be consistently approximated by its conditional counterpart (of the form of Eq. <a href="TS.html#eq:MALstar">(1.26)</a>), using consistent estimates <span class="math inline">\(\hat\varepsilon_t\)</span> of the <span class="math inline">\(\varepsilon_t\)</span>. The <span class="math inline">\(\hat\varepsilon_t\)</span>’s are computed recursively as:
<span class="math display" id="eq:recvareps">\[\begin{equation}
\hat\varepsilon_t = y_t - c - \phi_1 y_{t-1} - \dots - \phi_p y_{t-p} - \theta_1 \hat\varepsilon_{t-1} - \dots - \theta_q \hat\varepsilon_{t-q},\tag{1.31}
\end{equation}\]</span>
given some initial conditions, for instance:</p>
<ol style="list-style-type: lower-alpha">
<li>
<span class="math inline">\(\hat\varepsilon_0=\dots=\hat\varepsilon_{-q+1}=0\)</span> and <span class="math inline">\(y_{0}=\dots=y_{-p+1}=\mathbb{E}(y_i)=\mu\)</span>. (Recursions in Eq. <a href="TS.html#eq:recvareps">(1.31)</a> then start for <span class="math inline">\(t=1\)</span>.)</li>
<li>
<span class="math inline">\(\hat\varepsilon_p=\dots=\hat\varepsilon_{p-q+1}=0\)</span> and actual values of the <span class="math inline">\(y_{i}\)</span>’s for <span class="math inline">\(i \in [1,p]\)</span>. In that case, the first <span class="math inline">\(p\)</span> observations of <span class="math inline">\(y_t\)</span> will not be used. Recursions in Eq. <a href="TS.html#eq:recvareps">(1.31)</a> then start for <span class="math inline">\(t=p+1\)</span>.</li>
</ol>
</div>
<div id="specification-choice" class="section level3" number="1.2.9">
<h3>
<span class="header-section-number">1.2.9</span> Specification choice<a class="anchor" aria-label="anchor" href="#specification-choice"><i class="fas fa-link"></i></a>
</h3>
<p>The previouss section explains how to fit a given ARMA specification. But how to choose an appropriate specification? A possibility is to employ the (P)ACF approach (see Figure <a href="TS.html#fig:pacf">1.7</a>). However, the previous approach leads to either an AR or a MA process (and not an ARMA process). If one wants to consider various ARMA(p,q) specifications, for <span class="math inline">\(p \in \{1,\dots,P\}\)</span> and <span class="math inline">\(q \in \{1,\dots,Q\}\)</span>, say, then one can resort to <strong>information criteria</strong>.</p>
<p>In general, when choosing a specification, one faces the following dilemma:</p>
<ol style="list-style-type: lower-alpha">
<li>Too rich a specification may lead to “overfitting”/misspecification, implying additional estimation errors (in out-of-sample forecasts).</li>
<li>Too simple a specification may lead to potential omission of valuable information (e.g., contained in older lags).</li>
</ol>
<p>The lag selection approach based on the so-called <strong>information criteria</strong> consists in maximizing the fit of the data, but adding a penalty for the “richness” of the model. More precisely, using this approach amounts to minimizing a loss function that (a) negatively depends on the fitting errors and (b) positively depends on the number of parameters in the model.</p>
<div class="definition">
<p><span id="def:infocriteria" class="definition"><strong>Definition 1.20  (Information Criteria) </strong></span>The Akaike (AIC), Hannan-Quinn (HQ) and Schwarz information (BIC) criteria are of the form
<span class="math display">\[
c^{(i)}(k) = \underbrace{\frac{- 2 \log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\mathbf{y})}{T}}_{\mbox{decreases w.r.t. $k$}} \quad +
\underbrace{
\frac{k\phi^{(i)}(T)}{T},}_{\mbox{increases w.r.t. $k$}}
\]</span>
with <span class="math inline">\((i) \in\{AIC,HQ,BIC\}\)</span> and where <span class="math inline">\(\hat{\boldsymbol\theta}_T(k)\)</span> denotes the ML estimate of <span class="math inline">\(\boldsymbol\theta_0(k)\)</span>, which is a vector of parameters of length <span class="math inline">\(k\)</span>.</p>
<p>The lag suggested by criterion <span class="math inline">\((i)\)</span> is then given by:
<span class="math display">\[
\boxed{\hat{k}^{(i)} = \underset{k}{\mbox{argmin}} \quad c^{(i)}(k).}
\]</span></p>
</div>
<p>In the case of an ARMA(p,q) process, <span class="math inline">\(k=2+p+q\)</span>.</p>
<div class="proposition">
<p><span id="prp:infocriteria" class="proposition"><strong>Proposition 1.11  (Consistency of the criteria-based lag selection) </strong></span>The lag selection procedure is consistent (see Def. <a href="append.html#def:asmyptconsisttest">2.8</a>) if
<span class="math display">\[
\lim_{T \rightarrow \infty} \phi(T) = \infty \quad and \quad \lim_{T \rightarrow \infty} \phi(T)/T = 0.
\]</span>
This is notably the case of the HQ and the BIC criteria.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-12" class="proof"><em>Proof</em>. </span>The true number of lags is denoted by <span class="math inline">\(k_0\)</span>. We will show that <span class="math inline">\(\lim_{T \rightarrow \infty} \mathbb{P}(\hat{k}_T \ne k_0)=0\)</span>.</p>
<ul>
<li>Case <span class="math inline">\(k &lt; k_0\)</span>: The model with <span class="math inline">\(k\)</span> parameter is misspecified, therefore:
<span class="math display">\[
\mbox{plim}_{T \rightarrow \infty}  \log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\mathbf{y})/T &lt; \mbox{plim}_{T \rightarrow \infty}  \log \mathcal{L}(\hat{\boldsymbol\theta}_T(k_0);\mathbf{y})/T.
\]</span>
Hence, if <span class="math inline">\(\lim_{T \rightarrow \infty} \phi(T)/T = 0\)</span>, we have: <span class="math inline">\(\lim_{T \rightarrow \infty} \mathbb{P}(c(k_0) \ge c(k)) \rightarrow 0\)</span> and
<span class="math display">\[
\lim_{T \rightarrow \infty} \mathbb{P}(\hat{k}&lt;k_0) \le \lim_{T \rightarrow \infty} \mathbb{P}\left\{c(k_0) \ge c(k) \mbox{ for some $k &lt; k_0$}\right\} = 0.
\]</span>
</li>
<li>Case <span class="math inline">\(k &gt; k_0\)</span>: under the null hypothesis, the likelihood ratio (LR) test statistic (see Def. <a href="#def:LR"><strong>??</strong></a>) satisfies:
<span class="math display">\[
2 \left(\log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\mathbf{y})-\log \mathcal{L}(\hat{\boldsymbol\theta}_T(k_0);\mathbf{y})\right) \sim \chi^2(k-k_0).
\]</span>
If <span class="math inline">\(\lim_{T \rightarrow \infty} \phi(T) = \infty\)</span>, we have: <span class="math inline">\(\mbox{plim}_{T \rightarrow \infty} -2 \left(\log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\mathbf{y})-\log \mathcal{L}(\hat{\boldsymbol\theta}_T(k_0);\mathbf{y})\right)/\phi(T) = 0\)</span>. Hence <span class="math inline">\(\mbox{plim}_{T \rightarrow \infty} T[c(k_0) - c(k)]/\phi(T) \le -1\)</span> and <span class="math inline">\(\lim_{T \rightarrow \infty} \mathbb{P}(c(k_0) \ge c(k)) \rightarrow 0\)</span>, which implies, in the same spirit as before, that <span class="math inline">\(\lim_{T \rightarrow \infty} \mathbb{P}(\hat{k}&gt;k_0) = 0\)</span>.</li>
</ul>
<p>Therefore, <span class="math inline">\(\lim_{T \rightarrow \infty} \mathbb{P}(\hat{k}=k_0) = 1\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:ICOLS" class="example"><strong>Example 1.5  (Linear regression) </strong></span>Consider a linear regression with normal disturbances:
<span class="math display">\[
y_t = \mathbf{x}_t' \boldsymbol\beta + \varepsilon_t, \quad \varepsilon_t \sim i.i.d. \mathcal{N}(0,\sigma^2).
\]</span>
The associated log-likelihood is of the form of Eq. <a href="TS.html#eq:MALstar">(1.26)</a>. In that case, we have:
<span class="math display">\[\begin{eqnarray*}
c^{(i)}(k) &amp;=&amp; \frac{- 2 \log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\mathbf{y})}{T} + \frac{k\phi^{(i)}(T)}{T}\\
&amp;\approx&amp; \log(2\pi) + \log(\widehat{\sigma^2}) + \frac{1}{T}\sum_{t=1}^T \frac{\varepsilon_t^2}{\widehat{\sigma^2}} + \frac{k\phi^{(i)}(T)}{T}.
\end{eqnarray*}\]</span>
For a large <span class="math inline">\(T\)</span>, for all consistent estimation scheme, we have:
<span class="math display">\[
\widehat{\sigma^2} \approx \frac{1}{T}\sum_{t=1}^T \varepsilon_t^2 = SSR/T.
\]</span>
Hence <span class="math inline">\(\hat{k}^{(i)} \approx \underset{k}{\mbox{argmin}} \quad \log(SSR/T) + \dfrac{k\phi^{(i)}(T)}{T}\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:SwissGrowthAIC" class="example"><strong>Example 1.6  (Swiss GDP growth) </strong></span>Consider a long historical time series of the Swiss GDP growth (see Figure <a href="TS.html#fig:autocov">1.3</a>), taken from the <span class="citation">Jordà, Schularick, and Taylor (<a href="references.html#ref-JST_2017" role="doc-biblioref">2017</a>)</span> dataset. Let us look for the best ARMA specification using the AIC criteria:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">JST</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">JST</span>,<span class="va">iso</span><span class="op">==</span><span class="st">"CHE"</span><span class="op">)</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">gdp</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">data</span><span class="op">$</span><span class="va">gdp</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Use AIC criteria to look for appropriate specif:</span></span>
<span><span class="va">max.p</span> <span class="op">&lt;-</span> <span class="fl">3</span>;<span class="va">max.q</span> <span class="op">&lt;-</span> <span class="fl">3</span>;</span>
<span><span class="va">all.AIC</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">p</span> <span class="kw">in</span> <span class="fl">0</span><span class="op">:</span><span class="va">max.p</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="kw">for</span><span class="op">(</span><span class="va">q</span> <span class="kw">in</span> <span class="fl">0</span><span class="op">:</span><span class="va">max.q</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.html">arima</a></span><span class="op">(</span><span class="va">y</span>,order<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">p</span>,<span class="fl">0</span>,<span class="va">q</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="kw">if</span><span class="op">(</span><span class="va">res</span><span class="op">$</span><span class="va">aic</span><span class="op">&lt;</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">all.AIC</span><span class="op">)</span><span class="op">)</span><span class="op">{</span><span class="va">best.p</span><span class="op">&lt;-</span><span class="va">p</span>;<span class="va">best.q</span><span class="op">&lt;-</span><span class="va">q</span><span class="op">}</span></span>
<span>    <span class="va">all.AIC</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">all.AIC</span>,<span class="va">res</span><span class="op">$</span><span class="va">aic</span><span class="op">)</span><span class="op">}</span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">best.p</span>,<span class="va">best.q</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 1 0</code></pre>
<p>The best specification therefore is an AR(1) model. That is, although an AR(2) (say) would result in a better fit of the data, the fit improvement is not be large enough to compensate for the additional AIC cost associated with an additional parameter.</p>
</div>
</div>
</div>
<div id="VAR" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> Multivariate models<a class="anchor" aria-label="anchor" href="#VAR"><i class="fas fa-link"></i></a>
</h2>
<p>This section presents Vector Auto-Regressive Moving-Average (SVARMA) models. These models are widely used in macroeconomic analysis. While simple and easy to estimate, they make it possible to conveniently capture the dynamics of complex multivariate systems. VAR popularity is notably due to <span class="citation">Sims (<a href="references.html#ref-Sims_1980" role="doc-biblioref">1980</a>)</span>’s influential work. A nice survey if proposed by <span class="citation">J. H. Stock and Watson (<a href="references.html#ref-Stock_Watson_2016" role="doc-biblioref">2016</a>)</span>.</p>
<p>In economics, VAR models are often employed in order to identify <em>structural</em> shocks, that are independent primitive exogenous forces that drive economic variables (<span class="citation">Ramey (<a href="references.html#ref-Ramey_2016_NBER" role="doc-biblioref">2016</a>)</span>). They are often given a specific economic meaning (e.g., demand and supply shocks).</p>
<p>Working with these models (VAR and VARMA models) often is often based on two steps: in a first step, the <strong>reduced-form</strong> version of the model is estimated; in a second step, <strong>structural shocks</strong> are identified and IRFs are produced.</p>
<!-- @Kilian_1998 See [this page](https://rdrr.io/cran/VAR.etp/man/VAR.Boot.html) -->
<!-- Sign restrictions: [package](https://github.com/chrstdanne/VARsignR), @Danne_2015. -->
<!-- @vars -->
<!-- <!-- toBibtex(citation("vars")) -->
<p>–&gt;</p>
<!-- @bvartools: R package to estimate Bayesian VAR models. -->
<div id="definition-of-vars-and-svarma-models" class="section level3" number="1.3.1">
<h3>
<span class="header-section-number">1.3.1</span> Definition of VARs (and SVARMA) models<a class="anchor" aria-label="anchor" href="#definition-of-vars-and-svarma-models"><i class="fas fa-link"></i></a>
</h3>
<div class="definition">
<p><span id="def:SVAR" class="definition"><strong>Definition 1.21  ((S)VAR model) </strong></span>Let <span class="math inline">\(y_{t}\)</span> denote a <span class="math inline">\(n \times1\)</span> vector of random variables. Process <span class="math inline">\(y_{t}\)</span> follows a <span class="math inline">\(p^{th}\)</span>-order (S)VAR if, for all <span class="math inline">\(t\)</span>, we have
<span class="math display" id="eq:yVAR">\[\begin{eqnarray}
\begin{array}{rllll}
VAR:&amp; y_t &amp;=&amp; c + \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p} + \varepsilon_t,\\
SVAR:&amp; y_t &amp;=&amp; c + \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p} + B \eta_t,
\end{array}\tag{1.32}
\end{eqnarray}\]</span>
with <span class="math inline">\(\varepsilon_t = B\eta_t\)</span>, where <span class="math inline">\(\{\eta_{t}\}\)</span> is a white noise sequence whose components are mutually and serially independent.</p>
</div>
<p>The first line of Eq. <a href="TS.html#eq:yVAR">(1.32)</a> corresponds to the <strong>reduced-form</strong> of the VAR model (<strong>structural form</strong> for the second line).</p>
<p>While the structural shocks (the components of <span class="math inline">\(\eta_t\)</span>) are mutually uncorrelated, this is not the case of the <em>innovations</em>, that are the components of <span class="math inline">\(\varepsilon_t\)</span>. However, in boths cases, vectors <span class="math inline">\(\eta_t\)</span> and <span class="math inline">\(\varepsilon_t\)</span> are serially correlated (through time).</p>
<p>As was the case for univariate models, VARs can be extended with MA terms in <span class="math inline">\(\eta_t\)</span>:</p>
<div class="definition">
<p><span id="def:SVARMA" class="definition"><strong>Definition 1.22  ((S)VARMA model) </strong></span>Let <span class="math inline">\(y_{t}\)</span> denote a <span class="math inline">\(n \times1\)</span> vector of random variables. Process <span class="math inline">\(y_{t}\)</span> follows a VARMA model of order (p,q) if, for all <span class="math inline">\(t\)</span>, we have
<span class="math display" id="eq:yVARMA">\[\begin{eqnarray}
\begin{array}{rllll}
VARMA:&amp; y_t &amp;=&amp; c + \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p} + \varepsilon_t + \Theta_1\varepsilon_{t-1} + \dots + \Theta_q ,\\
SVARMA:&amp; y_t &amp;=&amp; c + \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p} + B_0 \eta_t+ B_1 \eta_{t-1} + \dots +  B_q \eta_{t-q},
\end{array}\tag{1.33}
\end{eqnarray}\]</span>
with <span class="math inline">\(\varepsilon_t = B_0\eta_t\)</span> (and <span class="math inline">\(B_j = \Theta_j B_0\)</span>, for <span class="math inline">\(j \ge 0\)</span>), where <span class="math inline">\(\{\eta_{t}\}\)</span> is a white noise sequence whose components are mutually and serially independent.</p>
</div>
</div>
<div id="IRFSVARMA" class="section level3" number="1.3.2">
<h3>
<span class="header-section-number">1.3.2</span> IRFs in SVARMA<a class="anchor" aria-label="anchor" href="#IRFSVARMA"><i class="fas fa-link"></i></a>
</h3>
<p>One of the main objectives of macro-econometrics is to derive IRFs, that represent the dynamic effects of structural shocks (components of <span class="math inline">\(\eta_t\)</span>) though the system of variables <span class="math inline">\(y_t\)</span>.</p>
<!-- As illustrated by Figure \@ref(fig:NgramIRF), that makes use of Google Ngram data, there is a close link between the development of "macroeconomic analysis" and the concept ofimpulse response functions. -->
<!-- ```{r NgramIRF, echo=FALSE,fig.cap="Source: Google Ngram. Fraction of books containing the blue and red keywords."} -->
<!-- library(ngramr) -->
<!-- keyw <- c("impulse response function","macroeconomic analysis") -->
<!-- res1 <- ngram(keyw[1],year_start = 1900);res2 <- ngram(keyw[2],year_start = 1900) -->
<!-- plot(res1$Year,res1$Frequency,type="l",lwd=2,xlab="",ylab="",col="blue",las=1) -->
<!-- lines(res2$Year,res2$Frequency,type="l",lwd=2,col="red") -->
<!-- legend("topleft", -->
<!--        keyw,lty=c(1),lwd=c(2), -->
<!--        col=c("blue","red")) -->
<!-- ``` -->
<p>Formally, an IRF is a difference in conditional expectations:
<span class="math display">\[
\boxed{\Psi_{i,j,h} = \mathbb{E}(y_{i,t+h}|\eta_{j,t}=1) - \mathbb{E}(y_{i,t+h})}
\]</span>
(effect on <span class="math inline">\(y_{i,t+h}\)</span> of a one-unit shock on <span class="math inline">\(\eta_{j,t}\)</span>).</p>
<p>If the dynamics of process <span class="math inline">\(y_t\)</span> can be described as a VARMA model, and if <span class="math inline">\(y_t\)</span> is covariance stationary (see Def. <a href="TS.html#def:covstat">1.4</a>), then <span class="math inline">\(y_t\)</span> admits the following infinite MA representation (MA(<span class="math inline">\(\infty\)</span>)):
<span class="math display" id="eq:InfMA">\[\begin{equation}
y_t = \mu + \sum_{h=0}^\infty \Psi_{h} \eta_{t-h}.\tag{1.34}
\end{equation}\]</span>
This is also the Wold decomposition of process <span class="math inline">\(\{y_t\}\)</span> (see Theorem <a href="TS.html#thm:Wold">1.3</a>).</p>
<p>Estimating IRFs amounts to estimating the <span class="math inline">\(\Psi_{h}\)</span>’s. In general, there exist three main approaches for that:</p>
<ul>
<li>Calibrate and solve a (purely structural) Dynamic Stochastic General Equilibrium (DSGE) model at the first order (linearization). The solution takes the form of Eq. <a href="TS.html#eq:InfMA">(1.34)</a>.</li>
<li>Directly estimate the <span class="math inline">\(\Psi_{h}\)</span> based on <strong>projection approaches</strong> (see Section <a href="TS.html#Projections">1.3.11</a>).</li>
<li>Approximate the infinite MA representation by estimating a parsimonious type of model, e.g. <strong>VAR(MA) models</strong> (see Section <a href="TS.html#estimVAR">1.3.4</a>). Once a (Structural) VARMA representation is obtained, Eq. <a href="TS.html#eq:InfMA">(1.34)</a> is easily deduced. For that, one can use the same recursive algorithm as for univariate processes (see Prop. <a href="TS.html#prp:computPsi">1.8</a>).</li>
</ul>
<p>Typically, consider the AR(2) case. The first steps of the algorithm mentioned in the last bullet point are as follows:
<span class="math display">\[\begin{eqnarray*}
y_t &amp;=&amp; \Phi_1 {\color{blue}y_{t-1}} + \Phi_2 y_{t-2} + B \eta_t  \\
&amp;=&amp; \Phi_1 \color{blue}{(\Phi_1 y_{t-2} + \Phi_2 y_{t-3} + B \eta_{t-1})} + \Phi_2 y_{t-2} + B \eta_t  \\
&amp;=&amp; B \eta_t + \Phi_1 B \eta_{t-1} + (\Phi_2 + \Phi_1^2) \color{red}{y_{t-2}} + \Phi_1\Phi_2 y_{t-3}  \\
&amp;=&amp; B \eta_t + \Phi_1 B \eta_{t-1} + (\Phi_2 + \Phi_1^2) \color{red}{(\Phi_1 y_{t-3} + \Phi_2 y_{t-4} + B \eta_{t-2})} + \Phi_1\Phi_2 y_{t-3} \\
&amp;=&amp; \underbrace{B}_{=\Psi_0} \eta_t + \underbrace{\Phi_1 B}_{=\Psi_1} \eta_{t-1} + \underbrace{(\Phi_2 + \Phi_1^2)B}_{=\Psi_2} \eta_{t-2} + f(y_{t-3},y_{t-4}).
\end{eqnarray*}\]</span></p>
<p>In particular, we have <span class="math inline">\(B = \Psi_0\)</span>. Matrix <span class="math inline">\(B\)</span> indeed captures the contemporaneous impact of <span class="math inline">\(\eta_t\)</span> on <span class="math inline">\(y_t\)</span>. That is why matrix <span class="math inline">\(B\)</span> is sometimes called <em>impulse matrix</em>.</p>
<div class="example">
<p><span id="exm:IRFVARMA" class="example"><strong>Example 1.7  (IRFs of an SVARMA model) </strong></span>Consider the following VARMA(1,1) model:
<span class="math display" id="eq:VARMA111">\[\begin{eqnarray}
\quad y_t &amp;=&amp;
\underbrace{\left[\begin{array}{cc}
0.5 &amp; 0.3 \\
-0.4 &amp; 0.7
\end{array}\right]}_{\Phi_1}
y_{t-1} +  
\underbrace{\left[\begin{array}{cc}
1 &amp; 2 \\
-1 &amp; 1
\end{array}\right]}_{B}\eta_t + \underbrace{\left[\begin{array}{cc}
2 &amp; 0 \\
1 &amp; 0.5
\end{array}\right]}_{\Theta_1} \underbrace{\left[\begin{array}{cc}
1 &amp; 2 \\
-1 &amp; 1
\end{array}\right]}_{B}\eta_{t-1}.\tag{1.35}
\end{eqnarray}\]</span></p>
<p>We can use function <code>simul.VARMA</code> of package <code>AEC</code> to produce IRFs (using <code>indic.IRF=1</code> in the list of arguments):</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="va">distri</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>type<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"gaussian"</span>,<span class="st">"gaussian"</span><span class="op">)</span>,df<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>,<span class="fl">4</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">distri</span><span class="op">$</span><span class="va">type</span><span class="op">)</span> <span class="co"># dimension of y_t</span></span>
<span><span class="va">nb.sim</span> <span class="op">&lt;-</span> <span class="fl">30</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu">simul.distri</span><span class="op">(</span><span class="va">distri</span>,<span class="va">nb.sim</span><span class="op">)</span></span>
<span><span class="va">Phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">n</span>,<span class="va">n</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Phi</span><span class="op">[</span>,,<span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.5</span>,<span class="op">-</span><span class="fl">.4</span>,<span class="fl">.3</span>,<span class="fl">.7</span><span class="op">)</span>,<span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Phi</span><span class="op">)</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span></span>
<span><span class="va">Theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">n</span>,<span class="va">n</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Theta</span><span class="op">[</span>,,<span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">.5</span><span class="op">)</span>,<span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Theta</span><span class="op">)</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span></span>
<span><span class="va">Mu</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="va">n</span><span class="op">)</span></span>
<span><span class="va">C</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="op">-</span><span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">1</span><span class="op">)</span>,<span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">Model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>  Mu <span class="op">=</span> <span class="va">Mu</span>,Phi <span class="op">=</span> <span class="va">Phi</span>,Theta <span class="op">=</span> <span class="va">Theta</span>,C <span class="op">=</span> <span class="va">C</span>,distri <span class="op">=</span> <span class="va">distri</span><span class="op">)</span></span>
<span><span class="va">Y0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="va">n</span><span class="op">)</span></span>
<span><span class="va">eta0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">res.sim.1</span> <span class="op">&lt;-</span> <span class="fu">simul.VARMA</span><span class="op">(</span><span class="va">Model</span>,<span class="va">nb.sim</span>,<span class="va">Y0</span>,<span class="va">eta0</span>,indic.IRF<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">eta0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">res.sim.2</span> <span class="op">&lt;-</span> <span class="fu">simul.VARMA</span><span class="op">(</span><span class="va">Model</span>,<span class="va">nb.sim</span>,<span class="va">Y0</span>,<span class="va">eta0</span>,indic.IRF<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.15</span>,<span class="fl">.95</span>,<span class="fl">.25</span>,<span class="fl">.8</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">res.sim.1</span><span class="op">$</span><span class="va">Y</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span>,las<span class="op">=</span><span class="fl">1</span>,</span>
<span>     type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">3</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,</span>
<span>     main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Response of "</span>,<span class="va">y</span><span class="op">[</span><span class="fl">1</span>,<span class="st">"*,*"</span>,<span class="va">t</span><span class="op">]</span>,</span>
<span>                           <span class="st">" to a one-unit increase in "</span>,<span class="va">eta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>,col<span class="op">=</span><span class="st">"grey"</span>,lty<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">res.sim.2</span><span class="op">$</span><span class="va">Y</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span>,las<span class="op">=</span><span class="fl">1</span>,</span>
<span>     type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">3</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,</span>
<span>     main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Response of "</span>,<span class="va">y</span><span class="op">[</span><span class="fl">1</span>,<span class="st">"*,*"</span>,<span class="va">t</span><span class="op">]</span>,</span>
<span>                           <span class="st">" to a one-unit increase in "</span>,<span class="va">eta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>,col<span class="op">=</span><span class="st">"grey"</span>,lty<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">res.sim.1</span><span class="op">$</span><span class="va">Y</span><span class="op">[</span><span class="fl">2</span>,<span class="op">]</span>,las<span class="op">=</span><span class="fl">1</span>,</span>
<span>     type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">3</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,</span>
<span>     main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Response of "</span>,<span class="va">y</span><span class="op">[</span><span class="fl">2</span>,<span class="st">"*,*"</span>,<span class="va">t</span><span class="op">]</span>,</span>
<span>                           <span class="st">" to a one-unit increase in "</span>,<span class="va">eta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>,col<span class="op">=</span><span class="st">"grey"</span>,lty<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">res.sim.2</span><span class="op">$</span><span class="va">Y</span><span class="op">[</span><span class="fl">2</span>,<span class="op">]</span>,las<span class="op">=</span><span class="fl">1</span>,</span>
<span>     type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">3</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,</span>
<span>     main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Response of "</span>,<span class="va">y</span><span class="op">[</span><span class="fl">2</span>,<span class="st">"*,*"</span>,<span class="va">t</span><span class="op">]</span>,</span>
<span>                           <span class="st">" to a one-unit increase in "</span>,<span class="va">eta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>,col<span class="op">=</span><span class="st">"grey"</span>,lty<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:simVAR"></span>
<img src="TimeSeries_files/figure-html/simVAR-1.png" alt="Impulse response functions" width="95%"><p class="caption">
Figure 1.14: Impulse response functions
</p>
</div>
</div>
<!-- \includegraphics[width=.9\linewidth]{figures/RcodesFigure_illustrIRF.pdf} -->
<!-- \begin{defn}[Autocovariance of order $j$] -->
<!-- The autocovariance of order $j$ of $y_t$ is $\mathbb{C}ov(y_t,y_{t-j})$. -->
<!-- \end{defn} -->
<!-- \begin{defn}[Covariance-stationary process] -->
<!-- Process $y_t$ is covariance-stationary if $\mathbb{E}(y_t)$ and all autocovariances of $y_t$ are finite and do not depend on $t$. -->
<!-- \end{defn} -->
</div>
<div id="covariance-stationary-varma-models" class="section level3" number="1.3.3">
<h3>
<span class="header-section-number">1.3.3</span> Covariance-stationary VARMA models<a class="anchor" aria-label="anchor" href="#covariance-stationary-varma-models"><i class="fas fa-link"></i></a>
</h3>
<p>Let’s come back to the infinite MA case (Eq. <a href="TS.html#eq:InfMA">(1.34)</a>):
<span class="math display">\[
y_t = \mu + \sum_{h=0}^\infty \Psi_{h} \eta_{t-h}.
\]</span>
For <span class="math inline">\(y_t\)</span> to be covariance-stationary (and ergodic for the mean), it has to be the case that
<span class="math display" id="eq:condiInfiniteMA">\[\begin{equation}
\sum_{i=0}^\infty \|\Psi_i\| &lt; \infty,\tag{1.36}
\end{equation}\]</span>
where <span class="math inline">\(\|A\|\)</span> denotes a norm of the matrix <span class="math inline">\(A\)</span> (e.g. <span class="math inline">\(\|A\|=\sqrt{tr(AA')}\)</span>). This notably implies that if <span class="math inline">\(y_t\)</span> is stationary (and ergodic for the mean), then <span class="math inline">\(\|\Psi_h\|\rightarrow 0\)</span> when <span class="math inline">\(h\)</span> gets large.</p>
<p>What should be satisfied by <span class="math inline">\(\Phi_k\)</span>’s and <span class="math inline">\(\Theta_k\)</span>’s for a VARMA-based process (Eq. <a href="#eq:VARMAstd">(<strong>??</strong>)</a>) to be stationary? The conditions will be similar to that we had in the univariate case (see Prop. <a href="TS.html#prp:stability">1.6</a>). Let us introduce the following notations:
<span class="math display" id="eq:VARMA2">\[\begin{eqnarray}
y_t &amp;=&amp; c + \underbrace{\Phi_1 y_{t-1} + \dots +\Phi_p y_{t-p}}_{\color{blue}{\mbox{AR component}}} +  \tag{1.37}\\
&amp;&amp;\underbrace{B \eta_t+ \Theta_1 B \eta_{t-1}+ \dots+ \Theta_q B \eta_{t-q}}_{\color{red}{\mbox{MA component}}} \nonumber\\
&amp;\Leftrightarrow&amp; \underbrace{(I - \Phi_1 L - \dots - \Phi_p L^p)}_{= \color{blue}{\Phi(L)}}y_t = c +  \underbrace{ \color{red}{(I - \Theta_1 L - \ldots - \Theta_q L^q)}}_{=\color{red}{\Theta(L)}} B \eta_{t}. \nonumber
\end{eqnarray}\]</span></p>
<p>Process <span class="math inline">\(y_t\)</span> is stationary iff the roots of <span class="math inline">\(\det(\Phi(z))=0\)</span> are strictly outside the unit circle or, equivalently, iff the eigenvalues of
<span class="math display" id="eq:matrixPHI">\[\begin{equation}
\Phi = \left[\begin{array}{cccc}
\Phi_{1} &amp; \Phi_{2} &amp; \cdots &amp; \Phi_{p}\\
I &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; \ddots &amp; 0 &amp; 0\\
0 &amp; 0 &amp; I &amp; 0\end{array}\right]\tag{1.38}
\end{equation}\]</span>
lie strictly within the unit circle. Hence, as was the case for univariate processes, the covariance-stationarity of a VARMA model depends only on the specification of its AR part.</p>
<p>Let’s derive the first two unconditional moments of a (covariance-stationary) VARMA process.</p>
<p>Based on Eq. <a href="TS.html#eq:VARMA2">(1.37)</a>, we have <span class="math inline">\(\mathbb{E}(\Phi(L)y_t)=c\)</span>, which gives <span class="math inline">\(\Phi(1)\mathbb{E}(y_t)=c\)</span>, or::
<span class="math display">\[
\mathbb{E}(y_t) = (I - \Phi_1 - \dots - \Phi_p)^{-1}c.
\]</span>
The autocovariances of <span class="math inline">\(y_t\)</span> can be deduced from the infinite MA representation (Eq. <a href="TS.html#eq:InfMA">(1.34)</a>). We have:
<span class="math display">\[
\gamma_j \equiv \mathbb{C}ov(y_t,y_{t-j}) = \sum_{i=j}^\infty \Psi_i \Psi_{i-j}'.
\]</span>
(Note that this infinite sum exists as soon as Eq. <a href="TS.html#eq:condiInfiniteMA">(1.36)</a> is satisfied.)</p>
<p>Conditional means and autocovariances can also be deduced from Eq. <a href="TS.html#eq:InfMA">(1.34)</a>. For <span class="math inline">\(0 \le h\)</span> and <span class="math inline">\(0 \le h_1 \le h_2\)</span>:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}_t(y_{t+h}) &amp;=&amp; \mu + \sum_{k=0}^\infty \Psi_{k+h} \eta_{t-k} \\
\mathbb{C}ov_t(y_{t+1+h_1},y_{t+1+h_2}) &amp;=&amp; \sum_{k=0}^{h_1} \Psi_{k}\Psi_{k+h_2-h_1}'.
\end{eqnarray*}\]</span></p>
<p>The previous formula implies in particular that the forecasting error <span class="math inline">\(y_{t+h} - \mathbb{E}_t(y_{t+h})\)</span> has a variance equal to:
<span class="math display">\[
\mathbb{V}ar_t(y_{t+h}) = \sum_{k=1}^{h} \Psi_{k}\Psi_{k}'.
\]</span>
Because the <span class="math inline">\(\eta_t\)</span> are mutually and serially independent (and therefore uncorrelated), we have:
<span class="math display">\[
\mathbb{V}ar(\Psi_k \eta_{t-k}) = \mathbb{V}ar\left(\sum_{i=1}^n \psi_{k,i} \eta_{i,t-k}\right)  = \sum_{i=1}^n \psi_{k,i}\psi_{k,i}',
\]</span>
where <span class="math inline">\(\psi_{k,i}\)</span> denotes the <span class="math inline">\(i^{th}\)</span> column of <span class="math inline">\(\Psi_k\)</span>.</p>
<p>This suggests the following decomposition of the variance of the forecast error (called <strong>variance decomposition</strong>):
<span class="math display">\[
\mathbb{V}ar_t(y_{t+h}) = \sum_{i=1}^n \underbrace{\sum_{k=1}^{h}  \psi_{k,i}\psi_{k,i}'}_{\mbox{Contribution of $\eta_{i,t}$}}.
\]</span></p>
<p>Let us now turn to the estimation of VAR(MA) models.</p>
<p>If there is a MA component, OLS regressions yield biased estimates (even for asymptotically large samples).</p>
<p>Assume <span class="math inline">\(y_t\)</span> follows a VARMA(1,1) model. We have:
<span class="math display">\[
y_{i,t} = \phi_i y_{t-1} + \varepsilon_{i,t},
\]</span>
where <span class="math inline">\(\phi_i\)</span> is the <span class="math inline">\(i^{th}\)</span> row of <span class="math inline">\(\Phi_1\)</span>, and where <span class="math inline">\(\varepsilon_{i,t}\)</span> is a linear combination of <span class="math inline">\(\eta_t\)</span> and <span class="math inline">\(\eta_{t-1}\)</span>.</p>
<p>Since <span class="math inline">\(y_{t-1}\)</span> (the regressor) is correlated to <span class="math inline">\(\eta_{t-1}\)</span>, it is also correlated to <span class="math inline">\(\varepsilon_{i,t}\)</span>.</p>
<p>The OLS regression of <span class="math inline">\(y_{i,t}\)</span> on <span class="math inline">\(y_{t-1}\)</span> yields a biased estimator of <span class="math inline">\(\phi_i\)</span>. Hence, SVARMA models cannot be consistently estimated by simple OLS regressions (contrary to VAR models, as we will see in the next section); instrumental-variable approaches can be employed to estimate SVARMA models.</p>
</div>
<div id="estimVAR" class="section level3" number="1.3.4">
<h3>
<span class="header-section-number">1.3.4</span> VAR estimation<a class="anchor" aria-label="anchor" href="#estimVAR"><i class="fas fa-link"></i></a>
</h3>
<p>This section discusses the estimation of VAR models. (The estimation of SVARMA models is more challenging, see, e.g., <span class="citation">Gouriéroux, Monfort, and Renne (<a href="references.html#ref-Gourieroux_Monfort_Renne_2020" role="doc-biblioref">2020</a>)</span>.) Eq. <a href="TS.html#eq:yVAR">(1.32)</a> can be written:
<span class="math display">\[
y_{t}=c+\Phi(L)y_{t-1}+\varepsilon_{t},
\]</span>
with <span class="math inline">\(\Phi(L) = \Phi_1 + \Phi_2 L + \dots + \Phi_p L^{p-1}\)</span>.</p>
<p>Consequently:
<span class="math display">\[
y_{t}\mid y_{t-1},y_{t-2},\ldots,y_{-p+1}\sim \mathcal{N}(c+\Phi_{1}y_{t-1}+\ldots\Phi_{p}y_{t-p},\Omega).
\]</span></p>
<p>Using <span class="citation">Hamilton (<a href="references.html#ref-Hamilton_1994" role="doc-biblioref">1994</a>)</span>’s notations, denote with <span class="math inline">\(\Pi\)</span> the matrix <span class="math inline">\(\left[\begin{array}{ccccc} c &amp; \Phi_{1} &amp; \Phi_{2} &amp; \ldots &amp; \Phi_{p}\end{array}\right]'\)</span> and with <span class="math inline">\(x_{t}\)</span> the vector <span class="math inline">\(\left[\begin{array}{ccccc} 1 &amp; y'_{t-1} &amp; y'_{t-2} &amp; \ldots &amp; y'_{t-p}\end{array}\right]'\)</span>, we have:
<span class="math display" id="eq:PIVAR">\[\begin{equation}
y_{t}= \Pi'x_{t} + \varepsilon_{t}. \tag{1.39}
\end{equation}\]</span>
The previous representation is convenient to discuss the estimation of the VAR model, as parameters are gathered in two matrices only: <span class="math inline">\(\Pi\)</span> and <span class="math inline">\(\Omega\)</span>.</p>
<p>Let us start with the case where the shocks are Gaussian.</p>
<div class="proposition">
<p><span id="prp:estimVARGaussian" class="proposition"><strong>Proposition 1.12  (MLE of a Gaussian VAR) </strong></span>If <span class="math inline">\(y_t\)</span> follows a VAR(p) (see Definition <a href="TS.html#def:SVAR">1.21</a>), and if <span class="math inline">\(\varepsilon_t \sim \,i.i.d.\,\mathcal{N}(0,\Omega)\)</span>, then the ML estimate of <span class="math inline">\(\Pi\)</span>, denoted by <span class="math inline">\(\hat{\Pi}\)</span> (see Eq. <a href="TS.html#eq:PIVAR">(1.39)</a>), is given by
<span class="math display" id="eq:Pi">\[\begin{equation}
\hat{\Pi}=\left[\sum_{t=1}^{T}x_{t}x'_{t}\right]^{-1}\left[\sum_{t=1}^{T}y_{t}'x_{t}\right]= (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y},\tag{1.40}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{X}\)</span> is the <span class="math inline">\(T \times (np)\)</span> matrix whose <span class="math inline">\(t^{th}\)</span> row is <span class="math inline">\(x_t\)</span> and where <span class="math inline">\(\mathbf{y}\)</span> is the <span class="math inline">\(T \times n\)</span> matrix whose <span class="math inline">\(t^{th}\)</span> row is <span class="math inline">\(y_{t}'\)</span>.</p>
<p>That is, the <span class="math inline">\(i^{th}\)</span> column of <span class="math inline">\(\hat{\Pi}\)</span> (<span class="math inline">\(b_i\)</span>, say) is the OLS estimate of <span class="math inline">\(\beta_i\)</span>, where:
<span class="math display" id="eq:betayx">\[\begin{equation}
y_{i,t} = \beta_i'x_t + \varepsilon_{i,t},\tag{1.41}
\end{equation}\]</span>
(i.e., <span class="math inline">\(\beta_i' = [c_i,\phi_{i,1}',\dots,\phi_{i,p}']'\)</span>).</p>
<p>The ML estimate of <span class="math inline">\(\Omega\)</span>, denoted by <span class="math inline">\(\hat{\Omega}\)</span>, coincides with the sample covariance matrix of the <span class="math inline">\(n\)</span> series of the OLS residuals in Eq. <a href="TS.html#eq:betayx">(1.41)</a>, i.e.:
<span class="math display">\[\begin{equation}
\hat{\Omega} = \frac{1}{T} \sum_{i=1}^T \hat{\varepsilon}_t\hat{\varepsilon}_t',\quad\mbox{with } \hat{\varepsilon}_t= y_t - \hat{\Pi}'x_t.
\end{equation}\]</span></p>
<p>The asymptotic distributions of these estimators are the ones resulting from standard OLS formula.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-13" class="proof"><em>Proof</em>. </span>See Appendix <a href="append.html#AppendixProof">2.5</a>.</p>
</div>
<p>As stated by Proposition <a href="TS.html#prp:OLSVAR">1.13</a>, when the shocks are not Gaussian, then the OLS regressions still provide consistent estimates of the model parameters. However, since <span class="math inline">\(x_t\)</span> correlates to <span class="math inline">\(\varepsilon_s\)</span> for <span class="math inline">\(s&lt;t\)</span>, the OLS estimator <span class="math inline">\(\mathbf{b}_i\)</span> of <span class="math inline">\(\boldsymbol\beta_i\)</span> is biased in small sample. (That is also the case for the ML estimator.)</p>
<p>Indeed, denoting by <span class="math inline">\(\boldsymbol\varepsilon_i\)</span> the <span class="math inline">\(T \times 1\)</span> vector of <span class="math inline">\(\varepsilon_{i,t}\)</span>’s, and using the notations of <span class="math inline">\(b_i\)</span> and <span class="math inline">\(\beta_i\)</span> introduced in Proposition <a href="TS.html#prp:estimVARGaussian">1.12</a>, we have:
<span class="math display" id="eq:olsar1">\[\begin{equation}
\mathbf{b}_i = \beta_i + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol\varepsilon_i.\tag{1.24}
\end{equation}\]</span>
We have non-zero correlation between <span class="math inline">\(x_t\)</span> and <span class="math inline">\(\varepsilon_{i,s}\)</span> for <span class="math inline">\(s&lt;t\)</span> and, therefore, <span class="math inline">\(\mathbb{E}[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol\varepsilon_i] \ne 0\)</span>.</p>
<p>However, when <span class="math inline">\(y_t\)</span> is covariance stationary, then <span class="math inline">\(\frac{1}{n}\mathbf{X}'\mathbf{X}\)</span> converges to a positive definite matrix <span class="math inline">\(\mathbf{Q}\)</span>, and <span class="math inline">\(\frac{1}{n}X'\boldsymbol\varepsilon_i\)</span> converges to 0. Hence <span class="math inline">\(\mathbf{b}_i \overset{p}{\rightarrow} \beta_i\)</span>. More precisely:</p>
<div class="proposition">
<p><span id="prp:OLSVAR" class="proposition"><strong>Proposition 1.13  (Asymptotic distribution of the OLS estimate of $\beta_i$) </strong></span>If <span class="math inline">\(y_t\)</span> follows a VAR model, as defined in Definition <a href="TS.html#def:SVAR">1.21</a>, we have:
<span class="math display">\[
\sqrt{T}(\mathbf{b}_i-\beta_i) =  \underbrace{\left[\frac{1}{T}\sum_{t=p}^T x_t x_t' \right]^{-1}}_{\overset{p}{\rightarrow} \mathbf{Q}^{-1}}
\underbrace{\sqrt{T} \left[\frac{1}{T}\sum_{t=1}^T x_t\varepsilon_{i,t} \right]}_{\overset{d}{\rightarrow} \mathcal{N}(0,\sigma_i^2\mathbf{Q})},
\]</span>
where <span class="math inline">\(\sigma_i = \mathbb{V}ar(\varepsilon_{i,t})\)</span> and where <span class="math inline">\(\mathbf{Q} = \mbox{plim }\frac{1}{T}\sum_{t=p}^T x_t x_t'\)</span> is given by:
<span class="math display" id="eq:Qols">\[\begin{equation}
\mathbf{Q} = \left[
\begin{array}{ccccc}
1 &amp; \mu' &amp;\mu' &amp; \dots &amp; \mu' \\
\mu &amp; \gamma_0 + \mu\mu' &amp; \gamma_1 + \mu\mu' &amp; \dots &amp; \gamma_{p-1} + \mu\mu'\\
\mu &amp; \gamma_1 + \mu\mu' &amp; \gamma_0 + \mu\mu' &amp; \dots &amp; \gamma_{p-2} + \mu\mu'\\
\vdots &amp;\vdots &amp;\vdots &amp;\dots &amp;\vdots \\
\mu &amp; \gamma_{p-1} + \mu\mu' &amp; \gamma_{p-2} + \mu\mu' &amp; \dots &amp; \gamma_{0} + \mu\mu'
\end{array}
\right].\tag{1.25}
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-14" class="proof"><em>Proof</em>. </span>See Appendix <a href="append.html#AppendixProof">2.5</a>.</p>
</div>
<p>The following proposition extends the previous proposition and includes covariances between different <span class="math inline">\(\beta_i\)</span>’s as well as the asymptotic distribution of the ML estimates of <span class="math inline">\(\Omega\)</span>.</p>
<div class="proposition">
<p><span id="prp:OLSVAR2" class="proposition"><strong>Proposition 1.14  (Asymptotic distribution of the OLS estimates) </strong></span>If <span class="math inline">\(y_t\)</span> follows a VAR model, as defined in Definition <a href="TS.html#def:SVAR">1.21</a>, we have:
<span class="math display" id="eq:asymptPi">\[\begin{equation}
\sqrt{T}\left[
\begin{array}{c}
vec(\hat\Pi - \Pi)\\
vec(\hat\Omega - \Omega)
\end{array}
\right]
\sim \mathcal{N}\left(0,
\left[
\begin{array}{cc}
\Omega \otimes \mathbf{Q}^{-1} &amp; 0\\
0 &amp; \Sigma_{22}
\end{array}
\right]\right),\tag{1.42}
\end{equation}\]</span>
where the component of <span class="math inline">\(\Sigma_{22}\)</span> corresponding to the covariance between <span class="math inline">\(\hat\sigma_{i,j}\)</span> and <span class="math inline">\(\hat\sigma_{k,l}\)</span> (for <span class="math inline">\(i,j,l,m \in \{1,\dots,n\}^4\)</span>) is equal to <span class="math inline">\(\sigma_{i,l}\sigma_{j,m}+\sigma_{i,m}\sigma_{j,l}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-15" class="proof"><em>Proof</em>. </span>See <span class="citation">Hamilton (<a href="references.html#ref-Hamilton_1994" role="doc-biblioref">1994</a>)</span>, Appendix of Chapter 11.</p>
</div>
<p>Naturally, in practice, <span class="math inline">\(\Omega\)</span> is replaced with <span class="math inline">\(\hat{\Omega}\)</span>, <span class="math inline">\(\mathbf{Q}\)</span> is replaced with <span class="math inline">\(\hat{\mathbf{Q}} = \frac{1}{T}\sum_{t=p}^T x_t x_t'\)</span> and <span class="math inline">\(\Sigma\)</span> with the matrix whose components are of the form <span class="math inline">\(\hat\sigma_{i,l}\hat\sigma_{j,m}+\hat\sigma_{i,m}\hat\sigma_{j,l}\)</span>, where the <span class="math inline">\(\hat\sigma_{i,l}\)</span>’s are the components of <span class="math inline">\(\hat\Omega\)</span>.</p>
<p>The simplicity of the VAR framework and the tractability of its MLE open the way to convenient econometric testing. Let’s illustrate this with the likelihood ratio test (see Def. <a href="#def:LR"><strong>??</strong></a>). The maximum value achieved by the MLE is
<span class="math display">\[
\log\mathcal{L}(Y_{T};\hat{\Pi},\hat{\Omega}) = -\frac{Tn}{2}\log(2\pi)+\frac{T}{2}\log\left|\hat{\Omega}^{-1}\right| -\frac{1}{2}\sum_{t=1}^{T}\left[\hat{\varepsilon}_{t}'\hat{\Omega}^{-1}\hat{\varepsilon}_{t}\right].
\]</span>
The last term is:
<span class="math display">\[\begin{eqnarray*}
\sum_{t=1}^{T}\hat{\varepsilon}_{t}'\hat{\Omega}^{-1}\hat{\varepsilon}_{t} &amp;=&amp; \mbox{Tr}\left[\sum_{t=1}^{T}\hat{\varepsilon}_{t}'\hat{\Omega}^{-1}\hat{\varepsilon}_{t}\right] = \mbox{Tr}\left[\sum_{t=1}^{T}\hat{\Omega}^{-1}\hat{\varepsilon}_{t}\hat{\varepsilon}_{t}'\right]\\
&amp;=&amp;\mbox{Tr}\left[\hat{\Omega}^{-1}\sum_{t=1}^{T}\hat{\varepsilon}_{t}\hat{\varepsilon}_{t}'\right] = \mbox{Tr}\left[\hat{\Omega}^{-1}\left(T\hat{\Omega}\right)\right]=Tn.
\end{eqnarray*}\]</span>
Therefore, the optimized log-likelihood is simply obtained by:
<span class="math display" id="eq:optimzedLogL">\[\begin{equation}
\log\mathcal{L}(Y_{T};\hat{\Pi},\hat{\Omega})=-(Tn/2)\log(2\pi)+(T/2)\log\left|\hat{\Omega}^{-1}\right|-Tn/2.\tag{1.43}
\end{equation}\]</span></p>
<p>Assume that we want to test the null hypothesis that a set of variables follows a VAR(<span class="math inline">\(p_{0}\)</span>) against the alternative
specification of <span class="math inline">\(p_{1}\)</span> (<span class="math inline">\(&gt;p_{0}\)</span>).</p>
<p>Let us denote by <span class="math inline">\(\hat{L}_{0}\)</span> and <span class="math inline">\(\hat{L}_{1}\)</span> the maximum log-likelihoods obtained with <span class="math inline">\(p_{0}\)</span> and <span class="math inline">\(p_{1}\)</span> lags, respectively.</p>
<p>Under the null hypothesis (<span class="math inline">\(H_0\)</span>: <span class="math inline">\(p=p_0\)</span>), we have:
<span class="math display">\[\begin{eqnarray*}
2\left(\hat{L}_{1}-\hat{L}_{0}\right)&amp;=&amp;T\left(\log\left|\hat{\Omega}_{1}^{-1}\right|-\log\left|\hat{\Omega}_{0}^{-1}\right|\right)  \sim \chi^2(n^{2}(p_{1}-p_{0})).
\end{eqnarray*}\]</span></p>
<p>What precedes can be used to help determine the appropriate number of lags to use in the specification. In a VAR, using too many lags consumes numerous degrees of freedom: with <span class="math inline">\(p\)</span> lags, each of the <span class="math inline">\(n\)</span> equations in the VAR contains <span class="math inline">\(n\times p\)</span> coefficients plus the intercept term. Adding lags improve in-sample fit, but is likely to result in over-parameterization and affect the <strong>out-of-sample</strong> prediction performance.</p>
<p>To select appropriate lag length, <strong>selection criteria</strong> can be used (see Definition <a href="TS.html#def:infocriteria">1.20</a>). In the context of VAR models, using Eq. <a href="TS.html#eq:optimzedLogL">(1.43)</a>, we have:
<span class="math display">\[\begin{eqnarray*}
AIC &amp; = &amp; cst + \log\left|\hat{\Omega}\right|+\frac{2}{T}N\\
BIC &amp; = &amp; cst + \log\left|\hat{\Omega}\right|+\frac{\log T}{T}N,
\end{eqnarray*}\]</span>
where <span class="math inline">\(N=p \times n^{2}\)</span>.</p>
</div>
<div id="BlockGranger" class="section level3" number="1.3.5">
<h3>
<span class="header-section-number">1.3.5</span> Block exogeneity and Granger causality<a class="anchor" aria-label="anchor" href="#BlockGranger"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Block exogeneity</strong></p>
<p>Let’s decompose <span class="math inline">\(y_t\)</span> into two subvectors <span class="math inline">\(y^{(1)}_{t}\)</span> (<span class="math inline">\(n_1 \times 1\)</span>) and <span class="math inline">\(y^{(2)}_{t}\)</span> (<span class="math inline">\(n_2 \times 1\)</span>), with <span class="math inline">\(y_t' = [{y^{(1)}_{t}}',{y^{(2)}_{t}}']\)</span> (and therefore <span class="math inline">\(n=n_1 +n_2\)</span>), such that:
<span class="math display">\[
\left[
\begin{array}{c}
y^{(1)}_{t}\\
y^{(2)}_{t}
\end{array}
\right] = \left[
\begin{array}{cc}
\Phi^{(1,1)} &amp; \Phi^{(1,2)}\\
\Phi^{(2,1)} &amp; \Phi^{(2,2)}
\end{array}
\right]
\left[
\begin{array}{c}
y^{(1)}_{t-1}\\
y^{(2)}_{t-1}
\end{array}
\right] + \varepsilon_t.
\]</span>
Using, e.g., a likelihood ratio test (see Def. <a href="#def:LR"><strong>??</strong></a>), one can easily test for block exogeneity of <span class="math inline">\(y_t^{(2)}\)</span> (say). The null assumption can be expressed as <span class="math inline">\(\Phi^{(2,1)}=0\)</span>.</p>
<!-- **Companion Form and Stability of a VAR process** -->
<!-- Let us introduce vector $y_{t}^{*}$, whihc stacks the last $p$ values of $y_t$: -->
<!-- $$ -->
<!-- y_{t}^{*}=\left[\begin{array}{cccc} -->
<!-- y'_{t} & y'_{t-1} & \ldots & y'_{t-p+1}\end{array}\right]^{'}, -->
<!-- $$ -->
<!-- Eq. \@ref(eq:yVAR) can then be rewritten in its companion form: -->
<!-- \begin{equation} -->
<!-- y_{t}^{*} = -->
<!-- \underbrace{\left[\begin{array}{c} -->
<!-- c\\ -->
<!-- 0\\ -->
<!-- \vdots\\ -->
<!-- 0\end{array}\right]}_{=c^*}+ -->
<!-- \underbrace{\left[\begin{array}{cccc} -->
<!-- \Phi_{1} & \Phi_{2} & \cdots & \Phi_{p}\\ -->
<!-- I & 0 & \cdots & 0\\ -->
<!-- 0 & \ddots & 0 & 0\\ -->
<!-- 0 & 0 & I & 0\end{array}\right]}_{=\Phi} -->
<!-- y_{t-1}^{*}+ -->
<!-- \underbrace{\left[\begin{array}{c} -->
<!-- \varepsilon_{t}\\ -->
<!-- 0\\ -->
<!-- \vdots\\ -->
<!-- 0\end{array}\right]}_{\varepsilon_t^*}(\#eq:ystarVAR) -->
<!-- \end{equation} -->
<!-- Matrices $\Phi$ and $\Sigma^* = \mathbb{V}ar(\varepsilon_t^*)$ are of dimension $np \times np$. $\Sigma^*$ is filled with zeros, except the $n\times n$ upper-left block that is equal to $\Sigma = \mathbb{V}ar(\varepsilon_t)$. -->
<!-- We then have: -->
<!-- \begin{eqnarray*} -->
<!-- y_{t}^{*} & = & c^{*}+\Phi\left(c^{*}+\Phi y_{t-2}^{*}+\varepsilon_{t-1}^{*}\right)+\varepsilon_{t}^{*} \nonumber \\ -->
<!-- & = & c^{*}+\varepsilon_{t}^{*}+\Phi(c^{*}+\varepsilon_{t-1}^{*})+\ldots+\Phi^{k}(c^{*}+\varepsilon_{t-k}^{*})+\Phi^k y_{t-k}^{*}. -->
<!-- \end{eqnarray*} -->
<!-- If the eigenvalues of $\Phi$ are strictly within the unit circle, then $\Phi^k$ geometrically decays to the zero matrix and we get the following Wold decomposition for $y_t$: -->
<!-- \begin{eqnarray} -->
<!-- y_{t}^{*}  & = & c^{*}+\varepsilon_{t}^{*}+\Phi(c^{*}+\varepsilon_{t-1}^{*})+\ldots+\Phi^{k}(c^{*}+\varepsilon_{t-k}^{*})+\ldots \nonumber \\ -->
<!-- & = & \mu^{*} +\varepsilon_{t}^{*}+\Phi\varepsilon_{t-1}^{*}+\ldots+\Phi^{k}\varepsilon_{t-k}^{*}+\ldots,(\#eq:VARstar) -->
<!-- \end{eqnarray} -->
<!-- where $\mu^* = (I - \Phi)^{-1} c^*$. -->
<!-- (It can also be seen that $\mu^{*} = [\mu',\dots,\mu']'$, where $\mu = (I - \Phi_1 - \dots - \Phi_p)^{-1}c$). -->
<!-- The unconditional variance of $y_t$ can be derived from Eq. \@ref(eq:VARstar), exploiting the fact that the $\varepsilon_{t}^{*}$ are serially uncorrelated: -->
<!-- $$ -->
<!-- \mathbb{V}ar(y_t^*)=\Omega^*+\Phi\Omega^*\Phi'+\ldots+\Phi^{k}\Omega^*\Phi'^{k}+\ldots, -->
<!-- $$ -->
<!-- with $\mathbb{V}ar(\varepsilon_t^*)=\Omega^*$. -->
<!-- The unconditional variance of $y_t$ is the upper-left $n\times n$ block of matrix $\mathbb{V}ar(y_t^*)$. -->
<!-- Eq. \@ref(eq:VARstar) also implies that the $\Psi_k$ matrices defining the IRFs  (see Eq. \@ref(eq:InfMA)) are given by: $\Psi_k = \widetilde{\Phi^k}B$, where $\widetilde{\Phi^k}$ is the upper-left matrix block of $\Phi^k$. -->
<p><strong>Granger Causality</strong></p>
<p><span class="citation">Granger (<a href="references.html#ref-Granger_1969" role="doc-biblioref">1969</a>)</span> developed a method to explore <strong>causal relationships</strong> among variables. The approach consists in determining whether the past values of <span class="math inline">\(y_{1,t}\)</span> can help explain the current <span class="math inline">\(y_{2,t}\)</span> (beyond the information already included in the past values of <span class="math inline">\(y_{2,t}\)</span>).</p>
<p>Formally, let us denote three information sets:
<span class="math display">\[\begin{eqnarray*}
\mathcal{I}_{1,t} &amp; = &amp; \left\{ y_{1,t},y_{1,t-1},\ldots\right\} \\
\mathcal{I}_{2,t} &amp; = &amp; \left\{ y_{2,t},y_{2,t-1},\ldots\right\} \\
\mathcal{I}_{t} &amp; = &amp; \left\{ y_{1,t},y_{1,t-1},\ldots y_{2,t},y_{2,t-1},\ldots\right\}.
\end{eqnarray*}\]</span>
We say that <span class="math inline">\(y_{1,t}\)</span> Granger-causes <span class="math inline">\(y_{2,t}\)</span> if
<span class="math display">\[
\mathbb{E}\left[y_{2,t}\mid \mathcal{I}_{2,t-1}\right]\neq \mathbb{E}\left[y_{2,t}\mid \mathcal{I}_{t-1}\right].
\]</span></p>
<p>To get the intuition behind the testing procedure, consider the following
bivariate VAR(<span class="math inline">\(p\)</span>) process:
<span class="math display">\[\begin{eqnarray*}
y_{1,t} &amp; = &amp; c_1+\Sigma_{i=1}^{p}\Phi_i^{(11)}y_{1,t-i}+\Sigma_{i=1}^{p}\Phi_i^{(12)}y_{2,t-i}+\varepsilon_{1,t}\\
y_{2,t} &amp; = &amp; c_2+\Sigma_{i=1}^{p}\Phi_i^{(21)}y_{1,t-i}+\Sigma_{i=1}^{p}\Phi_i^{(22)}y_{2,t-i}+\varepsilon_{2,t},
\end{eqnarray*}\]</span>
where <span class="math inline">\(\Phi_k^{(ij)}\)</span> denotes the element <span class="math inline">\((i,j)\)</span> of <span class="math inline">\(\Phi_k\)</span>.</p>
<p>Then, <span class="math inline">\(y_{1,t}\)</span> is said not to Granger-cause <span class="math inline">\(y_{2,t}\)</span> if
<span class="math display">\[
\Phi_1^{(21)}=\Phi_2^{(21)}=\ldots=\Phi_p^{(21)}=0.
\]</span>
Therefore the hypothesis testing is
<span class="math display">\[
\begin{cases}
H_{0}: &amp; \Phi_1^{(21)}=\Phi_2^{(21)}=\ldots=\Phi_p^{(21)}=0\\
H_{1}: &amp; \Phi_1^{(21)}\neq0\mbox{ or }\Phi_2^{(21)}\neq0\mbox{ or}\ldots\Phi_p^{(21)}\neq0.\end{cases}
\]</span>
Loosely speaking, we reject <span class="math inline">\(H_{0}\)</span> if some of the coefficients on the lagged <span class="math inline">\(y_{1,t}\)</span>’s are statistically significant. Formally, this can be tested using the <span class="math inline">\(F\)</span>-test or asymptotic chi-square test. The <span class="math inline">\(F\)</span>-statistic is
<span class="math display">\[
F=\frac{(RSS-USS)/p}{USS/(T-2p-1)},
\]</span>
where RSS is the Restricted sum of squared residuals and USS is the Unrestricted sum of squared residuals. Under <span class="math inline">\(H_{0}\)</span>, the <span class="math inline">\(F\)</span>-statistic is distributed as <span class="math inline">\(\mathcal{F}(p,T-2p-1)\)</span>. (We have <span class="math inline">\(pF\underset{T \rightarrow \infty}{\rightarrow}\chi^{2}(p)\)</span>.)</p>
</div>
<div id="identification-problem-and-standard-identification-techniques" class="section level3" number="1.3.6">
<h3>
<span class="header-section-number">1.3.6</span> Identification problem and standard identification techniques<a class="anchor" aria-label="anchor" href="#identification-problem-and-standard-identification-techniques"><i class="fas fa-link"></i></a>
</h3>
<p>In Section <a href="TS.html#estimVAR">1.3.4</a>, we have seen how to estimate <span class="math inline">\(\mathbb{V}ar(\varepsilon_t) =\Omega\)</span> and the <span class="math inline">\(\Phi_k\)</span> matrices in the context of a VAR model. But the IRFs are functions of <span class="math inline">\(B\)</span> and the <span class="math inline">\(\Phi_k\)</span>’s, not of <span class="math inline">\(\Omega\)</span> the <span class="math inline">\(\Phi_k\)</span>’s (see Section <a href="TS.html#IRFSVARMA">1.3.2</a>). We have <span class="math inline">\(\Omega = BB'\)</span>, but this is not sufficient to recover <span class="math inline">\(B\)</span>.</p>
<p>Indeed, seen a system of equations whose unknowns are the <span class="math inline">\(b_{i,j}\)</span>’s (components of <span class="math inline">\(B\)</span>), the system <span class="math inline">\(\Omega = BB'\)</span> contains only <span class="math inline">\(n(n+1)/2\)</span> linearly independent equations. For instance, for <span class="math inline">\(n=2\)</span>:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\left[
\begin{array}{cc}
\omega_{11} &amp; \omega_{12} \\
\omega_{12} &amp; \omega_{22}
\end{array}
\right] = \left[
\begin{array}{cc}
b_{11} &amp; b_{12} \\
b_{21} &amp; b_{22}
\end{array}
\right]\left[
\begin{array}{cc}
b_{11} &amp; b_{21} \\
b_{12} &amp; b_{22}
\end{array}
\right]\\
&amp;\Leftrightarrow&amp;\left[
\begin{array}{cc}
\omega_{11} &amp; \omega_{12} \\
\omega_{12} &amp; \omega_{22}
\end{array}
\right] = \left[
\begin{array}{cc}
b_{11}^2+b_{12}^2 &amp; \color{red}{b_{11}b_{21}+b_{12}b_{22}} \\
\color{red}{b_{11}b_{21}+b_{12}b_{22}} &amp; b_{22}^2 + b_{21}^2
\end{array}
\right].
\end{eqnarray*}\]</span></p>
<p>We then have 3 linearly independent equations but 4 unknowns. Therefore, <span class="math inline">\(B\)</span> is not identified based on second-order moments. Additional restrictions are required to identify <span class="math inline">\(B\)</span>. This section covers two standard identification schemes: <strong>short-run</strong> and <strong>long-run</strong> restrictions:</p>
<ol style="list-style-type: decimal">
<li>A <strong>short-run restriction (SRR)</strong> prevents a structural shock from affecting an endogenous variable contemporaneously.</li>
</ol>
<ul>
<li>Easy to implement: the appropriate entries of <span class="math inline">\(B\)</span> are set to 0.</li>
<li>Particular case: <strong>Cholesky, or recursive approach</strong>.</li>
<li>Examples: <span class="citation">Bernanke (<a href="references.html#ref-BERNANKE198649" role="doc-biblioref">1986</a>)</span>, <span class="citation">Sims (<a href="references.html#ref-Sims_1986" role="doc-biblioref">1986</a>)</span>, <span class="citation">Galí (<a href="references.html#ref-Gali_1992" role="doc-biblioref">1992</a>)</span>, <span class="citation">Ruibio-Ramírez, Waggoner, and Zha (<a href="references.html#ref-RubioRamirez_et_al_2010" role="doc-biblioref">2010</a>)</span>.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>A <strong>long-run restriction (LRR)</strong> prevents a structural shock from having a cumulative impact on one of the endogenous variables.</li>
</ol>
<ul>
<li>Additional computations are required to implement this. One needs to compute the cumulative effect of one of the structural shocks <span class="math inline">\(u_{t}\)</span> on one of the endogenous variable.</li>
<li>Examples: <span class="citation">Blanchard and Quah (<a href="references.html#ref-Blanchard_Quah_1989" role="doc-biblioref">1989</a>)</span>, <span class="citation">Faust and Leeper (<a href="references.html#ref-Faust_Leeper_1997" role="doc-biblioref">1997</a>)</span>, <span class="citation">Galí (<a href="references.html#ref-Gali_1999" role="doc-biblioref">1999</a>)</span>, <span class="citation">Erceg, Guerrieri, and Gust (<a href="references.html#ref-Erceg_et_al_2005" role="doc-biblioref">2005</a>)</span>, <span class="citation">Christiano, Eichenbaum, and Vigfusson (<a href="references.html#ref-NBERc11177" role="doc-biblioref">2007</a>)</span>.</li>
</ul>
<p>The two approaches can be combined (see, e.g., <span class="citation">Gerlach and Smets (<a href="references.html#ref-Gerlach_Smets_1995" role="doc-biblioref">1995</a>)</span>).</p>
<p>Let us consider a simple example that could motivate short-run restrictions. Consider the following stylized macro model:
<span class="math display" id="eq:systemI">\[\begin{equation}
\begin{array}{clll}
g_{t}&amp;=&amp; \bar{g}-\lambda(i_{t-1}-\mathbb{E}_{t-1}\pi_{t})+ \underbrace{{\color{blue}\sigma_d \eta_{d,t}}}_{\mbox{demand shock}}&amp; (\mbox{IS curve})\\
\Delta \pi_{t} &amp; = &amp; \beta (g_{t} - \bar{g})+ \underbrace{{\color{blue}\sigma_{\pi} \eta_{\pi,t}}}_{\mbox{cost push shock}} &amp; (\mbox{Phillips curve})\\
i_{t} &amp; = &amp; \rho i_{t-1} + \left[ \gamma_\pi \mathbb{E}_{t}\pi_{t+1}  + \gamma_g (g_{t} - \bar{g}) \right]\\
&amp;&amp; \qquad \qquad+\underbrace{{\color{blue}\sigma_{mp} \eta_{mp,t}}}_{\mbox{Mon. Pol. shock}} &amp; (\mbox{Taylor rule}),
\end{array}\tag{1.44}
\end{equation}\]</span>
where:
<span class="math display" id="eq:covU">\[\begin{equation}
\eta_t =
\left[
\begin{array}{c}
\eta_{\pi,t}\\
\eta_{d,t}\\
\eta_{mp,t}
\end{array}
\right]
\sim i.i.d.\,\mathcal{N}(0,I).\tag{1.45}
\end{equation}\]</span></p>
<p>Vector <span class="math inline">\(\eta_t\)</span> is assumed to be a vector of structural shocks, mutually and serially independent. On date <span class="math inline">\(t\)</span>:</p>
<ul>
<li>
<span class="math inline">\(g_t\)</span> is contemporaneously affected by <span class="math inline">\(\eta_{d,t}\)</span> only;</li>
<li>
<span class="math inline">\(\pi_t\)</span> is contemporaneously affected by <span class="math inline">\(\eta_{\pi,t}\)</span> and <span class="math inline">\(\eta_{d,t}\)</span>;</li>
<li>
<span class="math inline">\(i_t\)</span> is contemporaneously affected by <span class="math inline">\(\eta_{mp,t}\)</span>, <span class="math inline">\(\eta_{\pi,t}\)</span> and <span class="math inline">\(\eta_{d,t}\)</span>.</li>
</ul>
<p>System <a href="TS.html#eq:systemI">(1.44)</a> could be rewritten in the form:
<span class="math display" id="eq:BBBB">\[\begin{equation}
\left[\begin{array}{c}
d_t\\
\pi_t\\
i_t
\end{array}\right]
= \Phi(L)
\left[\begin{array}{c}
d_{t-1}\\
\pi_{t-1}\\
i_{t-1} +
\end{array}\right] +\underbrace{\underbrace{
\left[
\begin{array}{ccc}
0 &amp; \bullet &amp; 0 \\
\bullet &amp; \bullet &amp; 0 \\
\bullet &amp; \bullet &amp; \bullet
\end{array}
\right]}_{=B} \eta_t}_{=\varepsilon_t}\tag{1.46}
\end{equation}\]</span></p>
<p>This is the <strong>reduced-form</strong> of the model. This representation suggests three additional restrictions on the entries of <span class="math inline">\(B\)</span>; the latter matrix is therefore identified (up to the signs of its columns) as soon as <span class="math inline">\(\Omega = BB'\)</span> is known.</p>
<p>There are particular cases in which some well-known matrix decomposition of <span class="math inline">\(\Omega=\mathbb{V}ar(\varepsilon_t)\)</span> can be used to easily estimate some specific SVAR.</p>
<p>Consider the following context:</p>
<ul>
<li>A first shock (say, <span class="math inline">\(\eta_{n_1,t}\)</span>) can affect instantaneously
(i.e., on date <span class="math inline">\(t\)</span>) only one of the endogenous variable (say, <span class="math inline">\(y_{n_1,t}\)</span>);</li>
<li>A second shock (say, <span class="math inline">\(\eta_{n_2,t}\)</span>) can affect instantaneously
(i.e., on date <span class="math inline">\(t\)</span>) two endogenous variables, <span class="math inline">\(y_{n_1,t}\)</span> (the same as before) and <span class="math inline">\(y_{n_2,t}\)</span>;</li>
<li><span class="math inline">\(\dots\)</span></li>
</ul>
<p>This implies (1) that column <span class="math inline">\(n_1\)</span> of <span class="math inline">\(B\)</span> has only 1 non-zero entry (this is the <span class="math inline">\(n_1^{th}\)</span> entry), (2) that column <span class="math inline">\(n_2\)</span> of <span class="math inline">\(B\)</span> has 2 non-zero entries (the <span class="math inline">\(n_1^{th}\)</span> and the <span class="math inline">\(n_2^{th}\)</span> ones), etc. Without loss of generality, we can set <span class="math inline">\(n_1=n\)</span>, <span class="math inline">\(n_2=n-1\)</span>, etc. In this context, matrix <span class="math inline">\(B\)</span> is lower triangular.</p>
<p>The Cholesky decomposition of <span class="math inline">\(\Omega_{\varepsilon}\)</span> then provides an appropriate estimate of <span class="math inline">\(B\)</span>, since this matrix decomposition yields to a lower triangular matrix satisfying:
<span class="math display">\[
\Omega_\varepsilon = BB'.
\]</span></p>
<p>For instance, <span class="citation">Dedola and Lippi (<a href="references.html#ref-DEDOLA20051543" role="doc-biblioref">2005</a>)</span> estimate 5 structural VAR models for the US, the UK, Germany, France and Italy to analyse the monetary-policy transmission mechanisms. They estimate SVAR(5) models over the period 1975-1997. The shock-identification scheme is based on Cholesky decompositions, the ordering of the endogenous variables being: the industrial production, the consumer price index, a commodity price index, the short-term rate, monetary aggregate and the effective exchange rate (except for the US). This ordering implies that monetary policy reacts to the shocks affecting the first three variables but that the latter react to monetary policy shocks with a one-period lag only.</p>
<p>Importantly, the Cholesky approach can be useful when one is interested in one specific structural shock. This was the case, e.g., of <span class="citation">Christiano, Eichenbaum, and Evans (<a href="references.html#ref-Christiano_Eichenbaum_Evans_1996" role="doc-biblioref">1996</a>)</span>. Their identification is based on the following relationship between <span class="math inline">\(\varepsilon_t\)</span> and <span class="math inline">\(\eta_t\)</span>:
<span class="math display">\[
\left[\begin{array}{c}
\boldsymbol\varepsilon_{S,t}\\
\varepsilon_{r,t}\\
\boldsymbol\varepsilon_{F,t}
\end{array}\right] =
\left[\begin{array}{ccc}
B_{SS} &amp; 0 &amp; 0 \\
B_{rS} &amp; B_{rr} &amp; 0 \\
B_{FS} &amp; B_{Fr} &amp; B_{FF}
\end{array}\right]
\left[\begin{array}{c}
\boldsymbol\eta_{S,t}\\
\eta_{r,t}\\
\boldsymbol\eta_{F,t}
\end{array}\right],
\]</span>
where <span class="math inline">\(S\)</span>, <span class="math inline">\(r\)</span> and <span class="math inline">\(F\)</span> respectively correspond to <em>slow-moving variables</em>, the policy variable (short-term rate) and <em>fast-moving variables</em>. While <span class="math inline">\(\eta_{r,t}\)</span> is scalar, <span class="math inline">\(\boldsymbol\eta_{S,t}\)</span> and <span class="math inline">\(\boldsymbol\eta_{F,t}\)</span> may be vectors. The space spanned by <span class="math inline">\(\boldsymbol\varepsilon_{S,t}\)</span> is the same as that spanned by <span class="math inline">\(\boldsymbol\eta_{S,t}\)</span>. As a result, because <span class="math inline">\(\varepsilon_{r,t}\)</span> is a linear combination of <span class="math inline">\(\eta_{r,t}\)</span> and <span class="math inline">\(\boldsymbol\eta_{S,t}\)</span> (which are <span class="math inline">\(\perp\)</span>), it comes that the <span class="math inline">\(B_{rr}\eta_{r,t}\)</span>’s are the (population) residuals in the regression of <span class="math inline">\(\varepsilon_{r,t}\)</span> on <span class="math inline">\(\boldsymbol\varepsilon_{S,t}\)</span>. Because <span class="math inline">\(\mathbb{V}ar(\eta_{r,t})=1\)</span>, <span class="math inline">\(B_{rr}\)</span> is given by the square root of the variance of <span class="math inline">\(B_{rr}\eta_{r,t}\)</span>. <span class="math inline">\(B_{F,r}\)</span> is finally obtained by regressing the components of <span class="math inline">\(\boldsymbol\varepsilon_{F,t}\)</span> on the estimates of <span class="math inline">\(\eta_{r,t}\)</span>.</p>
<p>An equivalent approach consists in computing the Cholesky decomposition of <span class="math inline">\(BB'\)</span> and the contemporaneous impacts of the monetary policy shock (on the <span class="math inline">\(n\)</span> endogenous variables) are the components of the column of <span class="math inline">\(B\)</span> corresponding to the policy variable.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.pfaffikus.de">vars</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"USmonthly"</span><span class="op">)</span></span>
<span><span class="co"># Select sample period:</span></span>
<span><span class="va">First.date</span> <span class="op">&lt;-</span> <span class="st">"1965-01-01"</span>;<span class="va">Last.date</span> <span class="op">&lt;-</span> <span class="st">"1995-06-01"</span></span>
<span><span class="va">indic.first</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">USmonthly</span><span class="op">$</span><span class="va">DATES</span><span class="op">==</span><span class="va">First.date</span><span class="op">)</span></span>
<span><span class="va">indic.last</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">USmonthly</span><span class="op">$</span><span class="va">DATES</span><span class="op">==</span><span class="va">Last.date</span><span class="op">)</span></span>
<span><span class="va">USmonthly</span>   <span class="op">&lt;-</span> <span class="va">USmonthly</span><span class="op">[</span><span class="va">indic.first</span><span class="op">:</span><span class="va">indic.last</span>,<span class="op">]</span></span>
<span><span class="va">considered.variables</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"LIP"</span>,<span class="st">"UNEMP"</span>,<span class="st">"LCPI"</span>,<span class="st">"LPCOM"</span>,<span class="st">"FFR"</span>,<span class="st">"NBR"</span>,<span class="st">"TTR"</span>,<span class="st">"M1"</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">USmonthly</span><span class="op">[</span><span class="va">considered.variables</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">res.svar.ordering</span> <span class="op">&lt;-</span> <span class="fu">svar.ordering</span><span class="op">(</span><span class="va">y</span>,p<span class="op">=</span><span class="fl">3</span>,</span>
<span>                                   posit.of.shock <span class="op">=</span> <span class="fl">5</span>,</span>
<span>                                   nb.periods.IRF <span class="op">=</span> <span class="fl">20</span>,</span>
<span>                                   nb.bootstrap.replications <span class="op">=</span> <span class="fl">100</span>,</span>
<span>                                   confidence.interval <span class="op">=</span> <span class="fl">0.90</span>, <span class="co"># expressed in pp.</span></span>
<span>                                   indic.plot <span class="op">=</span> <span class="fl">1</span> <span class="co"># Plots are displayed if = 1.</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:CEE"></span>
<img src="TimeSeries_files/figure-html/CEE-1.png" alt="Response to a monetary-policy shock. Identification approach of Christiano, Eichenbaum and Evans (1996). Confidence intervals are obtained by boostrapping the estimated VAR model (see inference section)." width="95%"><p class="caption">
Figure 1.15: Response to a monetary-policy shock. Identification approach of Christiano, Eichenbaum and Evans (1996). Confidence intervals are obtained by boostrapping the estimated VAR model (see inference section).
</p>
</div>
<p>Let us now turn to <strong>Long-run restrictions</strong>. Such a restriction concerns the long-run influence of a shock on an endogenous variable. Let us consider for instance a structural shock that is assumed to have no “long-run influence” on GDP. How to express this? The long-run change in GDP can be expressed as <span class="math inline">\(GDP_{t+h} - GDP_t\)</span>, with <span class="math inline">\(h\)</span> large. Note further that:
<span class="math display">\[
GDP_{t+h} - GDP_t = \Delta GDP_{t+h} +\Delta GDP_{t+h-1} + \dots + \Delta GDP_{t+1}.
\]</span>
Hence, the fact that a given structural shock (<span class="math inline">\(\eta_{i,t}\)</span>, say) has no long-run influence on GDP means that
<span class="math display">\[
\lim_{h\rightarrow\infty}\frac{\partial GDP_{t+h}}{\partial \eta_{i,t}} = \lim_{h\rightarrow\infty} \frac{\partial}{\partial \eta_{i,t}}\left(\sum_{k=1}^h \Delta  GDP_{t+k}\right)= 0.
\]</span></p>
<p>This can be easily formulated as a function of <span class="math inline">\(B\)</span> and of the matrices <span class="math inline">\(\Phi_i\)</span> when <span class="math inline">\(y_t\)</span> (including <span class="math inline">\(\Delta GDP_t\)</span>) follows a VAR process.</p>
<p>Without loss of generality, we will only consider the VAR(1) case. Indeed, one can always write a VAR(<span class="math inline">\(p\)</span>) as a VAR(1). To see that, stack the last <span class="math inline">\(p\)</span> values of vector <span class="math inline">\(y_t\)</span> in vector <span class="math inline">\(y_{t}^{*}=[y_t',\dots,y_{t-p+1}']'\)</span>; Eq. <a href="TS.html#eq:yVAR">(1.32)</a> can then be rewritten in its <strong>companion form</strong>:
<span class="math display" id="eq:ystarVAR">\[\begin{equation}
y_{t}^{*} =
\underbrace{\left[\begin{array}{c}
c\\
0\\
\vdots\\
0\end{array}\right]}_{=c^*}+
\underbrace{\left[\begin{array}{cccc}
\Phi_{1} &amp; \Phi_{2} &amp; \cdots &amp; \Phi_{p}\\
I &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; \ddots &amp; 0 &amp; 0\\
0 &amp; 0 &amp; I &amp; 0\end{array}\right]}_{=\Phi}
y_{t-1}^{*}+
\underbrace{\left[\begin{array}{c}
\varepsilon_{t}\\
0\\
\vdots\\
0\end{array}\right]}_{\varepsilon_t^*},\tag{1.47}
\end{equation}\]</span>
where matrices <span class="math inline">\(\Phi\)</span> and <span class="math inline">\(\Omega^* = \mathbb{V}ar(\varepsilon_t^*)\)</span> are of dimension <span class="math inline">\(np \times np\)</span>; <span class="math inline">\(\Omega^*\)</span> is filled with zeros, except the <span class="math inline">\(n\times n\)</span> upper-left block that is equal to <span class="math inline">\(\Omega = \mathbb{V}ar(\varepsilon_t)\)</span>. (Matrix <span class="math inline">\(\Phi\)</span> had been introduced in Eq. <a href="TS.html#eq:matrixPHI">(1.38)</a>.)</p>
<p>Focusing on the VAR(1) case:
<span class="math display">\[\begin{eqnarray*}
y_{t} &amp;=&amp; c+\Phi y_{t-1}+\varepsilon_{t}\\
&amp; = &amp; c+\varepsilon_{t}+\Phi(c+\varepsilon_{t-1})+\ldots+\Phi^{k}(c+\varepsilon_{t-k})+\ldots \\
&amp; = &amp; \mu +\varepsilon_{t}+\Phi\varepsilon_{t-1}+\ldots+\Phi^{k}\varepsilon_{t-k}+\ldots \\
&amp; = &amp; \mu +B\eta_{t}+\Phi B\eta_{t-1}+\ldots+\Phi^{k}B\eta_{t-k}+\ldots,
\end{eqnarray*}\]</span></p>
<p>The sequence of shocks <span class="math inline">\(\{\eta_t\}\)</span> determines the sequence <span class="math inline">\(\{y_t\}\)</span>. What if <span class="math inline">\(\{\eta_t\}\)</span> is replaced with <span class="math inline">\(\{\tilde{\eta}_t\}\)</span>, where <span class="math inline">\(\tilde{\eta}_t=\eta_t\)</span> if <span class="math inline">\(t \ne s\)</span> and <span class="math inline">\(\tilde{\eta}_s=\eta_s + \gamma\)</span>? Assume <span class="math inline">\(\{\tilde{y}_t\}\)</span> is the associated “perturbated” sequence. We have <span class="math inline">\(\tilde{y}_t = y_t\)</span> if <span class="math inline">\(t&lt;s\)</span>. For <span class="math inline">\(t \ge s\)</span>, the Wold decomposition of <span class="math inline">\(\{\tilde{y}_t\}\)</span> implies:
<span class="math display">\[
\tilde{y}_t = y_t + \Phi^{t-s} B \gamma.
\]</span>
Therefore, the cumulative impact of <span class="math inline">\(\gamma\)</span> on <span class="math inline">\(\tilde{y}_t\)</span> will be (for <span class="math inline">\(t \ge s\)</span>):
<span class="math display" id="eq:cumul">\[\begin{eqnarray}
(\tilde{y}_t - y_t) +  (\tilde{y}_{t-1} - y_{t-1}) + \dots +  (\tilde{y}_s - y_s) &amp;=&amp; \nonumber \\
(Id + \Phi + \Phi^2 + \dots + \Phi^{t-s}) B \gamma.&amp;&amp; \tag{1.48}
\end{eqnarray}\]</span></p>
<p>Consider a shock on <span class="math inline">\(\eta_{1,t}\)</span>, with a magnitude of <span class="math inline">\(1\)</span>. This shock corresponds to <span class="math inline">\(\gamma = [1,0,\dots,0]'\)</span>. Given Eq. <a href="TS.html#eq:cumul">(1.48)</a>, the long-run cumulative effect of this shock on the endogenous variables is given by:
<span class="math display">\[
\underbrace{(Id+\Phi+\ldots+\Phi^{k}+\ldots)}_{=(Id - \Phi)^{-1}}B\left[\begin{array}{c}
1\\
0\\
\vdots\\
0\end{array}\right],
\]</span>
that is the first column of <span class="math inline">\(\Theta \equiv (Id - \Phi)^{-1}B\)</span>.</p>
<p>In this context, consider the following long-run restriction: <em>“<span class="math inline">\(j^{th}\)</span> structural shock has no cumulative impact on the <span class="math inline">\(i^{th}\)</span> endogenous variable”</em>. It is equivalent to
<span class="math display">\[
\Theta_{ij}=0,
\]</span>
where <span class="math inline">\(\Theta_{ij}\)</span> is the element <span class="math inline">\((i,j)\)</span> of <span class="math inline">\(\Theta\)</span>.</p>
<p><span class="citation">Blanchard and Quah (<a href="references.html#ref-Blanchard_Quah_1989" role="doc-biblioref">1989</a>)</span> have implemented such long-run restrictions in a small-scale VAR. Two variables are considered: GDP and unemployment. Consequently, the VAR is affected by two types of shocks. Specifically, authors want to identify <strong>supply shocks</strong> (that can have a permanent effect on output) and <strong>demand shocks</strong> (that cannot have a permanent effect on output).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The motivation of the authors regarding their long-run restrictions can be obtained from a traditional Keynesian view of fluctuations. The authors propose a variant of a model from &lt;span class=&quot;citation&quot;&gt;Fischer (&lt;a href=&quot;references.html#ref-Fischer_1977&quot; role=&quot;doc-biblioref&quot;&gt;1977&lt;/a&gt;)&lt;/span&gt;.
<!-- \begin{eqnarray} -->
<!-- Y_{t} & = & M_{t}-P_{t}+a.\theta_{t}(\#eq:demand)\\ -->
<!-- Y_{t} & = & N_{t}+\theta_{t}(\#eq:prodfunct)\\ -->
<!-- P_{t} & = & W_{t}-\theta_{t}(\#eq:PS)\\ -->
<!-- W_{t} & = & W\mid\left\{ \mathbb{E}_{t-1}N_{t}=\overline{N}\right\}. (\#eq:WS) -->
<!-- \end{eqnarray} -->
<!-- To close the model, the authors assume the following dynamics for the money supply and the productivity: -->
<!-- \begin{eqnarray*} -->
<!-- M_{t} & = & M_{t-1}+\varepsilon_{t}^{d}\\ -->
<!-- \theta_{t} & = & \theta_{t-1}+\varepsilon_{t}^{s}. -->
<!-- \end{eqnarray*} -->
<!-- In this context, it can be shown that -->
<!-- \begin{eqnarray*} -->
<!-- \Delta Y_{t} & = & (\varepsilon_{t}^{d}-\varepsilon_{t-1}^{d})+a.(\varepsilon_{t}^{s}-\varepsilon_{t-1}^{s})+\varepsilon_{t}^{s}\\ -->
<!-- u_{t} & = & -\varepsilon_{t}^{d}-a\varepsilon_{t}^{s} -->
<!-- \end{eqnarray*} -->
<!-- Then, it appears that the demand shocks have no long-run cumulative impact on $\Delta Y_{t}$, the GDP growth, i.e. no long-term impact on output $Y_t$. The vector of endogenous variables is $y_t = [\Delta Y_{t} \quad u_{t}]'$ where $\Delta Y_{t}$ denotes the GDP growth. -->&lt;/p&gt;"><sup>2</sup></a></p>
<p><span class="citation">Blanchard and Quah (<a href="references.html#ref-Blanchard_Quah_1989" role="doc-biblioref">1989</a>)</span>’s dataset is quarterly, spanning the period from 1950:2 to 1987:4. Their VAR features 8 lags. Here are the data they use:</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">BQ</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span><span class="op">(</span><span class="va">BQ</span><span class="op">$</span><span class="va">Date</span>,<span class="va">BQ</span><span class="op">$</span><span class="va">Dgdp</span>,type<span class="op">=</span><span class="st">"l"</span>,main<span class="op">=</span><span class="st">"GDP quarterly growth rate"</span>,</span>
<span>     xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span><span class="op">(</span><span class="va">BQ</span><span class="op">$</span><span class="va">Date</span>,<span class="va">BQ</span><span class="op">$</span><span class="va">unemp</span>,type<span class="op">=</span><span class="st">"l"</span>,ylim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3</span>,<span class="fl">6</span><span class="op">)</span>,main<span class="op">=</span><span class="st">"Unemployment rate (gap)"</span>,</span>
<span>     xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="TimeSeries_files/figure-html/BQ1-1.png" width="672"></div>
<p>Estimate a reduced-form VAR(8) model:</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.pfaffikus.de">vars</a></span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">BQ</span><span class="op">[</span>,<span class="fl">2</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span></span>
<span><span class="va">est.VAR</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/vars/man/VAR.html">VAR</a></span><span class="op">(</span><span class="va">y</span>,p<span class="op">=</span><span class="fl">8</span><span class="op">)</span></span>
<span><span class="va">Omega</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">est.VAR</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Now, let us define a loss function (<code>loss</code>) that is equal to zero if (a) <span class="math inline">\(BB'=\Omega\)</span> and (b) the element (1,1) of <span class="math inline">\(\Theta B\)</span> is equal to zero:</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute (Id - Phi)^{-1}:</span></span>
<span><span class="va">Phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/vars/man/A.html">Acoef</a></span><span class="op">(</span><span class="va">est.VAR</span><span class="op">)</span></span>
<span><span class="va">PHI</span> <span class="op">&lt;-</span> <span class="fu">make.PHI</span><span class="op">(</span><span class="va">Phi</span><span class="op">)</span></span>
<span><span class="va">sum.PHI.k</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">PHI</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">-</span> <span class="va">PHI</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span></span>
<span><span class="va">loss</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">param</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">B</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">param</span>,<span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="va">X</span> <span class="op">&lt;-</span> <span class="va">Omega</span> <span class="op">-</span> <span class="va">B</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">B</span><span class="op">)</span></span>
<span>  <span class="va">Theta</span> <span class="op">&lt;-</span> <span class="va">sum.PHI.k</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">B</span></span>
<span>  <span class="va">loss</span> <span class="op">&lt;-</span> <span class="fl">10000</span> <span class="op">*</span> <span class="op">(</span> <span class="va">X</span><span class="op">[</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">]</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">X</span><span class="op">[</span><span class="fl">2</span>,<span class="fl">1</span><span class="op">]</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">X</span><span class="op">[</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">]</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">Theta</span><span class="op">[</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">]</span><span class="op">^</span><span class="fl">2</span> <span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">loss</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">res.opt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span>,<span class="va">loss</span>,method<span class="op">=</span><span class="st">"BFGS"</span>,hessian<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">res.opt</span><span class="op">$</span><span class="va">par</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1]  0.8570358 -0.2396345  0.1541395  0.1921221</code></pre>
<p>(Note: one can use that type of approach, based on a loss function, to mix short- and long-run restrictions.)</p>
<p>Figure <a href="TS.html#fig:BQ4">1.16</a> displays the resulting IRFs. Note that, for GDP, we cumulate the GDP growth IRF, so as to have the response of the GDP in level.</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">B.hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">res.opt</span><span class="op">$</span><span class="va">par</span>,<span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">Omega</span>,<span class="va">B.hat</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">B.hat</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>##             Dgdp       unemp                       
## Dgdp   0.7582704 -0.17576173  0.7582694 -0.17576173
## unemp -0.1757617  0.09433658 -0.1757617  0.09433558</code></pre>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nb.sim</span> <span class="op">&lt;-</span> <span class="fl">40</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.15</span>,<span class="fl">.95</span>,<span class="fl">.15</span>,<span class="fl">.8</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu">simul.VAR</span><span class="op">(</span>c<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">2</span>,<span class="fl">1</span><span class="op">)</span>,<span class="va">Phi</span>,<span class="va">B.hat</span>,<span class="va">nb.sim</span>,y0.star<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">2</span><span class="op">*</span><span class="fl">8</span><span class="op">)</span>,</span>
<span>               indic.IRF <span class="op">=</span> <span class="fl">1</span>,u.shock <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">Y</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span><span class="op">)</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,main<span class="op">=</span><span class="st">"Demand shock on GDP"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span><span class="op">(</span><span class="va">Y</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,main<span class="op">=</span><span class="st">"Demand shock on UNEMP"</span><span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu">simul.VAR</span><span class="op">(</span>c<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">2</span>,<span class="fl">1</span><span class="op">)</span>,<span class="va">Phi</span>,<span class="va">B.hat</span>,<span class="va">nb.sim</span>,y0.star<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">2</span><span class="op">*</span><span class="fl">8</span><span class="op">)</span>,</span>
<span>               indic.IRF <span class="op">=</span> <span class="fl">1</span>,u.shock <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">Y</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span><span class="op">)</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,main<span class="op">=</span><span class="st">"Supply shock on GDP"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span><span class="op">(</span><span class="va">Y</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,main<span class="op">=</span><span class="st">"Supply shock on UNEMP"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:BQ4"></span>
<img src="TimeSeries_files/figure-html/BQ4-1.png" alt="IRF of GDP and unemployment to demand and supply shocks." width="95%"><p class="caption">
Figure 1.16: IRF of GDP and unemployment to demand and supply shocks.
</p>
</div>
</div>
<div id="Signs" class="section level3" number="1.3.7">
<h3>
<span class="header-section-number">1.3.7</span> Sign restrictions<a class="anchor" aria-label="anchor" href="#Signs"><i class="fas fa-link"></i></a>
</h3>
<p>To identifiy the structural shocks, we need to find a matrix <span class="math inline">\(B\)</span> that satisfies <span class="math inline">\(\Omega = BB'\)</span> (with <span class="math inline">\(\Omega = \mathbb{V}ar(\varepsilon_t)\)</span>) and other restrictions. Indeed, as explained above, <span class="math inline">\(\Omega = BB'\)</span> is not sufficient to identify <span class="math inline">\(B\)</span> since, if we take any orthogonal matrix <span class="math inline">\(Q\)</span> (see Def. <a href="TS.html#def:orthogonal">1.23</a>), then <span class="math inline">\(\mathcal{P}=BQ\)</span> also satisfies <span class="math inline">\(\Omega = \mathcal{P}\mathcal{P}'\)</span>.</p>
<div class="definition">
<p><span id="def:orthogonal" class="definition"><strong>Definition 1.23  (Orthogonal matrix) </strong></span>An orthogonal matrix <span class="math inline">\(Q\)</span> is a matrix such that <span class="math inline">\(QQ' = I,\)</span> i.e., all columns (rows) of <span class="math inline">\(Q\)</span> are are
orthogonal and unit vectors:
<span class="math display">\[q_i'q_j=0\text{ if }i\neq j\text{ and }q_i'q_j=1\text{ if }i= j,\]</span>
where <span class="math inline">\(q_i\)</span> is the <span class="math inline">\(i^{th}\)</span> column of <span class="math inline">\(Q\)</span>.</p>
</div>
<p>The idea behind the sign-restriction approach is to “draw” random matrices <span class="math inline">\(\mathcal{P}\)</span> that satisfy <span class="math inline">\(\Omega = \mathcal{P}\mathcal{P}'\)</span>, and then to constitute a set of admissible matrices, keeping in this set only the simulated <span class="math inline">\(\mathcal{P}\)</span> matrices that satisfy some predefined sign-based restriction. An example of restriction is “<em>after one year, a contractionary monetary-policy shocks has a negative impact on inflation</em>”.</p>
<p>As suggested above, if <span class="math inline">\(B\)</span> is any matrix that satisfies <span class="math inline">\(\Omega = BB'\)</span> (for instance, <span class="math inline">\(B\)</span> can be based on the Cholesky decomposition of <span class="math inline">\(\Omega\)</span>), then we also have <span class="math inline">\(\Omega = \mathcal{P}\mathcal{P}'\)</span> as soon as <span class="math inline">\(\mathcal{P}=BQ\)</span>, where <span class="math inline">\(Q\)</span> is an orthogonal matrix. Therefore, to draw <span class="math inline">\(\mathcal{P}\)</span> matrices, it suffices to draw in the set of orthogonal matrices.</p>
<p>To fix ideas, consider dimension 2. In that case, the orthogonal matrices are rotation matrices, and the set of orthogonal matrices can be parameterized by the angle <span class="math inline">\(x\)</span>, with:
<span class="math display">\[
Q_x=\begin{pmatrix}\cos(x)&amp;\cos\left(x+\frac{\pi}{2}\right)\\
\sin(x)&amp;\sin\left(x+\frac{\pi}{2}\right)\end{pmatrix}=\begin{pmatrix}\cos(x)&amp;-\sin(x)\\
\sin(x)&amp;\cos(x)\end{pmatrix}.
\]</span>
(This is an angle-<span class="math inline">\(x\)</span> counter-clockwise rotation.) Hence, in that case, by drawing <span class="math inline">\(x\)</span> randomly from <span class="math inline">\([0,2\pi]\)</span>, we draw randomly from the set of <span class="math inline">\(2\times2\)</span> rotation matrices. For high-dimensional VAR, we lose this simple geometrical representation, though. It is not always possible to parametrize a rotation matrix (high-dimentional VARs).</p>
<p>How to proceed, then? <span class="citation">Arias, Rubio-Ramírez, and Waggoner (<a href="references.html#ref-Arias_et_al_2018" role="doc-biblioref">2018</a>)</span> provide a procedure. Their approach is based on the so-called <span class="math inline">\(QR\)</span> decomposition: any square matrix <span class="math inline">\(X\)</span> may be decomposed as <span class="math inline">\(X=QR\)</span> where <span class="math inline">\(Q\)</span> is an orthogonal matrix and <span class="math inline">\(R\)</span> is an upper diagonal matrix. With this in mind, they propose a two-step approach:</p>
<ol style="list-style-type: lower-roman">
<li>Draw a random matrix <span class="math inline">\(X\)</span> by drawing each element from independent standard normal distribution.</li>
<li>Let <span class="math inline">\(X = QR\)</span> be the <span class="math inline">\(QR\)</span> decomposition of <span class="math inline">\(X\)</span> with the diagonal of <span class="math inline">\(R\)</span> normalized to be
positive. The random matrix <span class="math inline">\(Q\)</span> is orthogonal and is a draw from the uniform distribution over the set of orthogonal matrices.</li>
</ol>
<p>Equipped with this procedure, the sign-restriction is based on the following algorithm:</p>
<ol style="list-style-type: decimal">
<li>Draw a random orthogonal matrix <span class="math inline">\(Q\)</span> (using step i. and ii. described above).</li>
<li>Compute <span class="math inline">\(B = PQ\)</span> where <span class="math inline">\(P\)</span> is the Cholesky decomposition of the reduced form residuals <span class="math inline">\(\Omega_{\varepsilon}\)</span>.</li>
<li>Compute the impulse response associated with <span class="math inline">\(B\)</span> <span class="math inline">\(y_{t,t+k}=\Phi^kB\)</span> or the cumulated response <span class="math inline">\(\bar y_{t,t+k}=\sum_{j=0}^{k}\Phi^jB\)</span>.</li>
<li>Are the sign restrictions satisfied?</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>
<strong>Yes</strong>. Store the impulse response in the set of admissible response.</li>
<li>
<strong>No</strong>. Discard the impulse response.</li>
</ol>
<ol start="5" style="list-style-type: decimal">
<li>Perform <span class="math inline">\(N\)</span> replications and report the median impulse response (and its “confidence” intervals).</li>
</ol>
<p>Note: to take into account the uncertainty in <span class="math inline">\(B\)</span> and <span class="math inline">\(\Phi\)</span>, you can draw <span class="math inline">\(B\)</span> and <span class="math inline">\(\Phi\)</span> in Steps 2 and 3 using an inference method (see Section <a href="TS.html#Inference">1.3.12</a>).</p>
<p>The sign-restriction approach method has the advantage of being relatively agnostic. Moreover, it is fairly flexible, as one can impose sign restrictions on any variable, at any horizon. A prominent example is <span class="citation">Uhlig (<a href="references.html#ref-Uhlig_2005" role="doc-biblioref">2005</a>)</span>. Using US monthly data from 1965.I to 2003.XII, he employs sign restrictions to estimate the effect of monetary policy shocks.</p>
<p>According to conventional wisdom, monetary contractions should:<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Standard identification schemes often fail to achieve these 4 points Two puzzles regularly arise: &lt;em&gt;liquidity puzzle&lt;/em&gt;: when identifying monetary policy shocks as surprise increases in the stock of money, interest rates tend to go up, not down; &lt;em&gt;price puzzle&lt;/em&gt;: after a contractionary monetary policy shock, even with interest rates going up and money supply going down, inflation goes up rather than down.&lt;/p&gt;"><sup>3</sup></a></p>
<ul>
<li>Raise the federal funds rate,</li>
<li>Lower prices,</li>
<li>Decrease non-borrowed reserves,</li>
<li>Reduce real output.</li>
</ul>
<p>The restricitons considered by <span class="citation">Uhlig (<a href="references.html#ref-Uhlig_2005" role="doc-biblioref">2005</a>)</span> are as follows: an expansionary monetary policy shock leads to:</p>
<ul>
<li>Increases in prices</li>
<li>Increase in nonborrowed reserves</li>
<li>Decreases in the federal funds rate</li>
</ul>
<p>What about output? Since is the response of interest, we leave it un-restricted.</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span>;<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.pfaffikus.de">vars</a></span><span class="op">)</span>;<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://Matrix.R-forge.R-project.org/">Matrix</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"USmonthly"</span><span class="op">)</span></span>
<span><span class="va">First.date</span> <span class="op">&lt;-</span> <span class="st">"1965-01-01"</span></span>
<span><span class="va">Last.date</span> <span class="op">&lt;-</span> <span class="st">"1995-06-01"</span></span>
<span><span class="va">indic.first</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">USmonthly</span><span class="op">$</span><span class="va">DATES</span><span class="op">==</span><span class="va">First.date</span><span class="op">)</span></span>
<span><span class="va">indic.last</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">USmonthly</span><span class="op">$</span><span class="va">DATES</span><span class="op">==</span><span class="va">Last.date</span><span class="op">)</span></span>
<span><span class="va">USmonthly</span>   <span class="op">&lt;-</span> <span class="va">USmonthly</span><span class="op">[</span><span class="va">indic.first</span><span class="op">:</span><span class="va">indic.last</span>,<span class="op">]</span></span>
<span><span class="va">considered.variables</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"LIP"</span>,<span class="st">"UNEMP"</span>,<span class="st">"LCPI"</span>,<span class="st">"LPCOM"</span>,<span class="st">"FFR"</span>,<span class="st">"NBR"</span>,<span class="st">"TTR"</span>,<span class="st">"M1"</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">considered.variables</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">USmonthly</span><span class="op">[</span><span class="va">considered.variables</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">sign.restrictions</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">horizon</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#Define sign restrictions and horizon for restrictions</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">sign.restrictions</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">0</span>,<span class="va">n</span>,<span class="va">n</span><span class="op">)</span></span>
<span>  <span class="va">horizon</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="op">}</span></span>
<span><span class="va">sign.restrictions</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">[</span><span class="fl">1</span>,<span class="fl">3</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">sign.restrictions</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">[</span><span class="fl">2</span>,<span class="fl">5</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fl">1</span></span>
<span><span class="va">sign.restrictions</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">[</span><span class="fl">3</span>,<span class="fl">6</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">horizon</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">5</span></span>
<span><span class="va">res.svar.signs</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">svar.signs</span><span class="op">(</span><span class="va">y</span>,p<span class="op">=</span><span class="fl">3</span>,</span>
<span>             nb.shocks <span class="op">=</span> <span class="fl">1</span>, <span class="co">#number of identified shocks</span></span>
<span>             nb.periods.IRF <span class="op">=</span> <span class="fl">20</span>,</span>
<span>             bootstrap.replications <span class="op">=</span> <span class="fl">1</span>, <span class="co"># = 0 if no bootstrap</span></span>
<span>             confidence.interval <span class="op">=</span> <span class="fl">0.80</span>, <span class="co"># expressed in pp.</span></span>
<span>             indic.plot <span class="op">=</span> <span class="fl">1</span>, <span class="co"># Plots are displayed if = 1.</span></span>
<span>             nb.draws <span class="op">=</span> <span class="fl">10000</span>, <span class="co"># number of draws</span></span>
<span>             <span class="va">sign.restrictions</span>,</span>
<span>             <span class="va">horizon</span>,</span>
<span>             recursive <span class="op">=</span><span class="fl">1</span> <span class="co">#  =0 &lt;- draw Q directly, =1 &lt;- draw q recursively</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:signrestr1"></span>
<img src="TimeSeries_files/figure-html/signrestr1-1.png" alt="IRF associated with a monetary policy shock; sign-restriction approach." width="95%"><p class="caption">
Figure 1.17: IRF associated with a monetary policy shock; sign-restriction approach.
</p>
</div>
<!-- Another approach -->
<!-- If $B$ satisfies $BB'=\Omega_\varepsilon$, the vectors of $B$ are called **impulse vectors** -->
<!-- Let $b$ be en impulse vector, there exists a unit-length vector $q$ (i.e. $q'q=1$) so that \[b=Pq\] -->
<!-- $b$ describes the response of the variables to a shock that is a linear combination (with weights given by $q$) of the shocks associated to the Cholesky decomposition. It's a "candidate shock" for the sign restrictions. -->
<!-- Note that $b$ is associated to a unique unit-length vector $q$. To span the -->
<!-- potential $b$s, it is enough to span the $q$s. -->
<!-- Let $\psi_k^i$ be the vector response at horizon $k$ to the $i^{th}$ shock in the -->
<!-- Cholesky decomposition of $\Omega_\varepsilon$. The $\psi_k^i$ are the columns of $\Psi_k$, defined as follows: -->
<!-- \[\Psi_k=\Phi^kP.\] -->
<!-- The impulse response $\psi_k(q)$ associated to $q$ is then given by -->
<!-- \[\psi_k(q)=\sum_{i=1}^m q_i\psi_k^i=\Psi_kq\] -->
<!-- 1. Compute $\Psi_k=\Phi^kP$ (or $\sum_{j=0}^k\Psi_k$) using $P$, -->
<!-- the Cholesky decomposition of $\Omega_\varepsilon$. -->
<!-- 2. Draw a random orthogonal vector $q$ -->
<!-- 3. Compute the impulse response associated with $q$ \[ y_{t,t+k}=\Psi_kq\] or the cumulated response \[\bar y_{t,t+k}=\left(\sum_{j=0}^k\Psi_k\right)q\] -->
<!-- 4. Are the sign restrictions satisfied? -->
<!-- a. **Yes**. Store the impulse response -->
<!-- b. **No**. Discard the impulse response -->
<!-- 5. Perform N times (starting from 2.) and report the median impulse response (and its confidence intervals) -->
<!-- How to draw a unit-length vector? -->
<!-- 1. Draw a vector $u$ from the standard normal distribution on -->
<!-- $\mathbb{R}^m$. -->
<!-- 2. Compute $q = u/||u||$. -->
<!-- What if we identify $l>1$ shocks? We need to draw $q_1$,...,$q_l$ that are orthogonal. -->
<!-- * For $1\le j\le l$, draw $u_j\in \mathbb{R}^{m+1-j}$ from a standard normal distribution and set $w_j = u_j/||u_j||$.  -->
<!-- * Define $\begin{pmatrix}q_1&...&q_l\end{pmatrix}$ recursively by $q_j = K_jw_j$ for any matrix $K_j$ whose columns form an orthogonal basis for the null space of the matrix \[M_j = \begin{pmatrix} q_1&...&q_{j-1}\end{pmatrix}'\] ($q_j$ will be orthogonal to $\begin{pmatrix} q_1&...&q_{j-1}\end{pmatrix}$) -->
<p>It has to be stressed that the sign restriction approach does not lead to a unique IRF, but to a set of admissible IRFs. Also, we say that this approach is set-identified, not point-identified.</p>
<p>An alternative approach is the so-called <strong>penalty-function approach</strong> (PFA, <span class="citation">Uhlig (<a href="references.html#ref-Uhlig_2005" role="doc-biblioref">2005</a>)</span>, present in <span class="citation">Danne (<a href="references.html#ref-Danne_2015" role="doc-biblioref">2015</a>)</span>’s package). This approach relies on a <em>penalty function</em>:
<span class="math display">\[
\begin{array}{llll}f(x)&amp;=&amp;x&amp;\text{ if }x\le0\\
&amp;&amp;100.x&amp;\text{ if }x&gt;0\end{array}
\]</span>
which penalizes positive responses and rewards negative responses.</p>
<p>Let <span class="math inline">\(\psi_k^j(q)\)</span> be the impulse response of variable <span class="math inline">\(j\)</span>. The <span class="math inline">\(\psi_k^j(q)\)</span>’s are the elements of <span class="math inline">\(\psi_k(q)=\Psi_kq\)</span>.</p>
<p>Let <span class="math inline">\(\sigma_j\)</span> be the standard deviation of variable <span class="math inline">\(j\)</span>. Let <span class="math inline">\(\iota_{j,k}=1\)</span> if we restrict the response of variable <span class="math inline">\(j\)</span> at the <span class="math inline">\(k^th\)</span> horizon to be negative, <span class="math inline">\(\iota_{j,k}=-1\)</span> if we restrict it to be positive, and <span class="math inline">\(\iota_{j,k}=0\)</span> if there is no restriction. The total penalty is given by <span class="math display">\[
\mathbf{P}(q)=\sum_{j=1}^m\sum_{k=0}^Kf\left(\iota_{j,k}\frac{\psi_k^j(q)}{\sigma_j}\right).
\]</span></p>
<p>We are looking for a solution to
<span class="math display">\[\begin{array}{ll}&amp;\min_q \mathbf{P}(q)\\
&amp;\\
\text{s.t. }&amp;q'q=1.\end{array}\]</span></p>
<p>The problem is solved numerically.</p>
<!-- Sometimes we need to combine different restriction approaches. For instance: -->
<!-- * One shock satisfies both zero and sign restrictions. -->
<!-- * Some shocks can be identified with zero restrictions (SR or LR), others with sign restrictions. -->
<!-- * Some shocks satisfy the same zero restrictions (e.g. no LR effect on output) but can be distinguished from each other through sign restrictions. -->
<!-- In such instances, we must make independent draws from the set of all structural parameters satisfying the zero restrictions. How to do that?  -->
<!-- @Arias_et_al_2018 propose to impose the zero restrictions on $B$, and then check signs. Remember, $\mathcal{P}=BQ$ is a candidate impact IRF. For each structural shock $j$, define the $m$-column matrices $Z_j$ (zero restrictions) and $S_j$ (sign restrictions). -->
<!-- Each row of $Z_j$ (resp. $S_j$) defines a zero (resp. sign) restriction.  -->
<!-- $Z_j$ has $m-j$ rows at most (i.e. $m-j$ zero restriction at most). -->
<!-- Example: In a 4-variable VAR, we want to impose that the first structural shock has no effect on variable 1, affects positively variable 2 and negatively variable 3 on impact: -->
<!-- \[Z_1 = \begin{pmatrix}1 & 0 & 0 & 0\end{pmatrix} \] -->
<!-- \[S_1 = \begin{pmatrix}0 & 1 & 0 & 0\\ -->
<!-- 0 & 0 & -1 & 0\end{pmatrix} \] -->
<!-- For both zero and sign restrictions to be satisfied, we must have that \[Z_jb_j=0\] \[S_jb_j>0\] where $b_j$ is the $j^{th}$ column of $B$, i.e. the impact effect of the $j^{th}$ structural shock. -->
<!-- The algorithm is as follows: -->
<!-- 1. For $1\le j\le m$, draw $u_j\in \mathbb{R}^{m+1-j-z_j}$, where $z_j$ is the number of zero restrictions imposed on the $j^{th}$ shock, from a standard normal distribution and set $w_j = u_j/||u_j||$. -->
<!-- 2. Define $Q= \begin{pmatrix}q_1&...&q_m\end{pmatrix}$ recursively by $q_j = K_jw_j$ for any matrix $K_j$ whose columns form an orthogonal basis for the null space of the matrix \[M_j = -->
<!-- \begin{pmatrix} q_1&...&q_{j-1}&\color{blue}{(Z_jP)'}\end{pmatrix}'\] (Vector $q_j$ will then be orthogonal to $\begin{pmatrix} q_1&...&q_{j-1}\end{pmatrix}$ and satisfy the zero restriction.)  -->
<!-- 3. Set $B=PQ$. -->
<!-- 4. Check sign restrictions ($S_jb_j>0$ for all $j$?). -->
<!-- Perform $N$ replications and report the median impulse response (and its confidence intervals). -->
<!-- ```{r signrestr2, fig.align = 'left-aligned', out.width = "95%", fig.cap = "IRF associated with a monetary policy shock; sign-restriction approach."} -->
<!-- sign.restrictions <- list() -->
<!-- SR.restrictions <- list() -->
<!-- horizon <- list() -->
<!-- #Define sign restrictions and horizon for restrictions -->
<!-- for(i in 1:n){ -->
<!--   sign.restrictions[[i]] <- matrix(0,n,n) -->
<!--   horizon[[i]] <- 1 -->
<!-- } -->
<!-- sign.restrictions[[1]][1,6] <- 1 -->
<!-- sign.restrictions[[2]][1,7] <- 1 -->
<!-- sign.restrictions[[3]][1,1] <- 1 -->
<!-- sign.restrictions[[3]][2,5] <- 1 -->
<!-- sign.restrictions[[4]][1,2] <- -1 -->
<!-- sign.restrictions[[4]][2,5] <- 1 -->
<!-- sign.restrictions[[5]][1,3] <- 1 -->
<!-- sign.restrictions[[5]][2,5] <- 1 -->
<!-- sign.restrictions[[6]][1,5] <- -1 -->
<!-- sign.restrictions[[6]][2,3] <- 1 -->
<!-- sign.restrictions[[6]][3,6] <- 1 -->
<!-- horizon[[6]] <- 1:5 -->
<!-- #Define zero restrictions -->
<!-- SR.restrictions[[1]] <- array(0,c(1,n)) -->
<!-- SR.restrictions[[1]][1,5] <- 1 -->
<!-- SR.restrictions[[2]] <- array(0,c(1,n)) -->
<!-- SR.restrictions[[2]][1,5] <- 1 -->
<!-- for(i in 3:n){ -->
<!--   SR.restrictions[[i]] <- array(0,c(0,n)) -->
<!-- } -->
<!-- res.svar.signs.zeros <- svar.signs(y,p=3, -->
<!--                                   nb.shocks = 6, #number of identified shocks -->
<!--                                   nb.periods.IRF = 20, -->
<!--                                   bootstrap.replications = 100, # = 0 or 1 -->
<!--                                   confidence.interval = 0.90, # expressed in pp. -->
<!--                                   indic.plot = 1, # Plots are displayed if = 1. -->
<!--                                   nb.draws = 10000, # number of draws -->
<!--                                   sign.restrictions, -->
<!--                                   horizon, -->
<!--                                   recursive =0, -->
<!--                                   SR.restrictions -->
<!-- ) -->
<!-- IRFs.signs <- res.svar.signs.zeros$IRFs.signs -->
<!-- nb.rotations <- res.svar.signs.zeros$xx -->
<!-- ``` -->
</div>
<div id="forecast-error-variance-maximization" class="section level3" number="1.3.8">
<h3>
<span class="header-section-number">1.3.8</span> Forecast error variance maximization<a class="anchor" aria-label="anchor" href="#forecast-error-variance-maximization"><i class="fas fa-link"></i></a>
</h3>
<p>The approach presented in this section exploits the derivations of <span class="citation">Uhlig (<a href="references.html#ref-Uhlig_2004" role="doc-biblioref">2004</a>)</span>. <span class="citation">Barsky and Sims (<a href="references.html#ref-BARSKY2011273" role="doc-biblioref">2011</a>)</span> exploit this approach to identify a TFP news shock, that they define as the shock (a) that is orthogonal to the innovation in current utilization-adjusted TFP and (b) that best explains variation in future TFP.</p>
<p>Consider a process <span class="math inline">\(\{y_t\}\)</span> that admits the infinite MA representation of Eq. <a href="TS.html#eq:InfMA">(1.34)</a>. Let <span class="math inline">\(Q\)</span> be an orthogonal matrix, an alternative decomposition is:
<span class="math display">\[\begin{eqnarray}
y_t&amp;=&amp;\sum_{h=0}^{+\infty}\Psi_h\underbrace{\eta_{t-h}}_{Q\tilde \eta_{t-h}} = \sum_{h=0}^{+\infty}\underbrace{\Psi_hQ}_{\tilde\Psi_h}\tilde
\eta_{t-h} = \sum_{h=0}^{+\infty}\tilde\Psi_h\tilde \eta_{t-h},
\end{eqnarray}\]</span>
where <span class="math inline">\(\tilde \eta_{t-h}=Q'\eta_{t-h}\)</span> are the white-noise shocks associated with the new MA representation. (They also satisfy <span class="math inline">\(\mathbb{V}ar(\tilde\eta_t)=Id\)</span>.)</p>
<p>The <span class="math inline">\(h\)</span>-step ahead prediction error of <span class="math inline">\(y_{t+h}\)</span>, given all the data up to and including <span class="math inline">\(t-1\)</span> is given by
<span class="math display">\[
e_{t+h}(h)=y_{t+h}-\mathbb{E}_{t-1}(y_{t+h})=\sum_{j=0}^h\tilde \Psi_h\tilde \eta_{t+h-j}.
\]</span></p>
<p>The variance-covariance matrix of <span class="math inline">\(e_{t+h}(h)\)</span> is
<span class="math display">\[
\Omega(h)=\sum_{j=0}^h\tilde \Psi_j\tilde \Psi_j'=\sum_{j=0}^h \Psi_j \Psi_j'.
\]</span></p>
<p>We can decompose <span class="math inline">\(\Omega(h)\)</span> into the contribution of each shock <span class="math inline">\(l\)</span> (<span class="math inline">\(l^{th}\)</span> component of <span class="math inline">\(\tilde{\eta}_t\)</span>):
<span class="math display">\[
\Omega^{(h)}=\sum_{l=1}^n\Omega_l^{(h)}(Q)
\]</span>
with
<span class="math display">\[
\Omega_l^{(h)}(Q) =\sum_{j=0}^h(\Psi_jq_l)(\Psi_jq_l)',
\]</span>
where <span class="math inline">\(q_l\)</span> is the <span class="math inline">\(l^{th}\)</span> column of <span class="math inline">\(Q\)</span>.</p>
<p>This decomposition can be used with the objective of finding the <strong>impulse vector</strong> <span class="math inline">\(b\)</span> that is s.t. that it explains as much as possible of the sum of the <span class="math inline">\(h\)</span>-step ahead prediction error variance of some variable <span class="math inline">\(i\)</span>, say, for prediction horizons <span class="math inline">\(h \in [\underline{h} , \overline{h}]\)</span>.</p>
<p>Formally, the task is to explain as much as possible of the variance
<span class="math display">\[
\sigma^2(\underline{h},\overline{h},q_1)=\sum_{h=\underline{h}}^{\overline{h}} \sum_{j=0}^h\left[(\Psi_jq_1)(\Psi_jq_1)'\right]_{i,i}
\]</span>
with a single impulse vector <span class="math inline">\(q_1\)</span>.</p>
<p>Denote by <span class="math inline">\(E_{ii}\)</span> the matrix that is filled with zeros, except for its (<span class="math inline">\(i,i\)</span>) entry, set to 1. We have:
<span class="math display">\[\begin{eqnarray*}
\sigma^2(\underline{h},\overline{h},q_1)&amp;=&amp;\sum_{h=\underline{h}}^{\overline{h}} \sum_{j=0}^h\left[(\Psi_jq_1)(\Psi_jq_1)'\right]_{i,i}=\sum_{h=\underline{h}}^{\overline{h}} \sum_{j=0}^h Tr\left[E_{ii}(\Psi_jq_1)(\Psi_jq_1)'\right]\\
&amp;=&amp;\sum_{h=\underline{h}}^{\overline{h}} \sum_{j=0}^h Tr\left[q_1'\Psi_j'E_{ii}\Psi_j q_1\right]\\
&amp;=&amp; q_1'Sq_1,
\end{eqnarray*}\]</span>
where
<span class="math display">\[\begin{eqnarray*}
\begin{array}{lll}S&amp;=&amp;\sum_{h=\underline{h}}^{\overline{h}}\sum_{j=0}^{h}\Psi_j'E_{ii}\Psi_j\\
&amp;=&amp;\sum_{j=0}^{\overline{h}}(\overline{h}+1-max(\underline{h},j))\Psi_j'E_{ii}\Psi_j\\
&amp;=&amp;\sum_{j=0}^{\overline{h}}(\overline{h}+1-max(\underline{h},j))\Psi_{j,i}'\Psi_{j,i}\\
\end{array}
\end{eqnarray*}\]</span>
where <span class="math inline">\(\Psi_{j,i}\)</span> denotes row <span class="math inline">\(i\)</span> of <span class="math inline">\(\Psi_{j}\)</span>, i.e., the response of variable <span class="math inline">\(i\)</span> at horizon <span class="math inline">\(j\)</span> (when <span class="math inline">\(Q=Id\)</span>).</p>
<p>The maximization problem subject to the side constraint <span class="math inline">\(q_1'q_1=1\)</span> can be written as a Lagrangian: <span class="math display">\[
L=q_1'Sq_1-\lambda(q_1'q_1-1),
\]</span>
with the first-order condition <span class="math inline">\(Sq_1=\lambda q_1\)</span> (the side constraint is <span class="math inline">\(q_1'q_1=1\)</span>). From this equation, we see that the solution <span class="math inline">\(q_1\)</span> is an eigenvector of <span class="math inline">\(S\)</span>, the one associated with eigenvalue <span class="math inline">\(\lambda\)</span>. We also see that <span class="math inline">\(\sigma^2(\underline{h},\overline{h},q_1)=\lambda\)</span>. Thus, to maximize this variance, we need to find the eigenvector of <span class="math inline">\(S\)</span> that is associated with the maximal eigenvalue <span class="math inline">\(\lambda\)</span>. That defines the first principal component (see Section <a href="append.html#PCAapp">2.1</a>). That is, if <span class="math inline">\(S\)</span> admits the following spectral decomposition:
<span class="math display">\[
S = \mathcal{P}D\mathcal{P}',
\]</span>
where <span class="math inline">\(D\)</span> is diagonal matrix whose entries are the (ordered) eigenvalues: <span class="math inline">\(\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_n \ge 0\)</span>, then <span class="math inline">\(\sigma^2(\underline{h},\overline{h},q_1)\)</span> is maximized for <span class="math inline">\(q_1 = p_1\)</span>, where <span class="math inline">\(p_1\)</span> is the first column of <span class="math inline">\(\mathcal{P}\)</span>.</p>
</div>
<div id="NonGaussian" class="section level3" number="1.3.9">
<h3>
<span class="header-section-number">1.3.9</span> Identification based on non-normality of the shocks<a class="anchor" aria-label="anchor" href="#NonGaussian"><i class="fas fa-link"></i></a>
</h3>
<p>In this section, we show that the non-identification of the structural shocks (<span class="math inline">\(\eta_t\)</span>) is specific to the Gaussian case. We propose consistent estimation approaches for SVAR in the context of non-Gaussian shocks.</p>
<!-- In a first part, we focus on non-Gaussian SVAR models; in a second part, we discuss the case of non-Gaussian SVARMA models. -->
<p>We have seen in what precedes that we cannot identify <span class="math inline">\(B\)</span> based on first and second moments only. Since a Gaussian distribution is perfectly determined by the first two moments, it comes that one cannot achieve identification when the structural shocks are Gaussian. That is, even if we observe an infinite number of i.i.d. <span class="math inline">\(B \eta_t\)</span>, we cannot recover <span class="math inline">\(B\)</span> is the <span class="math inline">\(\eta_t\)</span>’s are Gaussian.</p>
<p>Indeed, if <span class="math inline">\(\eta_t \sim \mathcal{N}(0,Id)\)</span>, then the distribution of <span class="math inline">\(\varepsilon_t \equiv B \eta_t\)</span> is <span class="math inline">\(\mathcal{N}(0,BB')\)</span>. Hence <span class="math inline">\(\Omega = B B'\)</span> is observed (in the population), but for any orthogonal matrix <span class="math inline">\(Q\)</span> (i.e. <span class="math inline">\(QQ'=Id\)</span>), we also have <span class="math inline">\(BQ \eta_t \sim \mathcal{N}(0,\Omega)\)</span>.</p>
<p>To illustrate, consider the following bivariate Gaussian situations, with <span class="math inline">\(\Theta_1=0\)</span>):</p>
<p><span class="math inline">\(\left[\begin{array}{c}\eta_{1,t}\\ \eta_{2,t}\end{array}\right]\sim \mathcal{N}(0,Id)\)</span>, with
<span class="math inline">\(B = \left[\begin{array}{cc} 1 &amp; 2 \\ -1 &amp; 1 \end{array}\right]\)</span> and
<span class="math inline">\(Q = \left[\begin{array}{cc} \cos(\pi/3) &amp; -\sin(\pi/3) \\ \sin(\pi/3) &amp; \cos(\pi/3) \end{array}\right]\)</span> (rotation).</p>
<p>Figure <a href="TS.html#fig:preMadeFigureICA">1.18</a> shows that the distributions of <span class="math inline">\(B \eta_t\)</span> and of <span class="math inline">\(BQ\eta_t\)</span> are identical. However, the impulse response functions associated with one of the other impulse matrix (<span class="math inline">\(B\)</span> or <span class="math inline">\(BQ\)</span>) are different. This is illustrated by Figure <a href="TS.html#fig:preMadeFigureICA2">1.19</a>, that shows the IRFs associated with two identical models (defined by Eq. <a href="TS.html#eq:VARMA111">(1.35)</a>), the only difference being the impulse matrix (<span class="math inline">\(B\)</span> or <span class="math inline">\(BQ\)</span>).</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:preMadeFigureICA"></span>
<img src="images/Figure_A.png" alt="This figure compares the distributions of two Gaussian bivariate vectors, $B \eta_t$ and $BQ\eta_t$, where $\eta_{t} \sim \mathcal{N}(0,Id)$ (therefore $\eta_{1,t}$ and $\eta_{2,t}$ are independent), and $Q$  is an orthogonal matrix." width="95%"><p class="caption">
Figure 1.18: This figure compares the distributions of two Gaussian bivariate vectors, <span class="math inline">\(B \eta_t\)</span> and <span class="math inline">\(BQ\eta_t\)</span>, where <span class="math inline">\(\eta_{t} \sim \mathcal{N}(0,Id)\)</span> (therefore <span class="math inline">\(\eta_{1,t}\)</span> and <span class="math inline">\(\eta_{2,t}\)</span> are independent), and <span class="math inline">\(Q\)</span> is an orthogonal matrix.
</p>
</div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:preMadeFigureICA2"></span>
<img src="TimeSeries_files/figure-html/preMadeFigureICA2-1.png" alt="This figure shows that the impulse response functions associated with an impulse matrix equal to $B$ (black line) or $BQ$ (red line) are different (even if $BB'=BQ(BQ)'$)." width="95%"><p class="caption">
Figure 1.19: This figure shows that the impulse response functions associated with an impulse matrix equal to <span class="math inline">\(B\)</span> (black line) or <span class="math inline">\(BQ\)</span> (red line) are different (even if <span class="math inline">\(BB'=BQ(BQ)'\)</span>).
</p>
</div>
<p>Hence, in the Gaussian case, external restrictions (economic hypotheses) are needed to identify <span class="math inline">\(B\)</span> (see previous sections). But such restrictions may not be necessary if the structural shocks are not Gaussian. That is, the identification problem is very specific to normally-distributed <span class="math inline">\(\eta_t\)</span>’s (<span class="citation">Rigobon (<a href="references.html#ref-Rigobon_2003" role="doc-biblioref">2003</a>)</span>, <span class="citation">Normandin and Phaneuf (<a href="references.html#ref-NORMANDIN20041217" role="doc-biblioref">2004</a>)</span>, <span class="citation">Lanne and Lütkepohl (<a href="references.html#ref-Lanne_Lutkepohl_2008" role="doc-biblioref">2008</a>)</span>).</p>
<p>To better see why this can be the case, consider again a bivariate vector of independent structural shocks (<span class="math inline">\(\eta_{1,t}\)</span> and <span class="math inline">\(\eta_{2,t}\)</span>) but, now, assume that one of them is not Gaussian any more. Specifically, assume that <span class="math inline">\(\eta_{2,t}\)</span> is drawn from a Student distribution with 5 degrees of freedom:
<span class="math inline">\(\eta_{1,t} \sim \mathcal{N}(0,1)\)</span>, <span class="math inline">\(\eta_{2,t} \sim t(5)\)</span>,
<span class="math inline">\(B = \left[\begin{array}{cc} 1 &amp; 2 \\ -1 &amp; 1 \end{array}\right]\)</span> and
<span class="math inline">\(Q = \left[\begin{array}{cc} \cos(\pi/3) &amp; -\sin(\pi/3) \\ \sin(\pi/3) &amp; \cos(\pi/3) \end{array}\right]\)</span>.</p>
<p>Figure <a href="TS.html#fig:preMadeFigureICAGaussianStudent">1.20</a> shows that, in this case, <span class="math inline">\(B \eta_t\)</span> and <span class="math inline">\(BQ\eta_t\)</span> do not have the same distribution any more (in spite of the fact that, in both cases, we have <span class="math inline">\(\mathbb{V}ar(\varepsilon_t)=BB'\)</span>). This opens the door to the identification of the impulse matrix (<span class="math inline">\(BQ\)</span>) in the non-Gaussian case.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:preMadeFigureICAGaussianStudent"></span>
<img src="images/Figure_C.png" alt="This figure compares the distributions of two Gaussian bivariate vectors, $B \eta_t$ and $BQ\eta_t$, where $\eta_t{1,t} \sim \mathcal{N}(0,1)$, $\eta_t{2,t} \sim t(5)$, and $Q$  is an orthogonal matrix." width="95%"><p class="caption">
Figure 1.20: This figure compares the distributions of two Gaussian bivariate vectors, <span class="math inline">\(B \eta_t\)</span> and <span class="math inline">\(BQ\eta_t\)</span>, where <span class="math inline">\(\eta_t{1,t} \sim \mathcal{N}(0,1)\)</span>, <span class="math inline">\(\eta_t{2,t} \sim t(5)\)</span>, and <span class="math inline">\(Q\)</span> is an orthogonal matrix.
</p>
</div>
<!-- NB: In both cases, we have $\mathbb{V}ar(\varepsilon_t)=BB'$. -->
<!-- Example: Bivariate Student (5) case -->
<!-- $\eta_{1,t} \sim t(5)$, $\eta_{2,t} \sim t(5)$, -->
<!-- $B = \left[\begin{array}{cc} -->
<!-- 1 & 2 \\ -->
<!-- -1 & 1 -->
<!-- \end{array}\right]$ and -->
<!-- $Q = \left[\begin{array}{cc} -->
<!-- \cos(\pi/3) & -\sin(\pi/3) \\ -->
<!-- \sin(\pi/3) & \cos(\pi/3) -->
<!-- \end{array}\right]$. -->
<!-- $\Rightarrow$ Distribution of $B \eta_t$ versus that of $BQ\eta_t$? -->
<!-- ```{r simulStudentStudent, eval=FALSE} -->
<!-- theta.angle <- pi/3 -->
<!-- Q <- matrix(c(cos(theta.angle),sin(theta.angle),-sin(theta.angle),cos(theta.angle)),2,2) -->
<!-- #nb.sim <- 10^4 -->
<!-- nb.sim <- 10^2 -->
<!-- distri.1 <- list(type=c("gaussian"),name="Panel (a) Gaussian",name.4.table="Gaussian") -->
<!-- distri.2 <- list(type=c("mixt.gaussian"),mu=0,sigma=5,p=.03,name="Panel (b) Mixture of Gaussian",name.4.table="Mixture of Gaussian") -->
<!-- distri.3 <- list(type=c("student"),df=c(5),name="Panel (c) Student (df: 5)",name.4.table="Student (df: 5)") -->
<!-- distri.4 <- list(type=c("student"),df=c(10),name="Panel (d) Student (df: 10)",name.4.table="Student (df: 10)") -->
<!-- x.lim <- c(-7,7) -->
<!-- y.lim <- c(-5,5) -->
<!-- nb.points <- 100 -->
<!-- x.points <- seq(x.lim[1],x.lim[2],length.out=nb.points) -->
<!-- y.points <- x.points -->
<!-- all.x <- c(matrix(x.points,nb.points,nb.points)) -->
<!-- all.y <- c(t(matrix(x.points,nb.points,nb.points))) -->
<!-- eps <- cbind(all.x,all.y) -->
<!-- par(plt=c(.25,.9,.25,.8)) -->
<!-- eta.1 <- simul.distri(distri.3,nb.sim) -->
<!-- eta.2 <- simul.distri(distri.3,nb.sim) -->
<!-- epsilon.C <- cbind(eta.1,eta.2) %*% t(C) -->
<!-- epsilon.CQ <- cbind(eta.1,eta.2) %*% t(C %*% Q) -->
<!-- Model$distri <- list(type=c("student","student"),df=c(5,5)) -->
<!-- par(mfrow=c(1,2)) -->
<!-- plot(epsilon.C[,1],epsilon.C[,2],pch=19, -->
<!--      xlim=x.lim,ylim=y.lim,col="#00000044", -->
<!--      xlab=expression(epsilon[1]), -->
<!--      ylab=expression(epsilon[2]),cex.lab=1.6,cex.main=1.6, -->
<!--      main=expression(paste("Distribution of ",epsilon[t]," = ",B,eta[t],sep=""))) -->
<!-- z <- matrix(exp(g(eps,Model)),nb.points,nb.points) -->
<!-- par(new=TRUE) -->
<!-- max.z <- max(z) -->
<!-- levels <- c(.01,.1,.3,.6,.9)*max.z -->
<!-- contour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col="red",lwd=2) -->
<!-- plot(epsilon.CQ[,1],epsilon.CQ[,2],pch=19, -->
<!--      xlim=x.lim,ylim=y.lim,col="#00000044", -->
<!--      xlab=expression(epsilon[1]), -->
<!--      ylab=expression(epsilon[2]),cex.lab=1.6,cex.main=1.6, -->
<!--      main=expression(paste("Distribution of ",epsilon[t]," = ",BQ,eta[t],sep=""))) -->
<!-- z <- matrix(exp(g(eps,Model)),nb.points,nb.points) -->
<!-- par(new=TRUE) -->
<!-- max.z <- max(z) -->
<!-- levels <- c(.01,.1,.3,.6,.9)*max.z -->
<!-- contour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col="red",lwd=2) -->
<!-- ``` -->
<!-- NB: In both cases, we have $\mathbb{V}ar(\varepsilon_t)=BB'$. -->
<!-- ```{r preMadeFigureICAStudentStudent, fig.align = 'left-aligned', out.width = "95%", fig.cap = "XXXX.", echo=FALSE} -->
<!-- knitr::include_graphics("images/Figure_D.png") -->
<!-- ``` -->
<p>The exercise that consists in identifying non-Gaussian independent shocks out of linear combinations of these shocks is a well-known problem of the signal-processingliterature, called <strong>independent component analysis (ICA)</strong>. Without loss of generality, we can assume that <span class="math inline">\(BB' = Id\)</span> (i.e. <span class="math inline">\(B\)</span> is orthogonal). (If this is not the case, i.e. if <span class="math inline">\(\mathbb{V}ar(\varepsilon_t)=\Omega \ne Id\)</span>, then one can pre-multiply the data by <span class="math inline">\(\Omega^{-1/2}\)</span>.) The classical ICA problem is as follows: Find <span class="math inline">\(B\)</span> such that <span class="math inline">\(\varepsilon_t = B \eta_t\)</span> (or $_t= B’ _t $) given that</p>
<ol style="list-style-type: lower-roman">
<li>We observe the <span class="math inline">\(\varepsilon_t\)</span>’s,</li>
<li>The components of <span class="math inline">\(\eta_t\)</span> are independent,</li>
<li>
<span class="math inline">\(BB'=Id\)</span> (i.e., <span class="math inline">\(B\)</span> is orthogonal).</li>
</ol>
<p>Figure <a href="TS.html#fig:ThreePlots">1.21</a> represents again some bivariate distributions. The black (red) lines correspond to the distributions of <span class="math inline">\(\eta_t\)</span> (<span class="math inline">\(B\eta_t\)</span>). It is important to note that the two components of vector <span class="math inline">\(B \eta_t\)</span> are not independent (contrary to those of <span class="math inline">\(\eta_t\)</span>).</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:ThreePlots"></span>
<img src="images/Figure_E.png" alt="The three plots represent the bivariate distributions of $\eta_t$ (black) and of $B\eta_t$ (red), where the two components of $\eta_t$ are independent, of unit variance, and $B$ is orthogonal. Hence, for each of the three plots, $\mathbb{V}ar(B\eta_t)=Id$." width="95%"><p class="caption">
Figure 1.21: The three plots represent the bivariate distributions of <span class="math inline">\(\eta_t\)</span> (black) and of <span class="math inline">\(B\eta_t\)</span> (red), where the two components of <span class="math inline">\(\eta_t\)</span> are independent, of unit variance, and <span class="math inline">\(B\)</span> is orthogonal. Hence, for each of the three plots, <span class="math inline">\(\mathbb{V}ar(B\eta_t)=Id\)</span>.
</p>
</div>
<p>In all cases, we have <span class="math inline">\(\mathbb{V}ar(\varepsilon_t)=\mathbb{V}ar(\eta_t)=Id\)</span>. But the two components of <span class="math inline">\(\varepsilon_t\)</span> are not independent. For instance: We have <span class="math inline">\(\mathbb{E}(\varepsilon_{2,t}|\varepsilon_{1,t}&gt;4)&lt;0\)</span> (whereas <span class="math inline">\(\mathbb{E}(\eta_{2,t}|\eta_{1,t}&gt;4)=0\)</span>). The objctive of ICA is to rotate <span class="math inline">\(\varepsilon_t\)</span> to retrieve independent components (<span class="math inline">\(\eta_t\)</span>).</p>
<div class="hypothesis">
<p><span id="hyp:NonGauss" class="hypothesis"><strong>Hypothesis 1.1  </strong></span>Process <span class="math inline">\(\eta_t\)</span> satisfies:</p>
<ol style="list-style-type: lower-roman">
<li>The <span class="math inline">\(\eta_t\)</span>’s are i.i.d. (across time) with <span class="math inline">\(\mathbb{E}(\eta_t) = 0\)</span> and <span class="math inline">\(\mathbb{V}ar(\eta_t) = Id.\)</span>
</li>
<li>The components <span class="math inline">\(\eta_{1,t}, \ldots, \eta_{n,t}\)</span> are mutually independent.
iii We have
<span class="math display">\[
\varepsilon_t = B_0 \eta_t,
\]</span>
with <span class="math inline">\(\mathbb{V}ar(\varepsilon_t) = Id\)</span> (i.e. <span class="math inline">\(B_0\)</span> is orthogonal).</li>
</ol>
</div>
<div class="theorem">
<p><span id="thm:EK2004" class="theorem"><strong>Theorem 1.4  (Eriksson, Koivunen (2004)) </strong></span>If Hypothesis <a href="TS.html#hyp:NonGauss">1.1</a> is satisfied and if at most one of the components of <span class="math inline">\(\eta\)</span> is Gaussian, then matrix <span class="math inline">\(B_0\)</span> is identifiable up to the post multiplication by <span class="math inline">\(DP\)</span>, where <span class="math inline">\(P\)</span> is a permutation matrix and <span class="math inline">\(D\)</span> is a diagonal matrix whose diagonal entries are 1 or <span class="math inline">\(-1\)</span>.}</p>
</div>
<p>Hence, the structural shocks are identifiable. But how to estimate them based on observations of the <span class="math inline">\(\varepsilon_t\)</span>’s? <span class="citation">Gouriéroux, Monfort, and Renne (<a href="references.html#ref-Gourieroux_Monfort_Renne_2017" role="doc-biblioref">2017</a>)</span> have proposed a <strong>Pseudo-Maximum Likelihood (PML)</strong> approach. This approach consists in maximizing a so-called <strong>pseudo log-likelihood function</strong>, based on a set of p.d.f. <span class="math inline">\(g_i (\eta_i), i=1,\ldots,n\)</span> (that may be different from the true p.d.f. of the <span class="math inline">\(\eta_{i,t}\)</span>’s):
<span class="math display" id="eq:pseudolog">\[\begin{equation}
\log \mathcal{L}_T (B) = \sum^T_{t=1} \sum^n_{i=1} \log g_i (b'_i Y_t),\tag{1.49}
\end{equation}\]</span>
where <span class="math inline">\(b_i\)</span> is the <span class="math inline">\(i^{th}\)</span> column of matrix <span class="math inline">\(B\)</span> (or <span class="math inline">\(b'_i\)</span> is the <span class="math inline">\(i^{th}\)</span> row of <span class="math inline">\(B^{-1}\)</span> since <span class="math inline">\(B^{-1}=B'\)</span>).</p>
The log-likelihood function <a href="TS.html#eq:pseudolog">(1.49)</a> is computed as if the errors <span class="math inline">\(\eta_{i,t}\)</span> had the p.d.f. <span class="math inline">\(g_i (\eta_i)\)</span>. The PML estimator of matrix <span class="math inline">\(B\)</span> maximizes the pseudo log-likelihood function:
<span class="math display" id="eq:optimprob">\[\begin{equation}
\widehat{B_T} = \arg \max_B \sum^T_{t=1} \sum^n_{i=1} \log g_i (b'_i \varepsilon_t),\tag{1.50}
\end{equation}\]</span>
<p>The restrictions <span class="math inline">\(B'B = Id\)</span> can be eliminated by parameterizing <span class="math inline">\(B\)</span> in such a way that, whatever the consider parameters, <span class="math inline">\(B\)</span> is orthogonal. <span class="citation">Gouriéroux, Monfort, and Renne (<a href="references.html#ref-Gourieroux_Monfort_Renne_2017" role="doc-biblioref">2017</a>)</span> propose to use, for that, the Cayley’s representation: any orthogonal matrix with no eigenvalue equal to <span class="math inline">\(-1\)</span> can be written as
<span class="math display">\[\begin{equation}
B(A) = (Id+A) (Id-A)^{-1},
\end{equation}\]</span>
where <span class="math inline">\(A\)</span> is a skew symmetric (or antisymmetric) matrix, such that <span class="math inline">\(A'=-A\)</span>. There is a one-to-one relationship with <span class="math inline">\(A\)</span>, since:
<span class="math display">\[\begin{equation}
A = (B(A)+Id)^{-1} (B(A)-Id).
\end{equation}\]</span></p>
<p>Hence, the PML estimator of matrix <span class="math inline">\(B\)</span> is obtained as <span class="math inline">\(\widehat{B_T} = B(\hat{A}_T),\)</span> where:
<span class="math display" id="eq:optimprob2">\[\begin{equation}
\hat{A}_T = \arg \max_{a_{i,j}, i&gt;j} \sum^T_{t=1} \sum^n_{i=1} \log g_i [b_i (A)' \varepsilon_t].\tag{1.51}
\end{equation}\]</span></p>
<!-- The asymptotic properties of the PML estimator are derived  -->
<!-- **Asymptotic properties of the PML approach** -->
<!-- :::{.hypothesis #NonGauss2} -->
<!-- We have: -->
<!-- i. The functions $\log g_i$, $i=1,\ldots,n$, are twice continuously differentiable. -->
<!-- ii. $sup_{B: B'B = Id} \left|\sum^n_{i=1} \log g_i (b'_i y)\right| \leq h(y),$ where $\mathbb{E}_0 [h (Y)] < \infty$. -->
<!-- ::: -->
<!-- :::{.hypothesis #NonGauss3 name="Identification from the asymptotic FOC"} -->
<!-- The only solutions of the system of equations: -->
<!-- $$ -->
<!-- \left\{ -->
<!-- \begin{array}{l} \mathbb{E}_0 \left[b'_j \varepsilon_t \frac{d\log g_i}{d\eta} (b'_i \varepsilon_t)\right] = 0,\;  i \neq j, \\ -->
<!-- B' B = Id, -->
<!-- \end{array} -->
<!-- \right. -->
<!-- $$ -->
<!-- are the elements of $\mathcal{P}_0 \equiv \mathcal{P}(B_0)$, which is the set of matrices obtained by permutation and sign change of the columns of $B_0$. -->
<!-- ::: -->
<!-- :::{.hypothesis #NonGauss4 name="Local concavity"} -->
<!-- The asymptotic objective function is locally concave in a neighbourhood of a matrix $B$ of $\mathcal{P}(B_0)$, which is the case if and only if -->
<!-- $$ -->
<!-- \mathbb{E}_0 \left[ \frac{d^2 \log g_i (\eta_{i,t})}{d\eta^2} + \frac{d^2 \log g_j (\eta_{j,t})}{d\eta^2} - \eta_{j,t} \frac{d\log g_j (\eta_{j,t})}{d\eta}- \eta_{i,t} \frac{d\log g_i (\eta_{i,t})}{d\eta} \right] < 0, \forall i<j, -->
<!-- $$ -->
<!-- where $\eta_{i,t}$ is the $i^{th}$ component of the $\eta_t$ associated with this particular element $B$ of $\mathcal{P}(B_0)$. -->
<!-- ::: -->
<!-- This condition is in particular satisfied under the following set of conditions: derived in Hyvarinen (1997) XXX -->
<!-- \begin{equation} -->
<!-- \mathbb{E}_0 \left[\frac{d^2 \log g_i(\eta_{i,t})}{d\eta^2} - \eta_{i,t} \frac{d\log g_i(\eta_{i,t})}{d\eta}\right] <0,\quad  i=1,\ldots, n. (\#eq:HKO) -->
<!-- \end{equation} -->
<!-- Hyperbolic secant and the subgaussian distributions (see Table \@ref(tab:distriICA)): either one, or the other satisfy the inequality \@ref(eq:HKO) (see @Hyvarinen_Karhunen_Oja_2001). -->
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:distriICA">Table 1.1: </span> This table reports usual p.d.f. and their derivatives.</caption>
<colgroup>
<col width="8%">
<col width="31%">
<col width="25%">
<col width="35%">
</colgroup>
<thead><tr class="header">
<th></th>
<th><span class="math inline">\(\log g(x)\)</span></th>
<th><span class="math inline">\(\dfrac{d \log g(x)}{d x}\)</span></th>
<th><span class="math inline">\(\dfrac{d^2 \log g(x)}{d x^2}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Gaussian</td>
<td><span class="math inline">\(cst - x^2/2\)</span></td>
<td><span class="math inline">\(-x\)</span></td>
<td><span class="math inline">\(-1\)</span></td>
</tr>
<tr class="even">
<td>Student <span class="math inline">\(t(\nu&gt;4)\)</span>
</td>
<td><span class="math inline">\(-\dfrac{1-\nu}{2}\log\left( 1 +\dfrac{x^2}{\nu-2} \right)\)</span></td>
<td><span class="math inline">\(-\dfrac{x(1+\nu)}{\nu - 2 + x^2}\)</span></td>
<td><span class="math inline">\(- (1+\nu) \dfrac{\nu - 2 - x^2}{\nu - 2 + x^2}\)</span></td>
</tr>
<tr class="odd">
<td>Hyperbolic secant</td>
<td><span class="math inline">\(cst - \log\left( \cosh\left\{\dfrac{\pi}{2}x\right\} \right)\)</span></td>
<td><span class="math inline">\(-\dfrac{\pi}{2} anh\left(\dfrac{\pi}{2}x\right)\)</span></td>
<td><span class="math inline">\(-\left(\dfrac{\pi}{2}\dfrac{1}{\cosh\left(\dfrac{\pi}{2}x\right)}\right)^2\)</span></td>
</tr>
<tr class="even">
<td>Subgaussian</td>
<td><span class="math inline">\(cst + \pi x^2 + \log \left(\cosh\left\{\dfrac{\pi}{2}x\right\}\right)\)</span></td>
<td><span class="math inline">\(2\pi x+\dfrac{\pi}{2}\tanh\left(x \dfrac{\pi}{2}\right)\)</span></td>
<td><span class="math inline">\(2\pi +\left(\dfrac{\pi}{2}\dfrac{1}{\cosh\left(\dfrac{\pi}{2}x\right)}\right)^2\)</span></td>
</tr>
</tbody>
</table></div>
<!-- Note: Except for the Gaussian distribution, we have $\mathbb{E}[d^2 \log g(X)/d \varepsilon^2 - X d\log g(X)/d \varepsilon] < 0$ (i.e. Assumption\,4 is satisfied) when these pseudo-distributions coincide to the distribution of $X$. The subGaussian distribution is a mixture of Gaussian distributions: $X$ is drawn from this distribution if it is equal to $BY - (1-B)Y$, where $B$ is drawn from a Bernoulli distribution of parameter $1/2$ and $Y \sim \mathcal{N}(\sqrt{(\pi-2)/\pi},2/\pi)$. --><!-- Under Hypotheses \@ref(hyp:NonGauss)-\@ref(hyp:NonGauss4), --><p>Under assumptions on the <span class="math inline">\(g_i\)</span> functions (excluding the Gaussian distributions), <span class="citation">Gouriéroux, Monfort, and Renne (<a href="references.html#ref-Gourieroux_Monfort_Renne_2017" role="doc-biblioref">2017</a>)</span> derive the asymptotic properties of the PML estimator. Specifically, the PML estimator <span class="math inline">\(\widehat{B_T}\)</span> of <span class="math inline">\(B_0\)</span> is consistent (in <span class="math inline">\(\mathcal{P}_0\)</span>, the set of matrices obtained by permutation and sign change of the columns of <span class="math inline">\(B_0\)</span>) and asymptotically normal, with speed of convergence <span class="math inline">\(1/\sqrt{T}\)</span>.</p>
<p>The asymptotic variance-covariance matrix of <span class="math inline">\(vec \sqrt{T} (\widehat{B_T} - B_0)\)</span> is <span class="math inline">\(A^{-1} \left[\begin{array}{cc} \Gamma &amp; 0 \\ 0 &amp; 0 \end{array} \right] (A')^{-1}\)</span>, where matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(\Gamma\)</span> are detailed in <span class="citation">Gouriéroux, Monfort, and Renne (<a href="references.html#ref-Gourieroux_Monfort_Renne_2017" role="doc-biblioref">2017</a>)</span>.</p>
<p>Note that the potential misspecification of pseudo-distributions <span class="math inline">\(g_i\)</span> has no effect on the consistency of these specific PML estimators.</p>
<div class="example">
<p><span id="exm:GMR2017" class="example"><strong>Example 1.8  (Non-Gaussian monetary-policy shocks) </strong></span>We apply the PML-ICA approach on U.S. data coerving the period 1959:IV to 2015:I at the quarterly frequency (<span class="math inline">\(T=224\)</span>). We consider three dependent variables: inflation (<span class="math inline">\(\pi_t\)</span>), economic activity (<span class="math inline">\(z_t\)</span>, the output gap) and the nominal short-term interest rate (<span class="math inline">\(r_t\)</span>). Changes in the log of oil prices added as an exogenous variable (<span class="math inline">\(x_t\)</span>).</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="va">First.date</span> <span class="op">&lt;-</span> <span class="st">"1959-04-01"</span></span>
<span><span class="va">Last.date</span>  <span class="op">&lt;-</span> <span class="st">"2015-01-01"</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">US3var</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">Date</span><span class="op">&gt;=</span><span class="va">First.date</span><span class="op">)</span><span class="op">&amp;</span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">Date</span><span class="op">&lt;=</span><span class="va">Last.date</span><span class="op">)</span>,<span class="op">]</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">data</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"infl"</span>,<span class="st">"y.gdp.gap"</span>,<span class="st">"r"</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">names.var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"inflation"</span>,<span class="st">"real activity"</span>,<span class="st">"short-term rate"</span><span class="op">)</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span></code></pre></div>
<p>Let us denote by <span class="math inline">\(W_t\)</span> the set of information made of the past values of <span class="math inline">\(y_t= [\pi_t,z_t,r_t]\)</span>, that is <span class="math inline">\(\{y_{t-1},y_{t-2},\dots\}\)</span>, and of exogenous variables <span class="math inline">\(\{x_{t},x_{t-1},\dots\}\)</span>. The reduced-form VAR model reads:
<span class="math display">\[
y_t  = \underbrace{\mu + \sum_{i=1}^{p} \Phi_i y_{t-i} + \Theta x_t}_{a(W_t;\theta)} + u_t
\]</span>
where the <span class="math inline">\(u_t\)</span>’s are assumed to be serially independent, with zero mean and variance-covariance matrix <span class="math inline">\(\Sigma\)</span>.</p>
<p>Matrices <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\Phi_i\)</span>, <span class="math inline">\(\Theta\)</span> and <span class="math inline">\(\Sigma\)</span> are consistently estimated by OLS. Jarque-Bera tests support the hypothesis of non-normality for all residuals.</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nb.lags</span> <span class="op">&lt;-</span> <span class="fl">6</span> <span class="co"># number of lags used in the VAR model</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">nb.lags</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">lagged.Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="va">i</span>,<span class="va">n</span><span class="op">)</span>,<span class="va">Y</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">i</span><span class="op">)</span>,<span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">X</span>,<span class="va">lagged.Y</span><span class="op">)</span><span class="op">}</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">X</span>,<span class="va">data</span><span class="op">$</span><span class="va">commo</span><span class="op">)</span> <span class="co"># add exogenous variables</span></span>
<span><span class="va">Phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">0</span>,<span class="va">n</span>,<span class="va">n</span><span class="op">*</span><span class="va">nb.lags</span><span class="op">)</span>;<span class="va">mu</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="va">n</span><span class="op">)</span></span>
<span><span class="va">effect.commo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="va">n</span><span class="op">)</span></span>
<span><span class="va">U</span> <span class="op">&lt;-</span> <span class="cn">NULL</span> <span class="co"># Eta is the matrix of OLS residuals</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">eq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Y</span><span class="op">[</span>,<span class="va">i</span><span class="op">]</span> <span class="op">~</span> <span class="va">X</span><span class="op">)</span></span>
<span>  <span class="va">Phi</span><span class="op">[</span><span class="va">i</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="va">eq</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Phi</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span></span>
<span>  <span class="va">mu</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">eq</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="va">U</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">U</span>,<span class="va">eq</span><span class="op">$</span><span class="va">residuals</span><span class="op">)</span></span>
<span>  <span class="va">effect.commo</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">eq</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">eq</span><span class="op">$</span><span class="va">coef</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="op">}</span></span>
<span><span class="va">Omega</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">U</span><span class="op">)</span> <span class="co"># Covariance matrix of the OLS residuals.</span></span>
<span><span class="va">B</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/chol.html">chol</a></span><span class="op">(</span><span class="va">Omega</span><span class="op">)</span><span class="op">)</span> <span class="co"># Cholesky matrix associated with Omega (lower triang.)</span></span>
<span><span class="va">Eps</span> <span class="op">&lt;-</span> <span class="va">U</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="va">B</span><span class="op">)</span><span class="op">)</span> <span class="co"># Recover associated structural shocks</span></span></code></pre></div>
<p>We want to estimate the orthogonal matrix <span class="math inline">\(B\)</span> such that <span class="math inline">\(u_t=SB \eta_t\)</span>, where</p>
<ul>
<li>
<span class="math inline">\(S\)</span> results from the Cholesky decomposition of <span class="math inline">\(\Sigma\)</span> and</li>
<li>the components of <span class="math inline">\(\eta_t\)</span> are independent, zero-mean with unit variance.</li>
</ul>
<p>The PML approach is applied on standardized VAR residuals given by:
<span class="math display">\[
\hat\varepsilon_t = \hat{S}_T^{-1}\underbrace{[y_t - a(W_t;\hat\theta_T)]}_{\mbox{VAR residuals}}.
\]</span>
By construction of <span class="math inline">\(\hat{S}_T^{-1}\)</span>, it comes that the covariance matrix of these residuals is <span class="math inline">\(Id\)</span>.</p>
<p>The pseudo density functions are distinct and asymmetric mixtures of Gaussian distributions.</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">distri</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>  type<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"mixt.gaussian"</span>,<span class="st">"mixt.gaussian"</span>,<span class="st">"mixt.gaussian"</span><span class="op">)</span>,</span>
<span>  df<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="cn">NaN</span>,<span class="cn">NaN</span><span class="op">)</span>,</span>
<span>  p<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.5</span>,<span class="fl">.5</span>,<span class="fl">.5</span><span class="op">)</span>,mu<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.1</span>,<span class="fl">.1</span><span class="op">)</span>,sigma<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.5</span>,<span class="fl">.7</span>,<span class="fl">1.3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">AA.0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">res.optim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optim</a></span><span class="op">(</span><span class="va">AA.0</span>,<span class="va">func.2.minimize</span>,</span>
<span>                   Y <span class="op">=</span> <span class="va">Eps</span>, distri <span class="op">=</span> <span class="va">distri</span>,</span>
<span>                   gr <span class="op">=</span> <span class="va">d.func.2.minimize</span>,</span>
<span>                   method<span class="op">=</span><span class="st">"Nelder-Mead"</span>,</span>
<span>                   control<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace<span class="op">=</span><span class="cn">FALSE</span>,maxit<span class="op">=</span><span class="fl">1000</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">AA.0</span> <span class="op">&lt;-</span> <span class="va">res.optim</span><span class="op">$</span><span class="va">par</span></span>
<span><span class="va">res.optim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optim</a></span><span class="op">(</span><span class="va">AA.0</span>,<span class="va">func.2.minimize</span>,<span class="va">d.func.2.minimize</span>,</span>
<span>                   Y <span class="op">=</span> <span class="va">Eps</span>, distri <span class="op">=</span> <span class="va">distri</span>,</span>
<span>                   method<span class="op">=</span><span class="st">"BFGS"</span>,</span>
<span>                   control<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">AA.est</span> <span class="op">&lt;-</span> <span class="va">res.optim</span><span class="op">$</span><span class="va">par</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">Y</span><span class="op">)</span></span>
<span><span class="va">M</span> <span class="op">&lt;-</span> <span class="fu">make.M</span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">A.est</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">M</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">AA.est</span>,<span class="va">n</span>,<span class="va">n</span><span class="op">)</span></span>
<span><span class="va">C.PML</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="op">+</span> <span class="va">A.est</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="op">-</span> <span class="va">A.est</span><span class="op">)</span></span>
<span></span>
<span><span class="va">eta.PML</span> <span class="op">&lt;-</span> <span class="va">Eps</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">C.PML</span> <span class="co"># eta.PML are the ICA-estimated structural shocks</span></span>
<span></span>
<span><span class="va">A</span> <span class="op">&lt;-</span> <span class="fu">make.A.matrix</span><span class="op">(</span><span class="va">eta.PML</span>,<span class="va">distri</span>,<span class="va">C.PML</span><span class="op">)</span></span>
<span><span class="va">Omega</span> <span class="op">&lt;-</span> <span class="fu">make.Omega</span><span class="op">(</span><span class="va">eta.PML</span>,<span class="va">distri</span><span class="op">)</span></span>
<span><span class="co"># Compute asymptotic covariance matrix of C.PML:</span></span>
<span><span class="va">V</span> <span class="op">&lt;-</span> <span class="fu">make.Asympt.Cov.delta</span><span class="op">(</span><span class="va">eta.PML</span>,<span class="va">distri</span>,<span class="va">C.PML</span><span class="op">)</span></span>
<span><span class="va">param</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">C.PML</span><span class="op">)</span></span>
<span><span class="va">st.dev</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">V</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">t.stat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">C.PML</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">V</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">param</span>,<span class="va">st.dev</span>,<span class="va">t.stat</span><span class="op">)</span> <span class="co"># print results of PML estimation</span></span></code></pre></div>
<pre><code>##             param      st.dev      t.stat
##  [1,]  0.94417705 0.040848382  23.1141845
##  [2,] -0.32711569 0.118802653  -2.7534376
##  [3,]  0.03905164 0.074172945   0.5264944
##  [4,]  0.32070293 0.119270893   2.6888616
##  [5,]  0.93977707 0.041629110  22.5749976
##  [6,]  0.11818924 0.060821400   1.9432179
##  [7,] -0.07536139 0.071980455  -1.0469702
##  [8,] -0.09906759 0.062185577  -1.5930959
##  [9,]  0.99222290 0.007785691 127.4418551</code></pre>
<p>(Note: it is always useful to combine two optimization algorithms, such as <code>Nelder-Mead</code> and <code>BFGS</code>.)</p>
<p>We would obtain close results by neglecting commodity prices. In that case, one can simply use the function <code>estim.SVAR.ICA</code> of the <code>AEC</code> package. Let us compare the <span class="math inline">\(C\)</span> matrix obtained in the two cases (with or without commodity prices):</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ICA.res.no.commo</span> <span class="op">&lt;-</span> <span class="fu">estim.SVAR.ICA</span><span class="op">(</span><span class="va">Y</span>,distri <span class="op">=</span> <span class="va">distri</span>,p<span class="op">=</span><span class="fl">6</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">ICA.res.no.commo</span><span class="op">$</span><span class="va">C.PML</span>,<span class="cn">NaN</span>,<span class="va">C.PML</span><span class="op">)</span>,<span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code>##        [,1]  [,2]   [,3] [,4]   [,5]  [,6]   [,7]
## [1,]  0.956 0.287 -0.059  NaN  0.944 0.321 -0.075
## [2,] -0.292 0.950 -0.108  NaN -0.327 0.940 -0.099
## [3,]  0.025 0.121  0.992  NaN  0.039 0.118  0.992</code></pre>
<p>Once <span class="math inline">\(B\)</span> has been estimated, it remains to label the resulting structural shocks (components of <span class="math inline">\(\eta_{t}\)</span>). Postulated shocks are monetary-policy, supply, and demand shocks. This labelling can be based on the following considerations:</p>
<ul>
<li>Contractionary <strong>monetary-policy shocks</strong> have a negative impact on real activity and on inflation.</li>
<li>
<strong>Supply shock</strong> have influences of opposite signs on economic activity and on inflation.</li>
<li>
<strong>Demand shock</strong> have influences of same signs on economic activity and on inflation.</li>
</ul>
<p>Let us compute the IRFs associated with the three structural shocks. (For the sake of comparison, the first line of plots shows the IRFs to a monetary-policy shock obtained from a Cholesky-based approach where the short-term rate is ordered last.)</p>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">IRF.Chol</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">n</span>,<span class="fl">41</span>,<span class="va">n</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">IRF.ICA</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">n</span>,<span class="fl">41</span>,<span class="va">n</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">PHI</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="op">)</span>;<span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">nb.lags</span><span class="op">)</span><span class="op">{</span><span class="va">PHI</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span><span class="va">Phi</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">3</span>,<span class="va">nb.lags</span><span class="op">)</span><span class="op">)</span><span class="op">[</span>,,<span class="va">i</span><span class="op">]</span><span class="op">}</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">jjjj</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">u.shock</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="va">n</span><span class="op">)</span></span>
<span>  <span class="va">u.shock</span><span class="op">[</span><span class="va">jjjj</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span>  <span class="va">IRF.Chol</span><span class="op">[</span>,,<span class="va">jjjj</span><span class="op">]</span> <span class="op">&lt;-</span> </span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu">simul.VAR</span><span class="op">(</span>c<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">3</span><span class="op">)</span>,Phi<span class="op">=</span><span class="va">PHI</span>,B<span class="op">=</span><span class="va">B</span>,nb.sim<span class="op">=</span><span class="fl">41</span>,</span>
<span>                y0.star<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">3</span><span class="op">*</span><span class="va">nb.lags</span><span class="op">)</span>,indic.IRF <span class="op">=</span> <span class="fl">1</span>,u.shock <span class="op">=</span> <span class="va">u.shock</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">IRF.ICA</span><span class="op">[</span>,,<span class="va">jjjj</span><span class="op">]</span>  <span class="op">&lt;-</span> </span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu">simul.VAR</span><span class="op">(</span>c<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">3</span><span class="op">)</span>,Phi<span class="op">=</span><span class="va">PHI</span>,B<span class="op">=</span><span class="va">B</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">C.PML</span>,nb.sim<span class="op">=</span><span class="fl">41</span>,</span>
<span>                y0.star<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">3</span><span class="op">*</span><span class="va">nb.lags</span><span class="op">)</span>,indic.IRF <span class="op">=</span> <span class="fl">1</span>,u.shock <span class="op">=</span> <span class="va">u.shock</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:ICAFigIRF"></span>
<img src="TimeSeries_files/figure-html/ICAFigIRF-1.png" alt="The first row of plots shows the responses of the three endogenous variables to the monetary policy shock in the context of a Cholesky-idendtified SVAR (ordering: inflation, output gap, interest rate). The next three rows of plots show the repsonses of the endogenous variables to the three structural shocks identified by ICA. The last one (Shock 3) is close to the Cholesky-identified monetary policy shock." width="95%"><p class="caption">
Figure 1.22: The first row of plots shows the responses of the three endogenous variables to the monetary policy shock in the context of a Cholesky-idendtified SVAR (ordering: inflation, output gap, interest rate). The next three rows of plots show the repsonses of the endogenous variables to the three structural shocks identified by ICA. The last one (Shock 3) is close to the Cholesky-identified monetary policy shock.
</p>
</div>
<p>According to Figure <a href="TS.html#fig:ICAFigIRF">1.22</a>, Shock 1 is a supply shock, Shock 2 is a demand shock, and Shock 3 is a monetary-policy shock. Note that Shock 3 is close to the one resulting from the Cholesky approach.</p>
</div>
<!-- @Gourieroux_Monfort_Renne_2017 show that the asymptotic propoerties of the PML estimator can be exploited to build a test whose null hypothesis is: -->
<!-- *$H_0$: $B$ belongs to $\mathcal{P}_0$, where $\mathcal{P}_0$ is the set of orthogonal matrices obtained by permuting and changing the signs of the columns of a given orthogonal matrix $B_0$.* -->
<!-- Wald test: -->
<!-- ```{r ICA4, echo=FALSE, warning=FALSE, message=FALSE} -->
<!-- BB.Chol <- B -->
<!-- # all.permut contains all transformations of identity -->
<!-- # (up to permutations and sign changes of columns) -->
<!-- # i.e. 48 matrices (= layers): -->
<!-- all.permut <- do.signs(do.permut(n)) -->
<!-- p.value.W.best <- 0 -->
<!-- ksi.value.W.best <- 0 -->
<!-- dist.C.S.best <- 100000000 -->
<!-- V_1 <- make.Asympt.Cov.delta_1(eta.PML,distri,C.PML) -->
<!-- # Look for permutation of C that provides the highest p-value of the Wald test: -->
<!-- for(c.permut in 1:dim(all.permut)[3]){ -->
<!--   P <- all.permut[,,c.permut] -->
<!--   # Wald test: C = Id <=> C[2,1]=C[3,1]=C[3,2]=0 -->
<!--   R <- matrix(0,n,n^2) -->
<!--   R[1,2] <- 1 -->
<!--   R[2,3] <- 1 -->
<!--   R[3,6] <- 1 -->
<!--   invV <- V_1 -->
<!--   indices <- c(2,3,6) -->
<!--   # Compute correl matrix: -->
<!--   stdv.vec <- matrix(sqrt(diag(V)),ncol=1) -->
<!--   corrV <- V * (1/(stdv.vec %*% t(stdv.vec))) -->
<!--   # Visualize the pre-sum: -->
<!--   vecC.S <- matrix(c(C.PML - P),ncol=1) -->
<!--   # Compute distance between C.PML and Sigma.B.aux: -->
<!--   dist.C.S <- sum(vecC.S^2) -->
<!--   vec1 <- matrix(1,n^2,1) -->
<!--   Vall <- (vecC.S %*% t(vec1)) * invV * t(vecC.S %*% t(vec1)) -->
<!--   ksi.W <- t(vecC.S) %*% invV %*% vecC.S -->
<!--   # detect the three elements of Sigma.B.aux with the highest modulus -->
<!--   aux <- c(abs(P)) -->
<!--   aux.sorted <- sort(aux) -->
<!--   for(i in 1:n){ -->
<!--     aux.indic <- which(aux>=aux.sorted[n^2-n+1]) -->
<!--   } -->
<!--   R <- matrix(0,n,n^2) -->
<!--   R[1,aux.indic[1]] <- 1 -->
<!--   R[2,aux.indic[2]] <- 1 -->
<!--   R[3,aux.indic[3]] <- 1 -->
<!--   MC <- M %*% C.PML -->
<!--   mat.cov.Aronde <- make.Asympt.Cov.deltaAronde(eta.PML,distri,C.PML) -->
<!--   p.value.W <- 1-pchisq(ksi.W,3) -->
<!--   if(dist.C.S < dist.C.S.best){ -->
<!--     dist.C.S.best <- dist.C.S -->
<!--     p.value.W.best <- p.value.W -->
<!--     ksi.W.best <- ksi.W -->
<!--     B.permut.best <- t(all.permut[,,c.permut]) -->
<!--     c.permut.best <- c.permut -->
<!--     P.j.best <- P -->
<!--     Vall.best <- Vall -->
<!--     VecC.S.best <- vecC.S -->
<!--     eigen.best <- eigen(C.PML %*% P - P)$value -->
<!--   } -->
<!-- } -->
<!-- print(P.j.best) -->
<!-- print(c(ksi.W.best,p.value.W.best,c.permut.best)) -->
<!-- print(min(abs(eigen.best))) -->
<!-- ``` -->
<!-- Comparison of the previous IRFs with those stemming from "recursive" identification approaches based on specific short-run restrictions (SRRs). -->
<!-- SRRs approach (Section\,\ref{Section:Standard}) are based on the assumptions that -->
<!-- a. $Cov(\eta_t)=Id$, -->
<!-- b. the $k^{th}$ structural shock does not contemporaneously affects the first $k-1$ endogenous variables and -->
<!-- c. the contemporaneous effect of the $k^{th}$ structural shock on the $k^{th}$ dependent variable is positive. -->
<!-- Under these assumptions, the structural shocks are given by $S^{-1}u_t$. -->
<!-- SRR approaches assume --potentially wrongly-- that the contemporaneous impacts of some structural shocks on given variables are null. -->
<!-- The null hypothesis of these tests is $H_0= (P \in \mathcal{P}(Id))$. -->
<!-- The null hypothesis $H_0$ stating that the true value of $B$ belongs to $\mathcal{P}_0$ is not standard since it is a finite union of simple hypotheses $H_{0,j} = (B = B_{j,0})$. -->
<!-- **First testing procedure:** -->
<!-- * Define the Wald statistics $\hat\xi_{j,T}$, $j \in J$: -->
<!-- \begin{equation} -->
<!-- \hat\xi_{j,T} = T [vec\hat{B}_T-vec B_{j,0}]'\hat{A}_T' -->
<!-- \left[ -->
<!-- \begin{array}{cc} -->
<!-- \hat{\Omega}^{-1}_T & 0\\ -->
<!-- 0&0 -->
<!-- \end{array} -->
<!-- \right]\hat{A}_T -->
<!-- [vec\hat{B}_T-vec B_{j,0}], -->
<!-- \end{equation} -->
<!-- $\hat{A}_T$ and $\hat{\Omega}_T$ being consistent estimators of the matrices $A$ and $\Omega$. -->
<!-- Since the dimension of the asymptotic distribution of $\sqrt{T}[vec\hat{B}_T-vec B_{j,0}]$ is $\frac{1}{2}n(n-1)$, the asymptotic distribution of $\hat\xi_{j,T}$ under $H_{0,j}$ is $\chi^2\left(\frac{1}{2}n(n-1)\right)$. -->
<!-- * Define $\hat\xi_T = \underset{j \in J}{\min} \hat\xi_{j,T}$ as the test statistic for $H_0$. -->
<!-- * Under $H_0$, $\hat{B}_T$ converges to $B_{j_0,0}$ (say). -->
<!-- * By the asymptotic properties of the Wald statistics for simple hypotheses: -->
<!-- \begin{equation} -->
<!-- \hat\xi_{j_0,T} \overset{D}{\rightarrow} \chi^2\left(\frac{n(n-1)}{2}\right) \quad \mbox{and}\quad  \hat\xi_{j,T} \rightarrow \infty, \mbox{ if } j \ne j_0. -->
<!-- \end{equation} -->
<!-- Under the null hypothesis, $\hat\xi_T = \underset{j}{\min}$ $\hat\xi_{j,T}$ is asymptotically equal to $\hat\xi_{j_0,T}$  and its asymptotic distribution, $\chi^2\left(\frac{1}{2}n(n-1)\right)$, does not depend on $j_0$. Therefore $\hat\xi_T$ is asymptotically a pivotal statistic for the null hypothesis $H_0$ and the test of critical region $\hat\xi_T \ge \chi^2_{1-\alpha}\left(\frac{1}{2}n(n-1)\right)$ is of asymptotic level $\alpha$ and is consistent. -->
<!-- **Second testing procedure** -->
<!-- Define $B_{0,T} = \underset{B \in \mathcal{P}_0}{\mbox{Argmin }} d(\hat{B}_T,B)$ where $d$ is any distance, for instance the Euclidean one. -->
<!-- Under the null hypothesis $H_0$: $(B \in \mathcal{P}_0)$, $\hat{B}_T$ converges almost surely to an element of $\mathcal{P}_0$ denoted by $B_{j_0,0}$ and it is also the case for $B_{0,T}$ since, asymptotically, we have $B_{0,T}=B_{j_0,0}$. -->
<!-- Moreover: -->
<!-- $$ -->
<!-- \sqrt{T}(\hat{B}_T - B_{0,T})=\sqrt{T}(\hat{B}_T - B_{j_0,0}) + \sqrt{T}(B_{j_0,0} - B_{0,T}), -->
<!-- $$ -->
<!-- and, since $B_{0,T}$ is almost surely asymptotically equal to $B_{j_0,0}$, the asymptotic distribution of $\sqrt{T}(\hat{B}_T - B_{0,T})$ under $H_0$ is the same as that of $\sqrt{T}(\hat{B}_T - B_{j_0,0})$. -->
<!-- This implies that -->
<!-- $$ -->
<!-- \tilde\xi_{T} = T [vec\hat{B}_T-vec B_{0,T}]'\hat{A}_T' -->
<!-- \left[ -->
<!-- \begin{array}{cc} -->
<!-- \hat{\Omega}^{-1}_T & 0\\ -->
<!-- 0&0 -->
<!-- \end{array} -->
<!-- \right]\hat{A}_T -->
<!-- [vec\hat{B}_T-vec B_{0,T}] -->
<!-- $$ -->
<!-- is asymptotically distributed as $\chi^2\left(\frac{1}{2}n(n-1)\right)$ under $H_0$. -->
<!-- An advantage of this second method is that it necessitates the computation of only one Wald test statistic. -->
<!-- We consider two specific SRR schemes: -->
<!-- * In both of them, it is assumed that the monetary-policy shock has no contemporaneous impact on $\pi_t$ and $y_t$. -->
<!-- * In SRR Scheme 1: Inflation is contemporaneously impacted by one structural shock only. -->
<!-- * In SRR Scheme 2: Economic activity is contemporaneously impacted by one structural shock only. -->
<!-- \end{itemize} -->
<!-- * When economic activity is measured by means of the output gap, both SRRs are rejected at the 5\% level. -->
<p><strong>Relation with the Heteroskedasticity Identification</strong></p>
<p>In some cases, where the <span class="math inline">\(\varepsilon_t\)</span>’s are heteroskedastic, the <span class="math inline">\(B\)</span> matrix can be identified (<span class="citation">Rigobon (<a href="references.html#ref-Rigobon_2003" role="doc-biblioref">2003</a>)</span>, <span class="citation">Lanne, Lütkepohl, and Maciejowska (<a href="references.html#ref-LANNE2010121" role="doc-biblioref">2010</a>)</span>).</p>
<p>Consider the case where we still have <span class="math inline">\(\varepsilon_t = B \eta_t\)</span> but where <span class="math inline">\(\eta_t\)</span>’s variance conditionally depends on a regime <span class="math inline">\(s_t \in \{1,\dots,M\}\)</span>. That is:
<span class="math display">\[
\mathbb{V}ar(\eta_{k,t}|s_t) = \lambda_{s_t,k} \quad \mbox{for } k \in \{1,\dots,n\}
\]</span></p>
<p>Denoting by <span class="math inline">\(\Lambda_i\)</span> the diagonal matrix whose diagonal entries are the <span class="math inline">\(\lambda_{i,k}\)</span>’s, it comes that:
<span class="math display">\[
\mathbb{V}ar(\eta_{t}|s_t) = \Lambda_{s_t},\quad \mbox{and}\quad \mathbb{V}ar(\varepsilon_{t}|s_t) = B\Lambda_{s_t}B'.
\]</span></p>
<p>Without loss of generality, it can be assumed that <span class="math inline">\(\Lambda_1=Id\)</span>.</p>
<p>In this context, <span class="math inline">\(B\)</span> is identified, apart from sign reversal of its columns if for all <span class="math inline">\(k \ne j \in \{1,\dots,n\}\)</span>, there is a regime <span class="math inline">\(i\)</span> s.t. <span class="math inline">\(\lambda_{i,k} \ne \lambda_{i,j}\)</span>. (Prop.1 in @<span class="citation">Lanne, Lütkepohl, and Maciejowska (<a href="references.html#ref-LANNE2010121" role="doc-biblioref">2010</a>)</span>).</p>
<p>Bivariate regime case (<span class="math inline">\(M=2\)</span>): <span class="math inline">\(B\)</span> identified if the <span class="math inline">\(\lambda_{2,k}\)</span>’s are all different. That is, identification is ensured if “there is sufficient heterogeneity in the volatility changes” (<span class="citation">Lütkepohl and Netšunajev (<a href="references.html#ref-LUTKEPOHL20172" role="doc-biblioref">2017</a>)</span>).</p>
<p>If the regimes <span class="math inline">\(s_t\)</span> are exogenous and serially independent, then this situation is consistent with the “non-Gaussian” situation described above.</p>
<!-- **SVARMA** -->
<!-- In what precedes, we have seen that, if $y_t$ follows a VAR -->
<!-- \begin{eqnarray*} -->
<!-- y_t &=& \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p}  +   B \eta_{t}, -->
<!-- \end{eqnarray*} -->
<!-- and if the components of $\eta_t$ are non-Gaussian i.i.d. shocks, -->
<!-- then model parameters are identifiable (and can be consistently estimated). -->
<!-- What about Structural VARMAs? -->
<!-- \begin{eqnarray*} -->
<!-- &&y_t = \underbrace{\Phi_1 y_{t-1} + \dots +\Phi_p y_{t-p}}_{{\color{blue}\mbox{AR component}}} + \underbrace{B \eta_t+ \Theta_1 B \eta_{t-1}+ \dots+ \Theta_q B \eta_{t-q}}_{{\color{red}\mbox{MA component}}}\\ -->
<!-- &\Leftrightarrow&\underbrace{(I - \Phi_1 L - \dots - \Phi_p L^p)}_{= \Phi(L)}y_t =  \underbrace{ {\color{red} (I - \Theta_1 L - \ldots - \Theta_q L^q)}}_{={\color{red}\Theta(L)}} B \eta_{t} -->
<!-- \end{eqnarray*} -->
<!-- Still may have identification problems with $B$ (solved if $\eta_t$ non-Gaussian). Additional identification issue (w.r.t. SVAR case): -->
<!-- There are $2^n$ different $\Theta(L)$ yielding the exact same second-order properties for $y_t$. -->
<!-- %The SVARMA process may have a {\color{red}non-invertible MA matrix polynomial $\Theta(L)$} but, still, has the same second-order properties as a VARMA process in which the MA matrix polynomial is invertible (called fundamental representation). -->
<!-- **Univariate MA(1) Case** -->
<!-- The two MA(1) processes: -->
<!-- \begin{equation} -->
<!-- y_t = \varepsilon_t + \theta \varepsilon_{t-1} -->
<!-- \end{equation} -->
<!-- and -->
<!-- \begin{equation} -->
<!-- y^*_t = \theta \varepsilon_t + \varepsilon_{t-1}, -->
<!-- \end{equation} -->
<!-- have the same second-order properties (same variances, same auto-correlations). -->
<!-- Hence, if $\varepsilon_t $ is Gaussian, $y_t$ and $y^*_t$ are observationally equivalent. However, different IRFs: $(1,\theta,0,\dots)$ versus $(\theta,1,0,\dots)$. One of the 2 processes is said to be **fundamental**, or **invertible** (if $|\theta|<1$): -->
<!-- It is the one that is such that $\varepsilon_t$ can be deduced from past values of $y_t$ only. -->
<!-- The other one is **nonfundamental** ($\varepsilon_t$ cannot be deduced from past  values of $y_t$). -->
<!-- An MA process is said to be **invertible**, or **fundamental**, if we can obtain $\eta_t$ as a function of past values of $y_t$. -->
<!-- In the context of a univariate MA(1), -->
<!-- $$ -->
<!-- y_t = \eta_t + \theta \eta_{t-1} = (1 + \theta L) \eta_t, \quad \eta_t \sim \,i.i.d, -->
<!-- $$ -->
<!-- this is the case iff $|\theta|<1$. Indeed: -->
<!-- $$ -->
<!-- \begin{array}{lllll} -->
<!-- \mbox{When $|\theta|<1$, } \qquad \eta_t &=& \sum^\infty_{h=0} \theta^h y_{t-h} & \mbox{(invertible case)}\\ -->
<!-- \\ -->
<!-- \mbox{When $|\theta|>1$, } \qquad \eta_t &=& - \sum^\infty_{h=0} \dfrac{1}{\theta^{h+1}} y_{t+h+1} & \mbox{(non-invertible case)}. -->
<!-- \end{array} -->
<!-- $$ -->
<!-- Hence, when $|\theta|<1$ (respect. $|\theta|>1$), $\eta_t$ is a function of past (respect. future) values of $y_t$. -->
<!-- Context of a VMA(1): $y_t = \eta_t + \Theta \eta_{t-1}$. Process $y_t$ is invertible if the eigenvalues of $\Theta$ are strictly within the unit circle. -->
<!-- Consider the following (nonfundamental) VARMA(1,1) models: -->
<!-- $$ -->
<!-- y_t = \left[\begin{array}{cc} -->
<!-- 0.6 & 0.4 \\ -->
<!-- -0.2 & 0.9 -->
<!-- \end{array}\right] -->
<!-- y_{t-1} + B \eta_t - \Theta_1 B \eta_{t-1}, -->
<!-- $$ -->
<!-- with -->
<!-- $$ -->
<!-- B = \left[\begin{array}{cc} -->
<!-- 1.00 & 0.30 \\ -->
<!-- 0.50 & 1.00 -->
<!-- \end{array}\right],\quad \Theta_1 = \left[\begin{array}{cc} -->
<!-- -2.00 & 0.50 \\ -->
<!-- -0.20 & -0.70 -->
<!-- \end{array}\right]. -->
<!-- $$ -->
<!-- It is possible to compute the three other $\Theta_1$'s yielding to the same second-order properties (Lippi and Reichlin, 1994). Different IRFs (next slide, Model 2 is the fundamental one). -->
<!-- If the $\eta_t$'s are Gaussian, the resulting four models observationally equivalent. -->
<!-- Remark: The previous identification problem is still present here. That is: For a given $\Theta_1$, $B$ can be replaced by $BQ$, same second-order properties. -->
<!-- Hence, for Gaussian SVARMA models, two identification issues. -->
<!-- Standard estimation toolboxes return fundamental processes. However, in various contexts, no theoretical reasons to rule out **non-fundamentalness** (productivity shocks with lagged impacts, news/noise shocks, non-observability, rational expectations, prediction errors).  -->
<!-- * **Lagged impact (Lippi and Reichlin, 1993)** -->
<!-- Suppose that the productivity process, denoted by $y_t$, is given by: -->
<!-- $$ -->
<!-- y_t = \varepsilon_t + \theta \varepsilon_{t-1}, -->
<!-- $$ -->
<!-- where $\varepsilon_t$ denotes the productivity shock. The impact of the productivity shock may be maximal with a lag, i.e. $\theta > 1$. The MA(1) process is then non-fundamental. -->
<!-- \end{block} -->
<!-- * **Rational expectations**  -->
<!-- Simple example of Hansen and Sargent (1991). The economic variable $y_t$ is defined as: -->
<!-- $$ -->
<!-- y_t = E_t (\Sigma^\infty_{h=0} \beta^h w_{t+h}), \; \mbox{with }\; w_t = \varepsilon_t - \theta \varepsilon_{t-1},\; 0 < \beta <1, \; |\theta|<1. -->
<!-- $$ -->
<!-- If the information set available to the economic agent at date $t$ is $I_t = (\varepsilon_t, -->
<!-- \varepsilon_{t-1}, \ldots)$: -->
<!-- $$ -->
<!--   y_t  = (1-\beta \theta) \varepsilon_t - \theta \varepsilon_{t-1}. -->
<!-- $$ -->
<!-- The abs. value of the root of the moving average polynomial is larger or smaller than 1. -->
<!-- * **Advanced indicator (noise / news shocks) and  information structure.** -->
<!-- Consider a process $(x_t)$ that summarizes ``fundamentals'' (technology, preferences, endowments, or government policy) s.t.: -->
<!-- \begin{equation} -->
<!-- x_t = a(L)u_{t}, -->
<!-- \end{equation} -->
<!-- where process $(u_t)$ is a strong white noise. On date $t$, the consumer observes $x_t$ as well as a *noisy* signal about the future value of fundamentals: -->
<!-- \begin{equation} -->
<!-- s_t = x_{t+1} + v_t, -->
<!-- \end{equation} -->
<!-- where $(v_t)$ is also a strong white noise, independent of $(u_t)$ (incremental information about future fundamentals, Barsky and Sims, 2012). Moving average representation: -->
<!-- \begin{equation} -->
<!-- \left[ -->
<!-- \begin{array}{c} -->
<!-- x_t\\ -->
<!-- s_t -->
<!-- \end{array} -->
<!-- \right] =  -->
<!-- \left[ -->
<!-- \begin{array}{cc} -->
<!-- La(L) & 0\\ -->
<!-- a(L) & 1 -->
<!-- \end{array} -->
<!-- \right] -->
<!-- \left[ -->
<!-- \begin{array}{c} -->
<!-- u^a_t\\ -->
<!-- v_t -->
<!-- \end{array} -->
<!-- \right],\quad \mbox{where $u^a_t = u_{t+1}$.} -->
<!-- \end{equation} -->
<!-- The determinant of the moving average polynomial has a root equal to zero, which is within the unit circle $\Rightarrow$ non-fundamentalness. -->
<!-- Standard estimation toolboxes may lead to misspecified IRFs. -->
<!-- Gouri\'eroux, Monfort and Renne (2019) XXX show that both identification issues ["Q" (or **static issue**) + fundamentalness regime (or **dynamic issue**)}] disappear when the structural shocks are non-Gaussian. -->
<!-- Moreover: they develop consistent parametric and semi-parametric estimation methods when the MA part of the process may be non-fundamental, they illustrate the functioning and performances of these methods by applying them on both simulated and real data. -->
<!-- If  $\mathbb{V}ar(\varepsilon_t)=1$ and  $\mathbb{V}ar(\varepsilon^*_t)=\frac{1}{4}$, then the following two MA processes: -->
<!-- \begin{eqnarray*} -->
<!-- y_t &=& \varepsilon_t + \frac{1}{2} \varepsilon_{t-1}\\ -->
<!-- y^*_t &=&  \varepsilon^*_t + 2 \varepsilon^*_{t-1}, -->
<!-- \end{eqnarray*} -->
<!-- have the same second-order properties. -->
<!-- Processes $y_t$ and $y_t^*$ are observationally equivalent if the shocks are Gaussian. -->
<!-- Next slide: For each of the following four distributions, we compare the distributions of $(y_{t-1},y_t)$ and of $(y^*_{t-1},y^*_t)$. -->
<!-- ```{r FourDistriSVARMA, fig.align = 'left-aligned', out.width = "95%", fig.cap = "XXXX.", echo=FALSE} -->
<!-- knitr::include_graphics("images/Figure_distri4MC.png") -->
<!-- ``` -->
<!-- ```{r FourDistriSVARMA2, fig.align = 'left-aligned', out.width = "95%", fig.cap = "XXXX.", echo=FALSE} -->
<!-- knitr::include_graphics("images/Figure_simulSVARMA.png") -->
<!-- ``` -->
<!-- The fact that SVARMA parameterization is identified in non-Gaussian context already present in the literature [Chan, Ho and Tong, 2006, Biometrika] XXX -->
<!-- GMR (2019) propose two consistent estimation approaches of potentially-non-fundamental SVARMA($p$,$q$): -->
<!-- 1. Semi-parametric approach (2SLS-GMM). Two steps: -->
<!--     a. Consistent estimates of $\Pi= [\Phi_1,\dots,\Phi_p]$ can be obtained by applying two-stage least squares (2SLS). -->
<!--     b. Estimate $\Theta(L)$ by fitting higher-order moments of MA components. -->
<!-- 2. Maximum-Likelihood approach. Key ingredient: algorithm to recover $\eta_t$'s from the $y_t$'s, whatever the (non)fundamentalness regime. -->
<!-- First step of the 2SLS-GMM approach: -->
<!-- Consistent estimates of $\Pi= [\Phi_1,\dots,\Phi_p]$ obtained by 2SLS, using $y_{t-2},\dots,y_{t-k-1}$ ($k \ge p$) as instruments $\Rightarrow$ Consistent estimates $\hat{Z}_t$ of $\Theta(L)\varepsilon_t$ ($\hat{Z}_t \equiv \hat{\Phi}(L)y_t$). -->
<!-- \end{block} -->
<!-- Second step of the 2SLS-GMM approach: Using the Taylor expansion of the log-Laplace transf. of $(Z_t,Z_{t-1})$, it can be shown that: -->
<!-- \begin{equation*} -->
<!-- \begin{array}{ccll} -->
<!-- E[(u'Z_t + v'Z_{t-1})^2] &=& \sum_{j=1}^{n} [(u'B_{0j})^2 + (u'B_{1j}+v'B_{0j})^2 + (v'B_{1j})^2] & \mbox{(order 2)}\\ -->
<!-- E[(u'Z_t + v'Z_{t-1})^3] &=& \sum_{j=1}^{n} \kappa_{3j}[(u'B_{0j})^3 + (u'B_{1j}+v'B_{0j})^3 + (v'B_{1j})^3] & \mbox{(order 3)}\\ -->
<!-- E[(u'Z_t + v'Z_{t-1})^4] &=& \sum_{j=1}^{n} \kappa_{4j}[(u'B_{0j})^4 + (u'B_{1j}+v'B_{0j})^4 + (v'B_{1j})^4]\\ -->
<!-- && +3\left(\sum_{j=1}^{n}[(u'B_{0j})^2 + (u'B_{1j}+v'B_{0j})^2 + (v'B_{1j})^2]\right)^2 & \mbox{(order 4)}, -->
<!-- \end{array} -->
<!-- \end{equation*} -->
<!-- where $Z_t := \Phi(L)Y_t = B_0 \eta_t + B_1 \eta_{t-1}$ (i.e. $C=B_0$ and $B_1 \equiv - \Theta C$), and where the  $\kappa_{3j}$'s and $\kappa_{4j}$'s are the third-order and fourth-order cumulants of $\eta_{j,t}$.  -->
<!-- The previous formula are used to express moment restrictions of the form -->
<!-- $$ -->
<!-- E\left[h(Y_t,Y_{t-1},\dots,Y_{t-p-1};\alpha,\beta)\right]=0. -->
<!-- $$ -->
<!-- If $r$ is the dimension of $h(Y_t,Y_{t-1},\dots,Y_{t-p-1};\alpha,\beta)$, then we have the order condition $r \ge 2n^2+2n$ (if both third-order an fourth-order moments are used). -->
<!-- :::{.hypothesis #StrongSVARMA name="Strong stationary SVARMA process"} -->
<!-- We have: -->
<!-- i. The errors $\varepsilon_t$ are i.i.d. and such that $\mathbb{E}(\varepsilon_t)=0$ and $\mathbb{E}(\| \varepsilon_t \|^2)<\infty$. -->
<!-- ii. $\varepsilon_t = B \eta_t$, where the components $\eta_{j,t}$ are zero, unit variance, mutually independent. -->
<!-- iii. All the roots of $\det(\Phi (L))$ have a modulus strictly larger than 1 (stationarity) -->
<!-- iv. The roots of $\det(\Theta(z))$ are not on the unit circle. -->
<!-- v. The components of the first row of $B$ are positive and in increasing order. -->
<!-- vi. Each component of $\eta_t$ has a non-zero $r^{th}$ cumulant, with $r \ge 3$, and a finite moment of order $s$, where $s$ is an even integer greater than, or equal to, $r$. -->
<!-- ::: -->
<!-- Assumption vi is satisfied by most "usual" non-Gaussian distributions. -->
<!-- **Maximum Likelihood approach** (Univariate case $y_t = \varepsilon_t - \theta \varepsilon_{t-1}$)} -->
<!-- \begin{eqnarray*} -->
<!-- \mbox{When $|\theta|<1$, } \qquad \varepsilon_t &=& \sum^\infty_{h=0} \theta^h y_{t-h} = \underbrace{\sum^{t-1}_{h=0} \theta^h y_{t-h}}_{\mbox{truncated value }\hat{\varepsilon}_t(\theta)} + \mbox{ trunc. error}\\ -->
<!-- \mbox{When $|\theta|>1$, } \qquad \varepsilon_t &=& - \sum^\infty_{h=0} \frac{1}{\theta^{h+1}} y_{t+h+1} = \underbrace{- \sum^{T-t-1}_{h=0} \frac{1}{\theta^{h+1}} y_{t+h+1}}_{\mbox{truncated value }\hat{\varepsilon}_t(\theta)} + \mbox{ trunc. error}. -->
<!-- \end{eqnarray*} -->
<!-- Truncated log-likelihood function: $\log \mathcal{L}(y_1,\dots,y_T;\theta, \gamma) = \sum^T_{t=1} \log g \left(\hat{\varepsilon}_t(\theta); \gamma\right)$, where $g(.;\gamma)$ is the (parametric) p.d.f. of $\varepsilon_t$. -->
<!-- \end{block} -->
<!-- Maximum Likelihood approach (Multivariate case) -->
<!-- We propose an algorithm ($\mathcal{E}$) to get estimates of the $\varepsilon_t$'s ($t \in \{1,\dots,T\}$): -->
<!-- \begin{eqnarray*} -->
<!-- \mathcal{E}: &&\\ -->
<!-- && Y_{-p+1}^T,\Phi_1,\dots,\Phi_p,\Theta -->
<!-- \quad \mapsto \quad \mathcal{E}(Y_{-p+1}^T,\Phi_1,\dots,\Phi_p,\Theta), -->
<!-- \end{eqnarray*} -->
<!-- where $Y_{-p+1}^T=\{Y_{-p+1},\dots,Y_1,\dots,Y_T\}$. -->
<!-- Algorithm $\mathcal{E}$ is based on the Schur decomposition of $\Theta$ and on a mixture of forward and backward recursions.  -->
<!-- $\mathcal{E}(Y_{-p+1}^T,\Phi_1,\dots,\Phi_p,\Theta)$ converges to $\varepsilon_t$ in mean square. -->
<!-- Truncated log-likelihood: -->
<!-- \begin{equation} -->
<!-- \log \mathcal{L} (Y_{-p+1}^T;\Phi,\Theta, \Gamma) = - T \sum_{k=1}^n \log |\lambda_k| \mathbb{I}_{\{|\lambda_k| \ge 1\}} + \sum_{t=1}^{T} \log g(\mathcal{E}(Y_{-p+1}^T,\Phi,\Theta), \Gamma), (\#eq:VARMAapproxML) -->
<!-- \end{equation} -->
<!-- where the $\lambda_k$'s are the eigenvalues of $\Theta$ and where $g(.,\Gamma)$ is the p.d.f. of $\varepsilon_t$. -->
<!-- **Monte Carlo experiments** -->
<!-- VMA(1) process: $y_t = C \eta_t - \Theta C \eta_{t-1}$, with -->
<!-- \begin{equation} -->
<!-- C = \left[ -->
<!-- \begin{array}{cc} -->
<!-- 0 & 1 \\ -->
<!-- 1 & 0.5 -->
<!-- \end{array} -->
<!-- \right] \quad \mbox{and} \quad -->
<!-- \Theta = \left[ -->
<!-- \begin{array}{cc} -->
<!-- -0.5 & 0 \\ -->
<!-- 1 & -2 -->
<!-- \end{array} -->
<!-- \right], (\#eq:CTHETA) -->
<!-- \end{equation} -->
<!-- $\eta_{1,t}$ is drawn from a mixture of Gaussian (skewness of 2, excess kurtosis of 6), $\eta_{2,t}$, is drawn from a Student's distribution with 6 df. -->
<!-- The data generating process is non-fundamental. -->
<!-- Three sample sizes: $T=100$, 200 and 500. %Pseudo ML: in the (truncated) log-likelihood, the distributions of both $\eta_{1,t}$ and $\eta_{2,t}$ are mixtures of Gaussian. -->
<!-- Better small-sample performances for MLE than for 2SLS-GMM.  -->
</div>
<div id="factor-augmented-var-favar" class="section level3" number="1.3.10">
<h3>
<span class="header-section-number">1.3.10</span> Factor-Augmented VAR (FAVAR)<a class="anchor" aria-label="anchor" href="#factor-augmented-var-favar"><i class="fas fa-link"></i></a>
</h3>
<p>VAR models are subject to the curse of dimensionality: If <span class="math inline">\(n\)</span>, is large, then the number of parameters (in <span class="math inline">\(n^2\)</span>) explodes.</p>
<p>In the case where one suspects that the <span class="math inline">\(y_{i,t}\)</span>’s are mainly driven by a small number of random sources, a factor structure may be imposed, and <strong>principal component analysis</strong> (PCA, see Appendix <a href="append.html#PCAapp">2.1</a>) can be employed to estimate the relevant factors (<span class="citation">Bernanke, Boivin, and Eliasz (<a href="references.html#ref-Bernanke_Boivin_Eliasz_2005" role="doc-biblioref">2005</a>)</span>).</p>
<p>Let us denote by <span class="math inline">\(F_t\)</span> a <span class="math inline">\(k\)</span>-dimensional vector of latent factors accounting for important shares of the variances of the <span class="math inline">\(y_{i,t}\)</span>’s (with <span class="math inline">\(K \ll n\)</span>) and by <span class="math inline">\(x_t\)</span> is a small <span class="math inline">\(M\)</span>-dimensional subset of <span class="math inline">\(y_t\)</span> (with <span class="math inline">\(M \ll n\)</span>). The following factor structure is posited:
<span class="math display">\[
y_t = \Lambda^f F_t + \Lambda^x x_t + e_t,
\]</span>
where the <span class="math inline">\(e_t\)</span> are “small” serially and mutually i.i.d. error terms. That is <span class="math inline">\(F_t\)</span> and <span class="math inline">\(x_t\)</span> are supposed to drive most of the fluctuations of <span class="math inline">\(y_t\)</span>’s components.</p>
<p>The model is complemented by positing a VAR dynamics for <span class="math inline">\([F_t',x_t']'\)</span>:
<span class="math display" id="eq:FAVAR">\[\begin{equation}
\left[\begin{array}{c}F_t\\x_t\end{array}\right] = \Phi(L)\left[\begin{array}{c}F_{t-1}\\ x_{t-1}\end{array}\right] + v_t.\tag{1.52}
\end{equation}\]</span></p>
<p>Standard identification techniques of structural shocks can be employed in Eq. <a href="TS.html#eq:FAVAR">(1.52)</a>: Cholesky approach can be used for instance if the last component of <span class="math inline">\(x_t\)</span> is the short-term interest rate and if it is assumed that a MP shock has no contemporaneous impact on other macro-variables (in <span class="math inline">\(x_t\)</span>).</p>
<p>In their identification procedure, <span class="citation">Bernanke, Boivin, and Eliasz (<a href="references.html#ref-Bernanke_Boivin_Eliasz_2005" role="doc-biblioref">2005</a>)</span> exploit the fact that macro-finance variables can be decomposed in two sets —fast-moving and slow-moving variables— and that only the former reacts contemporaneously to monetary-policy shocks. Now, how to estimate the (unobserved) factors <span class="math inline">\(F_t\)</span>? <span class="citation">Bernanke, Boivin, and Eliasz (<a href="references.html#ref-Bernanke_Boivin_Eliasz_2005" role="doc-biblioref">2005</a>)</span> note that the first <span class="math inline">\(K+M\)</span> PCA of the whole dataset (<span class="math inline">\(y_t\)</span>), that they denote by <span class="math inline">\(\hat{C}(F_t,x_t)\)</span> should span the same space as <span class="math inline">\(F_t\)</span> and <span class="math inline">\(x_t)\)</span>. To get an estimate of <span class="math inline">\(F_t\)</span>, the dependence of <span class="math inline">\(\hat{C}(F_t,x_t)\)</span> in <span class="math inline">\(x_t)\)</span> has to be removed. This is done by regressing, by OLS, <span class="math inline">\(\hat{C}(F_t,x_t)\)</span> on <span class="math inline">\(x_t)\)</span> and on <span class="math inline">\(\hat{C}^*(F_t)\)</span>, where the latter is an estimate of the common components other than <span class="math inline">\(x_t\)</span>. To proxy for <span class="math inline">\(\hat{C}^*(F_t)\)</span>, <span class="citation">Bernanke, Boivin, and Eliasz (<a href="references.html#ref-Bernanke_Boivin_Eliasz_2005" role="doc-biblioref">2005</a>)</span> take principal components from the set of slow-moving variables, that are not comtemporaneously correlated to <span class="math inline">\(x_t\)</span>. Vector <span class="math inline">\(\hat{F}_t\)</span> is then computed as <span class="math inline">\(\hat{C}(F_t,x_t) - b_x x_t\)</span>, where <span class="math inline">\(b_x\)</span> are the coefficients coming from the previous OLS regressions.</p>
<p>Note that this approach implies that the vectorial space spanned by <span class="math inline">\((\hat{F}_t,x_t)\)</span> is the same as that spanned by <span class="math inline">\(\hat{C}(F_t,x_t)\)</span>.</p>
<p>Below, we employ this method on the dataset built by <span class="citation">McCracken and Ng (<a href="references.html#ref-McCracken_Ng_2016" role="doc-biblioref">2016</a>)</span> —the <a href="https://research.stlouisfed.org/wp/more/2015-012">FRED:MD</a> database— that includes 119 time series.</p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/nk027/bvar">BVAR</a></span><span class="op">)</span><span class="co"># contains the fred_md dataset</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.pfaffikus.de">vars</a></span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/BVAR/man/fred_transform.html">fred_transform</a></span><span class="op">(</span><span class="va">fred_md</span>,na.rm <span class="op">=</span> <span class="cn">FALSE</span>, type <span class="op">=</span> <span class="st">"fred_md"</span><span class="op">)</span></span>
<span><span class="va">First.date</span> <span class="op">&lt;-</span> <span class="st">"1959-02-01"</span></span>
<span><span class="va">Last.date</span> <span class="op">&lt;-</span> <span class="st">"2020-01-01"</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">&gt;</span><span class="va">First.date</span><span class="op">)</span><span class="op">&amp;</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">&lt;</span><span class="va">Last.date</span><span class="op">)</span>,<span class="op">]</span></span>
<span><span class="va">variables.with.na</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">data</span>,<span class="fl">2</span>,<span class="va">sum</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span>,<span class="op">-</span><span class="va">variables.with.na</span><span class="op">]</span></span>
<span><span class="va">data.values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/scale.html">scale</a></span><span class="op">(</span><span class="va">data</span>, center <span class="op">=</span> <span class="cn">TRUE</span>, scale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">data_scaled</span> <span class="op">&lt;-</span> <span class="va">data</span></span>
<span><span class="va">data_scaled</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,<span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">data.values</span></span>
<span><span class="va">K</span> <span class="op">&lt;-</span> <span class="fl">3</span></span>
<span><span class="va">M</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">PCA</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="va">data_scaled</span><span class="op">)</span> <span class="co"># implies that PCA$x %*% t(PCA$rotation) = data</span></span>
<span><span class="va">C.hat</span> <span class="op">&lt;-</span> <span class="va">PCA</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">K</span><span class="op">+</span><span class="va">M</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">fast_moving</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"HOUST"</span>,<span class="st">"HOUSTNE"</span>,<span class="st">"HOUSTMW"</span>,<span class="st">"HOUSTS"</span>,<span class="st">"HOUSTW"</span>,<span class="st">"HOUSTS"</span>,<span class="st">"AMDMNOx"</span>,</span>
<span>                 <span class="st">"FEDFUNDS"</span>,<span class="st">"CP3Mx"</span>,<span class="st">"TB3MS"</span>,<span class="st">"TB6MS"</span>,<span class="st">"GS1"</span>,<span class="st">"GS5"</span>,<span class="st">"GS10"</span>,</span>
<span>                 <span class="st">"COMPAPFFx"</span>,<span class="st">"TB3SMFFM"</span>,<span class="st">"TB6SMFFM"</span>,<span class="st">"T1YFFM"</span>,<span class="st">"T5YFFM"</span>,<span class="st">"T10YFFM"</span>,</span>
<span>                 <span class="st">"AAAFFM"</span>,<span class="st">"EXSZUSx"</span>,<span class="st">"EXJPUSx"</span>,<span class="st">"EXUSUKx"</span>,<span class="st">"EXCAUSx"</span><span class="op">)</span></span>
<span><span class="va">data.slow</span> <span class="op">&lt;-</span> <span class="va">data_scaled</span><span class="op">[</span>,<span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">fast_moving</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">PCA.star</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="va">data.slow</span><span class="op">)</span> <span class="co"># implies that PCA$x %*% t(PCA$rotation) = data</span></span>
<span><span class="va">C.hat.star</span> <span class="op">&lt;-</span> <span class="va">PCA.star</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="va">K</span><span class="op">]</span></span>
<span><span class="va">D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">FEDFUNDS</span>,<span class="va">C.hat.star</span><span class="op">)</span></span>
<span><span class="va">b.x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">D</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">C.hat</span></span>
<span><span class="va">F.hat</span> <span class="op">&lt;-</span> <span class="va">C.hat</span> <span class="op">-</span> <span class="va">data</span><span class="op">$</span><span class="va">FEDFUNDS</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">b.x</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span>,nrow<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">data_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">F.hat</span>, FEDFUNDS <span class="op">=</span> <span class="va">data</span><span class="op">$</span><span class="va">FEDFUNDS</span><span class="op">)</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span><span class="va">var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/vars/man/VAR.html">VAR</a></span><span class="op">(</span><span class="va">data_var</span>, <span class="va">p</span><span class="op">)</span></span>
<span><span class="va">Omega</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">var</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">B</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/chol.html">chol</a></span><span class="op">(</span><span class="va">Omega</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">F.hat</span>,<span class="va">data</span><span class="op">$</span><span class="va">FEDFUNDS</span><span class="op">)</span></span>
<span><span class="va">loadings</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span><span class="va">D</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">data_scaled</span><span class="op">)</span></span>
<span><span class="va">irf</span> <span class="op">&lt;-</span> <span class="fu">simul.VAR</span><span class="op">(</span>c<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="op">(</span><span class="va">K</span><span class="op">+</span><span class="va">M</span><span class="op">)</span><span class="op">*</span><span class="va">p</span><span class="op">)</span>,Phi<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/vars/man/A.html">Acoef</a></span><span class="op">(</span><span class="va">var</span><span class="op">)</span>,<span class="va">B</span>,nb.sim<span class="op">=</span><span class="fl">120</span>,</span>
<span>                 y0.star<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="op">(</span><span class="va">K</span><span class="op">+</span><span class="va">M</span><span class="op">)</span><span class="op">*</span><span class="va">p</span><span class="op">)</span>,indic.IRF <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                 u.shock <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="va">K</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">irf.all</span> <span class="op">&lt;-</span> <span class="va">irf</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">loadings</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">variables.2.plot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"FEDFUNDS"</span>,<span class="st">"INDPRO"</span>,<span class="st">"UNRATE"</span>,<span class="st">"CPIAUCSL"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.2</span>,<span class="fl">.95</span>,<span class="fl">.3</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">variables.2.plot</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">irf.all</span><span class="op">[</span>,<span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">variables.2.plot</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">==</span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">)</span><span class="op">]</span><span class="op">)</span>,lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>       type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">"months after shock"</span>,ylab<span class="op">=</span><span class="va">variables.2.plot</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:FAVAR"></span>
<img src="TimeSeries_files/figure-html/FAVAR-1.png" alt="Responses of a monetary-policy shock. FAVAR approach of Bernanke, Boivin, and Eliasz (2005). FRED-MD dataset." width="95%"><p class="caption">
Figure 1.23: Responses of a monetary-policy shock. FAVAR approach of Bernanke, Boivin, and Eliasz (2005). FRED-MD dataset.
</p>
</div>
</div>
<div id="Projections" class="section level3" number="1.3.11">
<h3>
<span class="header-section-number">1.3.11</span> Projection Methods<a class="anchor" aria-label="anchor" href="#Projections"><i class="fas fa-link"></i></a>
</h3>
<p>Consider the infinite MA representation of <span class="math inline">\(y_t\)</span> (Eq. <a href="TS.html#eq:InfMA">(1.34)</a>):
<span class="math display">\[
y_t = \mu + \sum_{h=0}^\infty \Psi_{h} \eta_{t-h}.
\]</span>
As seen in Section <a href="TS.html#IRFSVARMA">1.3.2</a>, the entries <span class="math inline">\((i,j)\)</span> of the sequence of the <span class="math inline">\(\Psi_h\)</span> matrices define the IRF of <span class="math inline">\(\eta_{j,t}\)</span> on <span class="math inline">\(y_{i,t}\)</span>.</p>
<p>Assume that you observe <span class="math inline">\(\eta_{j,t}\)</span>, then a consistent estimate of <span class="math inline">\(\Psi_{i,j,h}\)</span> is simply obtained by the OLS regression of <span class="math inline">\(y_{i,t+h}\)</span> on <span class="math inline">\(\eta_{j,t}\)</span>:
<span class="math display" id="eq:OLS1">\[\begin{equation}
y_{i,t+h} = \mu_i + \Psi_{i,j,h}\eta_{j,t} + u_{i,j,t+h}.\tag{1.53}
\end{equation}\]</span>
Because the residuals <span class="math inline">\(u_{i,j,t+h}\)</span> are autocorrelated (for <span class="math inline">\(h&gt;0\)</span>), estimates of the covariance of the OLS estimators of the <span class="math inline">\(\Psi_{i,j,h}\)</span> then have to be based on robust estimators (e.g. Newey-West, see Eq. <a href="#eq:NW">(<strong>??</strong>)</a>). This is the core idea of the <strong>local projection approach</strong> proposed by <span class="citation">Jordà (<a href="references.html#ref-Jorda_2005" role="doc-biblioref">2005</a>)</span>.</p>
<p>Now, how to proceed in the (usual) case where <span class="math inline">\(\eta_{j,t}\)</span> is not observed? We consider two situations.</p>
<p><strong>Situation A: Without IV</strong></p>
<p>This corresponds to the original <span class="citation">Jordà (<a href="references.html#ref-Jorda_2005" role="doc-biblioref">2005</a>)</span>’s approach.</p>
<p>Assume that the structural shock of interest (<span class="math inline">\(\eta_{1,t}\)</span>, say) can be consistently obtained as the residual of a regression of a variable <span class="math inline">\(x_t\)</span> on a set of control variables <span class="math inline">\(w_t\)</span> independent from <span class="math inline">\(\eta_{1,t}\)</span>:
<span class="math display" id="eq:xetaw">\[\begin{equation}
\eta_{1,t} = x_t - \mathbb{E}(x_t|w_t),\tag{1.54}
\end{equation}\]</span>
where <span class="math inline">\(\mathbb{E}(x_t|w_t)\)</span> is affine in <span class="math inline">\(w_t\)</span> and where <span class="math inline">\(w_t\)</span> is an affine transformation of <span class="math inline">\(\eta_{2:n,t}\)</span> and of past shocks <span class="math inline">\(\eta_{t-1},\eta_{t-2},\dots\)</span>.</p>
<p>Eq. <a href="TS.html#eq:xetaw">(1.54)</a> implies that, conditional on <span class="math inline">\(w_t\)</span>, the additional knowledge of <span class="math inline">\(x_t\)</span> is useful only when it comes to forecast something that depends on <span class="math inline">\(\eta_{1,t}\)</span>. Hence, given that <span class="math inline">\(u_{i,1,t+h}\)</span> (see Eq. <a href="TS.html#eq:OLS1">(1.53)</a>) is independent from <span class="math inline">\(\eta_{1,t}\)</span> (it depends on <span class="math inline">\(\eta_{t+h},\dots,\eta_{t+1},\color{blue}{\eta_{2:n,t}},\eta_{t-1},\eta_{t-2},\dots\)</span>), it comes that
<span class="math display">\[
\mathbb{E}(u_{i,1,t+h}|x_t,w_t)= \mathbb{E}(u_{i,1,t+h}|w_t).
\]</span>
This is the <em>conditional mean independence</em> case.</p>
<p>Let’s rewrite Eq. <a href="TS.html#eq:OLS1">(1.53)</a> as follows:
<span class="math display">\[\begin{eqnarray*}
y_{i,t+h} &amp;=&amp; \mu_i + \Psi_{i,1,h}\eta_{1,t} + u_{i,1,t+h}\\
&amp;=&amp;  \mu_i + \Psi_{i,1,h}x_t  \color{blue}{-\Psi_{i,1,h}\mathbb{E}(x_t|w_t) + u_{i,1,t+h}},
\end{eqnarray*}\]</span></p>
<p>What precedes implies that the expectation of the blue term, conditional on <span class="math inline">\(x_t\)</span> and <span class="math inline">\(w_t\)</span>, is linear in <span class="math inline">\(w_t\)</span>. Standard results in the conditional mean independence case imply that the regression of <span class="math inline">\(y_{i,t+h}\)</span> on <span class="math inline">\(x_t\)</span>, controlling for <span class="math inline">\(w_t\)</span>, provides a consistent estimate of <span class="math inline">\(\Psi_{i,1,h}\)</span>:
<span class="math display">\[\begin{equation}
y_{i,t+h} = \alpha_i + \Psi_{i,1,h}x_t + \beta'w_t + v_{i,t+h}.
\end{equation}\]</span></p>
<p>This is for instance consistent with the case where <span class="math inline">\([\Delta GDP_t, \pi_t,i_t]'\)</span> follows a VAR(1) and the monetary-policy shock do not contemporaneously affect <span class="math inline">\(\Delta GDP_t\)</span> and <span class="math inline">\(\pi_t\)</span>.</p>
<p>The IRFs can be estimated by LP, taking <span class="math inline">\(x_t = i_t\)</span> and <span class="math inline">\(w_t = [\Delta GDP_t,\pi_t,\Delta GDP_{t-1}, \pi_{t-1},i_{t-1}]'\)</span>.</p>
<p>This approach closely relates to the SVAR Cholesky-based identification approach. Specifically, if <span class="math inline">\(w_t = [{\color{blue}y_{1,t},\dots,y_{k-1,t}}, y_{t-1}',\dots,y_{t-p}']'\)</span>, with <span class="math inline">\(k\le n\)</span>, and <span class="math inline">\(x_t = y_{k,t}\)</span>, then this approach corresponds, for <span class="math inline">\(h=0\)</span>, to the SVAR(<span class="math inline">\(p\)</span>) Cholesky-based IRF (focusing on the responses to the <span class="math inline">\(k^{th}\)</span> structural shock). However, the two approaches differ for <span class="math inline">\(h&gt;0\)</span>, because the LP methodology does not assumes a VAR dynamics for <span class="math inline">\(y_t\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;This is reminiscent of the distinction betweem direct forecasting –based on regressions of &lt;span class="math inline"&gt;\(y_{t+h}\)&lt;/span&gt; on &lt;span class="math inline"&gt;\(\{y_t,y_{t-1},\dots\}\)&lt;/span&gt;– and iterated forecasting –based on a recursive model where &lt;span class="math inline"&gt;\(y_{t+1} = g(y_t,y_{t-1},\dots)+\varepsilon_{t+1}\)&lt;/span&gt;, see &lt;span class="citation"&gt;Marcellino, Stock, and Watson (&lt;a href="references.html#ref-Marcellino_et_al_2006" role="doc-biblioref"&gt;2006&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;'><sup>4</sup></a></p>
<p><strong>Situation B: IV approach</strong></p>
<p>Consider now that we have a valid instrument <span class="math inline">\(z_t\)</span> for <span class="math inline">\(\eta_{1,t}\)</span> (with <span class="math inline">\(\mathbb{E}(z_t)=0\)</span>). That is:
<span class="math display" id="eq:IV1">\[\begin{equation}
\left\{
\begin{array}{llll}
(IV.i) &amp; \mathbb{E}(z_t \eta_{1,t}) &amp;\ne 0 &amp; \mbox{(relevance condition)} \\
(IV.ii) &amp; \mathbb{E}(z_t \eta_{j,t}) &amp;= 0 \quad \mbox{for } j&gt;1 &amp; \mbox{(exogeneity condition)}
\end{array}\right.\tag{1.55}
\end{equation}\]</span>
The instrument <span class="math inline">\(z_t\)</span> can be used to identify the structural shock. Eq. <a href="TS.html#eq:IV1">(1.55)</a> implies that there exist <span class="math inline">\(\rho \ne 0\)</span> and a mean-zero variable <span class="math inline">\(\xi_t\)</span> such that:
<span class="math display">\[
\eta_{1,t} = \rho z_t + \xi_t,
\]</span>
where <span class="math inline">\(\xi_t\)</span> is correlated neither to <span class="math inline">\(z_t\)</span>, nor to <span class="math inline">\(\eta_{j,t}\)</span>, <span class="math inline">\(j\ge2\)</span>.</p>
<div class="proof">
<p><span id="unlabeled-div-16" class="proof"><em>Proof</em>. </span>Define <span class="math inline">\(\rho = \frac{\mathbb{E}(\eta_{1,t}z_t)}{\mathbb{V}ar(z_t)}\)</span> and <span class="math inline">\(\xi_t = \eta_{1,t} - \rho z_t\)</span>. It is easily seen that <span class="math inline">\(\xi_t\)</span> satisfies the moment restrictions given above.</p>
</div>
<p><span class="citation">Ramey (<a href="references.html#ref-Ramey_2016_NBER" role="doc-biblioref">2016</a>)</span> reviews the different approaches employed to construct monetary policy-shocks (the two main approaches are presented in <a href="TS.html#exm:HighFreq">1.9</a> and <a href="TS.html#exm:RomerRomer">1.10</a> below). She has also collected time series of such shocks, see <a href="https://econweb.ucsd.edu/~vramey/research.html#mon">her website</a>.</p>
<div class="example">
<p><span id="exm:HighFreq" class="example"><strong>Example 1.9  (Identification of Monetary-Policy Shocks Based on High-Frequency Data) </strong></span>Instruments for monetary-policy shocks can be extracted from high-frequency market data associated with interest-rate products.</p>
<p>The quotes of all interest-rate-related financial products are sensitive to monetary-policy announcements. That is because these quotes mainly depends on investors’ expectations regarding future short-term rates: <span class="math inline">\(\mathbb{E}_t(i_{t+s})\)</span>. Typically, if agents were risk-neutral, the maturity-<span class="math inline">\(h\)</span> interest rate would approximatively be given by:
<span class="math display">\[
i_{t,h} \approx \mathbb{E}_t\left(\frac{1}{h}\int_{0}^{h} i_{t+s} ds\right) = \frac{1}{h}\int_{0}^{h} \mathbb{E}_t\left(i_{t+s}\right) ds.
\]</span>
In general, changes in <span class="math inline">\(\mathbb{E}_t(i_{t+s})\)</span>, for <span class="math inline">\(s&gt;0\)</span>, can be affected by all types of shocks that may trigger a reaction by the central bank.</p>
<p>However, if a MP announcement takes place between <span class="math inline">\(t\)</span> and <span class="math inline">\(t+\epsilon\)</span>, then most of <span class="math inline">\(\mathbb{E}_{t+\epsilon}(i_{t+s})-\mathbb{E}_t(i_{t+s})\)</span> is to be attributed to the MP shock (see Figure <a href="TS.html#fig:HighFreq">1.24</a>, from <span class="citation">Gürkaynak, Sack, and Swanson (<a href="references.html#ref-Gurkaynak_et_al_2005" role="doc-biblioref">2005</a>)</span>). Hence, a monthly time series of MP shocks can be obtained by summing, over each month, the changes <span class="math inline">\(i_{t+ \epsilon,h} - i_{t,h}\)</span> associated with a given interest rate (T-bills, futures, swaps) and a given maturity <span class="math inline">\(h\)</span>.</p>
<p>See among others: <span class="citation">Kuttner (<a href="references.html#ref-KUTTNER2001523" role="doc-biblioref">2001</a>)</span>, <span class="citation">Cochrane and Piazzesi (<a href="references.html#ref-Cochrane_Piazzesi_2002" role="doc-biblioref">2002</a>)</span>,<span class="citation">Gürkaynak, Sack, and Swanson (<a href="references.html#ref-Gurkaynak_et_al_2005" role="doc-biblioref">2005</a>)</span>, <span class="citation">Piazzesi and Swanson (<a href="references.html#ref-Piazzesi_Swanson_2008" role="doc-biblioref">2008</a>)</span>, <span class="citation">Gertler and Karadi (<a href="references.html#ref-Gertler_Karadi_2015" role="doc-biblioref">2015</a>)</span>.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:HighFreq"></span>
<img src="images/GSS2005_HFI.png" alt="Source: Gurkaynak, Sack and Swanson (2005). Transaction rates of Federal funds futures on June 25, 2003, day on which a regularly scheduled FOMC meeting was scheduled. At 2:15 p.m., the FOMC announced that it was lowering its target for the federal funds rate from 1.25\% to 1\%, while many market participants were expecting a 50 bp cut. This shows that (i) financial markets seem to fully adjust to the policy action within just a few minutes and (ii) the federal funds rate surprise is not necessarily in the same direction as the federal funds rate action itself." width="95%"><p class="caption">
Figure 1.24: Source: Gurkaynak, Sack and Swanson (2005). Transaction rates of Federal funds futures on June 25, 2003, day on which a regularly scheduled FOMC meeting was scheduled. At 2:15 p.m., the FOMC announced that it was lowering its target for the federal funds rate from 1.25% to 1%, while many market participants were expecting a 50 bp cut. This shows that (i) financial markets seem to fully adjust to the policy action within just a few minutes and (ii) the federal funds rate surprise is not necessarily in the same direction as the federal funds rate action itself.
</p>
</div>
</div>
<div class="example">
<p><span id="exm:RomerRomer" class="example"><strong>Example 1.10  (Identification of Monetary-Policy Shocks Based on the Narrative Approach) </strong></span><span class="citation">Romer and Romer (<a href="references.html#ref-Romer_Romer_2004" role="doc-biblioref">2004</a>)</span> propose a two-step approach:</p>
<ol style="list-style-type: lower-alpha">
<li>derive a series for Federal Reserve intentions for the federal funds rate (the explicit target of the Fed) around FOMC meetings,</li>
<li>control for Federal Reserve forecasts.</li>
</ol>
<p>This gives a measure of intended monetary policy actions not driven by information about future economic developments.
a. “intentions” are measured as a combination of narrative and quantitative evidence. Sources: (among others) Minutes of FOMC and “Blue Books”.
b. Controls = variables spanning the information the Federal Reserve has about future developments. Data: Federal Reserve’s internal forecasts (inflation, real output and unemployment), “Greenbook’s forecasts” – usually issued 6 days before the FOMC meeting.</p>
<p>The shock measure is the residual series in the linear regression of (a) on (b).</p>
</div>
<p>There are two main IV approaches to estimate IRFs see <span class="citation">James H. Stock and Watson (<a href="references.html#ref-Stock_Watson_2018" role="doc-biblioref">2018</a>)</span>:</p>
<ol style="list-style-type: lower-alpha">
<li>The LP-IV approach, where <span class="math inline">\(y_t\)</span>’s DGP is left unspecified,</li>
<li>The SVAR-IV approach.</li>
</ol>
<p>The LP-IV approach is based on a set of IV regressions (for each variable of interest, one for each forecast horizon). The SVAR-IV approach is based on IV regressions of VAR innovations only (one for each series of VAR innovations).</p>
<p>If the VAR adequately captures the DGP, then the IV-SVAR is optimal for all horizons. However, if the VAR is misspecified, then specification errors are compounded at each horizon and a local projection method would lead to better results.</p>
<p><strong>Situation B.1: SVAR-IV approach</strong></p>
<p>Assume you have consistent estimates of <span class="math inline">\(\varepsilon_t = B\eta_t\)</span>, these estimates (<span class="math inline">\(\hat\varepsilon_{t}\)</span>) coming from the estimation of a VAR model. You have, for <span class="math inline">\(i \in \{1,\dots,n\}\)</span>:
<span class="math display">\[\begin{eqnarray}
\varepsilon_{i,t} &amp;=&amp; b_{i,1} \eta_{1,t} + u_{i,t} (\#eq:eps_rho)\\
&amp;=&amp; b_{i,1} \rho z_t + \underbrace{b_{i,1}\xi_t + u_{i,t}}_{\perp z_t}. \nonumber
\end{eqnarray}\]</span>
(<span class="math inline">\(u_{i,t}\)</span> is a linear combination of the <span class="math inline">\(\eta_{j,t}\)</span>’s, <span class="math inline">\(j\ge2\)</span>).</p>
<p>Hence, up to a multiplicative factor (<span class="math inline">\(\rho\)</span>), the (OLS) regressions of the <span class="math inline">\(\hat\varepsilon_{i,t}\)</span>’s on <span class="math inline">\(z_t\)</span> provide consistent estimates of the <span class="math inline">\(b_{i,1}\)</span>’s.</p>
<p>Combined with the estimated VAR (the <span class="math inline">\(\Phi_k\)</span> matrices), this provides consistent estimates of the IRFs of <span class="math inline">\(\eta_{1,t}\)</span> on <span class="math inline">\(y_t\)</span>, though up to a multiplicative factor. This scale ambiguity can be solved by rescaling the structural shock (“unit-effect normalisation”, see <span class="citation">James H. Stock and Watson (<a href="references.html#ref-Stock_Watson_2018" role="doc-biblioref">2018</a>)</span>). Let us consider <span class="math inline">\(\tilde\eta_{1,t}=b_{1,1}\eta_{1,t}\)</span>; by construction, <span class="math inline">\(\tilde\eta_{1,t}\)</span> has a one-unit contemporaneous effect on <span class="math inline">\(y_{1,t}\)</span>. Denoting by <span class="math inline">\(\tilde{B}_{i,1}\)</span> the contemporaneous impact of <span class="math inline">\(\tilde\eta_{1,t}\)</span>, we get:
<span class="math display">\[
\tilde{B}_{1} = \frac{1}{b_{1,1}} {B}_{1},
\]</span>
where <span class="math inline">\(B_{1}\)</span> denotes the <span class="math inline">\(1^{st}\)</span> column of <span class="math inline">\(B\)</span> and <span class="math inline">\(\tilde{B}_{1}=[1,\tilde{B}_{2,1},\dots,\tilde{B}_{n,1}]'\)</span>.</p>
<p>Eq. @ref(eq:eps_rho) gives:
<span class="math display">\[\begin{eqnarray*}
\varepsilon_{1,t} &amp;=&amp; \tilde\eta_{1,t} + u_{1,t}\\
\varepsilon_{i,t} &amp;=&amp; \tilde{B}_{i,1} \tilde\eta_{1,t} + u_{i,t}.
\end{eqnarray*}\]</span>
This suggests that <span class="math inline">\(\tilde{B}_{i,1}\)</span> can be estimated by regressing <span class="math inline">\(\varepsilon_{i,t}\)</span> on <span class="math inline">\(\varepsilon_{1,t}\)</span>, using <span class="math inline">\(z_t\)</span> as an instrument.</p>
<p>What about inference? Once cannot use the usual TSLS standard deviations because the <span class="math inline">\(\varepsilon_{i,t}\)</span>’s are not directly observed. Bootstrap procedures can be resorted to. <span class="citation">James H. Stock and Watson (<a href="references.html#ref-Stock_Watson_2018" role="doc-biblioref">2018</a>)</span> propose, in particular, a Gaussian parametric bootstrap:</p>
<p>Assume you have estimated <span class="math inline">\(\{\widehat{\Phi}_1,\dots,\widehat{\Phi}_p,\widehat{B}_1\}\)</span> using the SVAR-IV approach based on a size-<span class="math inline">\(T\)</span> sample. Generate <span class="math inline">\(N\)</span> (where <span class="math inline">\(N\)</span> is large) size-<span class="math inline">\(T\)</span> samples from the following VAR:
<span class="math display">\[
\left[
\begin{array}{cc}
\widehat{\Phi}(L) &amp; 0 \\
0 &amp; \widehat{\rho}(L)
\end{array}
\right]
\left[
\begin{array}{c}
y_t \\
z_t
\end{array}
\right] =
\left[
\begin{array}{c}
\varepsilon_t \\
e_t
\end{array}
\right],
\]</span>
<span class="math display">\[
\mbox{where} \quad \left[
\begin{array}{c}
\varepsilon_t \\
e_t
\end{array}
\right]\sim \, i.i.d.\,\mathcal{N}\left(\left[\begin{array}{c}0\\0\end{array}\right],
\left[\begin{array}{cc}
\Omega &amp; S'_{\varepsilon,e}\\
S_{\varepsilon,e}&amp; \sigma^2_{e}
\end{array}\right]
\right),
\]</span>
where <span class="math inline">\(\widehat{\rho}(L)\)</span> and <span class="math inline">\(\sigma^2_{e}\)</span> result from the estimation of an AR process for <span class="math inline">\(z_t\)</span>, and where <span class="math inline">\(\Omega\)</span> and <span class="math inline">\(S_{\varepsilon,e}\)</span> are sample covariances for the VAR/AR residuals.</p>
<p>For each simulated sample (of <span class="math inline">\(\tilde{y}_t\)</span> and <span class="math inline">\(\tilde{z}_t\)</span>, say), estimate <span class="math inline">\(\{\widetilde{\widehat{\Phi}}_1,\dots,\widetilde{\widehat{\Phi}}_p,\widetilde{\widehat{B}}_1\}\)</span> and associated <span class="math inline">\(\widetilde{\Psi}_{i,1,h}\)</span>. This provides e.g. a sequence of <span class="math inline">\(N\)</span> estimates of <span class="math inline">\(\Psi_{i,1,h}\)</span>, from which quantiles and conf. intervals can be deduced.</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load vars package:</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.pfaffikus.de">vars</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"USmonthly"</span><span class="op">)</span></span>
<span><span class="va">First.date</span> <span class="op">&lt;-</span> <span class="st">"1990-05-01"</span></span>
<span><span class="va">Last.date</span> <span class="op">&lt;-</span> <span class="st">"2012-6-01"</span></span>
<span><span class="va">indic.first</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">USmonthly</span><span class="op">$</span><span class="va">DATES</span><span class="op">==</span><span class="va">First.date</span><span class="op">)</span></span>
<span><span class="va">indic.last</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">USmonthly</span><span class="op">$</span><span class="va">DATES</span><span class="op">==</span><span class="va">Last.date</span><span class="op">)</span></span>
<span><span class="va">USmonthly</span>   <span class="op">&lt;-</span> <span class="va">USmonthly</span><span class="op">[</span><span class="va">indic.first</span><span class="op">:</span><span class="va">indic.last</span>,<span class="op">]</span></span>
<span><span class="va">shock.name</span> <span class="op">&lt;-</span> <span class="st">"FF4_TC"</span> <span class="co">#"FF4_TC", "ED2_TC", "ff1_vr", "rrshock83b"</span></span>
<span><span class="va">indic.shock.name</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">USmonthly</span><span class="op">)</span><span class="op">==</span><span class="va">shock.name</span><span class="op">)</span></span>
<span><span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">USmonthly</span><span class="op">[</span>,<span class="va">indic.shock.name</span><span class="op">]</span>,ncol<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.95</span>,<span class="fl">.1</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span><span class="op">(</span><span class="va">USmonthly</span><span class="op">$</span><span class="va">DATES</span>,<span class="va">Z</span>,type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:essaiIV0"></span>
<img src="TimeSeries_files/figure-html/essaiIV0-1.png" alt="Gertler-Karadi monthly shocks, fed funds futures 3 months." width="95%"><p class="caption">
Figure 1.25: Gertler-Karadi monthly shocks, fed funds futures 3 months.
</p>
</div>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">considered.variables</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"GS1"</span>,<span class="st">"LIP"</span>,<span class="st">"LCPI"</span>,<span class="st">"EBP"</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">USmonthly</span><span class="op">[</span>,<span class="va">considered.variables</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">considered.variables</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">considered.variables</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.15</span>,<span class="fl">.95</span>,<span class="fl">.15</span>,<span class="fl">.8</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">res.svar.iv</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">svar.iv</span><span class="op">(</span><span class="va">y</span>,<span class="va">Z</span>,p <span class="op">=</span> <span class="fl">4</span>,names.of.variables<span class="op">=</span><span class="va">considered.variables</span>,</span>
<span>          nb.periods.IRF <span class="op">=</span> <span class="fl">20</span>,</span>
<span>          z.AR.order<span class="op">=</span><span class="fl">1</span>, </span>
<span>          nb.bootstrap.replications <span class="op">=</span> <span class="fl">100</span>, </span>
<span>          confidence.interval <span class="op">=</span> <span class="fl">0.90</span>,</span>
<span>          indic.plot<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:essaiIV1"></span>
<img src="TimeSeries_files/figure-html/essaiIV1-1.png" alt="Reponses to a monetary-policy shock, SVAR-IV approach." width="95%"><p class="caption">
Figure 1.26: Reponses to a monetary-policy shock, SVAR-IV approach.
</p>
</div>
<p><strong>Situation B.2: LP-IV</strong></p>
<p>If you do not want to posit a VAR-type dynamics for <span class="math inline">\(y_t\)</span> –e.g. because you suspect that the true generating model may be a non-invertible VARMA model– you can directly proceed by IV-projection methods to obtain the <span class="math inline">\(\tilde\Psi_{i,1,h}\equiv \Psi_{i,1,h}/b_{1,1}\)</span> (that are the IRFs of <span class="math inline">\(\tilde\eta_{1,t}\)</span> on <span class="math inline">\(y_{i,t}\)</span>).</p>
<p>However, Assumptions (IV.i) and (IV.ii) (Eq. <a href="TS.html#eq:IV1">(1.55)</a>) have to be complemented with (IV.iii):
<span class="math display">\[\begin{equation*}
\begin{array}{llll}
(IV.iii) &amp; \mathbb{E}(z_t \eta_{j,t+h}) &amp;= 0 \, \mbox{ for } h \ne 0 &amp; \mbox{(lead-lag exogeneity)}
\end{array}
\end{equation*}\]</span></p>
<p>When (IV.i), (IV.ii) and (IV.iii) are satisfied, <span class="math inline">\(\tilde\Psi_{i,1,h}\)</span> can be estimated by regressing <span class="math inline">\(y_{i,t+h}\)</span> on <span class="math inline">\(y_{1,t}\)</span>, using <span class="math inline">\(z_t\)</span> as an instrument, i.e. by considering the TSLS estimation of:
<span class="math display" id="eq:regIV1">\[\begin{equation}
y_{i,t+h} = \alpha_i + \tilde\Psi_{i,1,h}y_{1,t} + \nu_{i,t+h},\tag{1.56}
\end{equation}\]</span>
where <span class="math inline">\(\nu_{i,t+h}\)</span> is correlated to <span class="math inline">\(y_{1,t}\)</span>, but not to <span class="math inline">\(z_t\)</span>.</p>
<p>We have indeed:
<span class="math display">\[\begin{eqnarray*}
y_{1,t} &amp;=&amp; \alpha_1 + \tilde\eta_{1,t} + v_{1,t}\\
y_{i,t+h} &amp;=&amp; \alpha_i + \tilde\Psi_{i,1,h}\tilde\eta_{1,t} + v_{i,t+h},
\end{eqnarray*}\]</span>
where the <span class="math inline">\(v_{i,t+h}\)</span>’s are uncorrelated to <span class="math inline">\(z_t\)</span> under (IV.i), (IV.ii) and (IV.iii).</p>
<p>Note again that, for <span class="math inline">\(h&gt;0\)</span>, the <span class="math inline">\(v_{i,t+h}\)</span> (and <span class="math inline">\(\nu_{i,t+h}\)</span>) are auto-correlated. Newey-West corrections therefore have to be used to compute std errors of the <span class="math inline">\(\tilde\Psi_{i,1,h}\)</span>’s estimates.</p>
<p>Consider the linear regression:
<span class="math display">\[
\mathbf{Y} = \mathbf{X}\boldsymbol\beta + \boldsymbol\varepsilon,
\]</span>
where <span class="math inline">\(\mathbb{E}(\boldsymbol\varepsilon)=0\)</span>, but where the explicative variables <span class="math inline">\(\mathbf{X}\)</span> are supposed to be correlated to the residuals <span class="math inline">\(\boldsymbol\varepsilon\)</span>.</p>
<p>Moreover, the <span class="math inline">\(\boldsymbol\varepsilon\)</span> are supposed to be possibly heteroskedastic and auto-correlated.</p>
<p>We consider the instruments <span class="math inline">\(\mathbf{Z}\)</span>, with <span class="math inline">\(\mathbb{E}(\mathbf{X}'\mathbf{Z}) \ne 0\)</span> but <span class="math inline">\(\mathbb{E}(\boldsymbol\varepsilon'\mathbf{Z}) = 0\)</span>.</p>
<p>The IV estimator of <span class="math inline">\(\boldsymbol\beta\)</span> is obtained by regressing <span class="math inline">\(\hat{\mathbf{Y}}\)</span> on <span class="math inline">\(\hat{\mathbf{X}}\)</span>, where <span class="math inline">\(\hat{\mathbf{Y}}\)</span> and <span class="math inline">\(\hat{\mathbf{X}}\)</span> are the respective residuals of the regressions of <span class="math inline">\(\mathbf{Y}\)</span> and <span class="math inline">\(\mathbf{X}\)</span> on <span class="math inline">\(\mathbf{Z}\)</span>.
<span class="math display">\[\begin{eqnarray*}
\mathbf{b}_{iv} &amp;=&amp; [\mathbf{X}'\mathbf{Z}(\mathbf{Z}'\mathbf{Z})^{-1}\mathbf{Z}'\mathbf{X}]^{-1}\mathbf{X}'\mathbf{Z}(\mathbf{Z}'\mathbf{Z})^{-1}\mathbf{Z}'\mathbf{Y}\\
\mathbf{b}_{iv} &amp;=&amp; \boldsymbol\beta + \frac{1}{\sqrt{T}}\underbrace{T[\mathbf{X}'\mathbf{Z}(\mathbf{Z}'\mathbf{Z})^{-1}\mathbf{Z}'\mathbf{X}]^{-1}\mathbf{X}'\mathbf{Z}(\mathbf{Z}'\mathbf{Z})^{-1}}_{=Q(\mathbf{X},\mathbf{Z}) \overset{p}{\rightarrow} \mathbf{Q}_{xz}}\underbrace{\sqrt{T}\left(\frac{1}{T}\mathbf{Z}'\boldsymbol\varepsilon\right)}_{\overset{d}{\rightarrow} \mathcal{N}(0,S)},
\end{eqnarray*}\]</span>
where <span class="math inline">\(\mathbf{S}\)</span> is the long-run variance of <span class="math inline">\(\mathbf{z}_t\varepsilon_t\)</span> (see next slide).</p>
<p>The asymptotic covariance matrix of <span class="math inline">\(\sqrt{T}\mathbf{b}_{iv}\)</span> is <span class="math inline">\(\mathbf{Q}_{xz} \mathbf{S} \mathbf{Q}_{xz}'\)</span>.</p>
<p>The covariance matrix of <span class="math inline">\(\mathbf{b}_{iv}\)</span> can be approximated by <span class="math inline">\(\frac{1}{T}Q(\mathbf{X},\mathbf{Z})\hat{\mathbf{S}}Q(\mathbf{X},\mathbf{Z})'\)</span> where <span class="math inline">\(\hat{\mathbf{S}}\)</span> is the Newey-West estimator of <span class="math inline">\(\mathbf{S}\)</span> (see Eq. <a href="#eq:NW">(<strong>??</strong>)</a>)</p>
<p>(IV.iii) is usually not restrictive for <span class="math inline">\(h&gt;0\)</span> (<span class="math inline">\(z_t\)</span> is usually not affected by future shocks). By contrast, it may be restrictive for <span class="math inline">\(h&lt;0\)</span>. This can be solved by adding controls in Regression <a href="TS.html#eq:regIV1">(1.56)</a>. These controls should span the space of <span class="math inline">\(\{\eta_{t-1},\eta_{t-2},\dots\}\)</span>.</p>
<p>If <span class="math inline">\(z_t\)</span> is suspected to be correlated to past values of <span class="math inline">\(\eta_{1,t}\)</span> but not to the <span class="math inline">\(\eta_{j,t}\)</span>’s, <span class="math inline">\(j&gt;1\)</span>, then one can add lags of <span class="math inline">\(z_t\)</span> as controls (method e.g. advocated by Ramey, 2016, p.108, considering the instrument by <span class="citation">Gertler and Karadi (<a href="references.html#ref-Gertler_Karadi_2015" role="doc-biblioref">2015</a>)</span>).</p>
<p>In the general case, one can use lags of <span class="math inline">\(y_t\)</span> as controls. Note that, even if (IV.iii) holds, adding controls may reduce the variance of the regression error.</p>
<p>As noted by <span class="citation">James H. Stock and Watson (<a href="references.html#ref-Stock_Watson_2018" role="doc-biblioref">2018</a>)</span>, the relevant variance is the long-run variance of the instrument-times-error term. They also recommend (p.926) using leads and lags of <span class="math inline">\(z_t\)</span> to improve efficiency.</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">res.LP.IV</span> <span class="op">&lt;-</span> <span class="fu">make.LPIV.irf</span><span class="op">(</span><span class="va">y</span>,<span class="va">Z</span>,</span>
<span>                           nb.periods.IRF <span class="op">=</span> <span class="fl">20</span>,</span>
<span>                           nb.lags.Y.4.control<span class="op">=</span><span class="fl">4</span>,</span>
<span>                           nb.lags.Z.4.control<span class="op">=</span><span class="fl">4</span>,</span>
<span>                           indic.plot <span class="op">=</span> <span class="fl">1</span>, <span class="co"># Plots are displayed if = 1.</span></span>
<span>                           confidence.interval <span class="op">=</span> <span class="fl">0.90</span><span class="op">)</span></span></code></pre></div>
</div>
<div id="Inference" class="section level3" number="1.3.12">
<h3>
<span class="header-section-number">1.3.12</span> Inference<a class="anchor" aria-label="anchor" href="#Inference"><i class="fas fa-link"></i></a>
</h3>
<p>Consider the following SVAR model:
<span class="math display">\[y_t = \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p} + \varepsilon_t\]</span>
with <span class="math inline">\(\varepsilon_t=B\eta_t\)</span>, <span class="math inline">\(\Omega_\varepsilon=BB'\)</span>.</p>
<p>The corresponding infinite MA representation (Eq. <a href="TS.html#eq:InfMA">(1.34)</a>, or Wold theorem, Theorem <a href="TS.html#thm:Wold">1.3</a>) is:
<span class="math display">\[
y_t = \sum_{h=0}^\infty\Psi_h \eta_{t-h},
\]</span>
where <span class="math inline">\(\Psi_0=B\)</span> and for <span class="math inline">\(h=1,2,\dots\)</span>:
<span class="math display">\[
\Psi_h = \sum_{j=1}^h\Psi_{h-j}\Phi_j,
\]</span>
with <span class="math inline">\(\Phi_j=0\)</span> for <span class="math inline">\(j&gt;p\)</span> (see Prop. <a href="TS.html#prp:computPsi">1.8</a> for this recursive computation of the <span class="math inline">\(\Psi_j\)</span>’s).</p>
<p>Inference on the VAR coefficients <span class="math inline">\(\{\Phi_j\}_{j=1,...,p}\)</span> is straightforward (standard OLS inference). But inference is more complicated regarding IRF. Indeed, as shown by the previous equation, the (infinite) MA coefficients <span class="math inline">\(\{\Psi_j\}_{j=1,...}\)</span> are non-linear functions of the <span class="math inline">\(\{\Phi_j\}_{j=1,...,p}\)</span> and of <span class="math inline">\(\Omega_\varepsilon\)</span>. An other issue pertain to small sample bias: typically, for persistent process, auto-regressive parameters are known to be downward biased.</p>
<p>The main inference methods are the following:</p>
<ul>
<li>Monte Carlo method (<span class="citation">Hamilton (<a href="references.html#ref-Hamilton_1994" role="doc-biblioref">1994</a>)</span>)</li>
<li>Asymptotic normal approximation (<span class="citation">Lütkepohl (<a href="references.html#ref-Lutkepohl_1990" role="doc-biblioref">1990</a>)</span>), or Delta method</li>
<li>Bootstrap method (Kilian_1998)</li>
</ul>
<p><strong>Monte Carlo method</strong></p>
<p>We use Monte Carlo when we need to approximate the distribution of a variable whose distribution is unknown (here: the <span class="math inline">\(\Psi_j\)</span>’s) but which is a function of another variable whose distribution is known (here, the <span class="math inline">\(\Phi_j\)</span>’s).</p>
<p>For instance, suppose we know the distribution of a random variable <span class="math inline">\(X\)</span>, which takes values in <span class="math inline">\(\mathbb{R}\)</span>, with density function <span class="math inline">\(p\)</span>. Assume we want to compute the mean of <span class="math inline">\(\varphi(X)\)</span>. We have:
<span class="math display">\[
\mathbb{E}(\varphi(X))=\int_{-\infty}^{+\infty}\varphi(x)p(x)dx
\]</span>
Suppose that the above integral does not have a simple expression. We cannot compute <span class="math inline">\(\mathbb{E}(\varphi(X))\)</span> but, by virtue of the law of large numbers (Theorem <a href="#LLNappendix"><strong>??</strong></a>), we can approximate it as follows:
<span class="math display">\[
\mathbb{E}(\varphi(X))\approx\frac{1}{N}\sum_{i=1}^N\varphi(X^{(i)}),
\]</span>
where <span class="math inline">\(\{X^{(i)}\}_{i=1,...,N}\)</span> are <span class="math inline">\(N\)</span> independent draws of <span class="math inline">\(X\)</span>. More generally, the distribution of <span class="math inline">\(\varphi(X)\)</span> can be approximated by the empirical distribution of the <span class="math inline">\(\varphi(X^{(i)}\)</span>’s. Typically, if 10’000 values of <span class="math inline">\(\varphi(X^{(i)}\)</span> are drawn, the <span class="math inline">\(5^{th}\)</span> percentile of the p.d.f. of <span class="math inline">\(\varphi(X)\)</span> can be approximated by the <span class="math inline">\(500^{th}\)</span> value of the 10’000 draws of <span class="math inline">\(\varphi(X^{(i)}\)</span> (after arranging these values in ascending order).</p>
<p>As regards the computation of confidence intervals around IRFs, one has to think of <span class="math inline">\(\{\widehat{\Phi}_j\}_{j=1,...,p}\)</span>, and of <span class="math inline">\(\widehat{\Omega}\)</span> as <span class="math inline">\(X\)</span> and <span class="math inline">\(\{\widehat{\Psi}_j\}_{j=1,...}\)</span> as <span class="math inline">\(\varphi(X)\)</span>. (Proposition <a href="TS.html#prp:OLSVAR2">1.14</a> provides us with the asymptotic distribution of the “<span class="math inline">\(X\)</span>.”)</p>
<p>To summarize, here are the steps one can implement to derive confidence intervals for the IRFs using the Monte-Carlo approach:</p>
<p>For each iteration <span class="math inline">\(k\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Draw <span class="math inline">\(\{\widehat{\Phi}_j^{(k)}\}_{j=1,...,p}\)</span> and <span class="math inline">\(\widehat{\Omega}^{(k)}\)</span> from their asymptotic distribution (using Proposition <a href="TS.html#prp:OLSVAR2">1.14</a>).</li>
<li>Compute the matrix <span class="math inline">\(B^{(k)}\)</span> so that <span class="math inline">\(\widehat{\Omega}^{(k)}=B^{(k)}B^{(k)'}\)</span>, according to your identification strategy.</li>
<li>Compute the associated IRFs <span class="math inline">\(\{\widehat{\Psi}_j\}^{(k)}\)</span>.</li>
</ol>
<p>Perform <span class="math inline">\(N\)</span> replications and report the median impulse response (and its confidence intervals).</p>
<p><strong>Delta method</strong></p>
<p>Suppose <span class="math inline">\(\beta\)</span> is a vector of parameters and <span class="math inline">\(\beta\)</span> is an estimator such that
<span class="math display">\[
\sqrt{T}(\hat\beta-\beta)\overset{d}{\rightarrow}\mathcal{N}(0,\Sigma_\beta),
\]</span>
where <span class="math inline">\(d\)</span> denotes convergence in distribution, <span class="math inline">\(N(0,\Sigma_\beta)\)</span> denotes the multivariate normal distribution with mean vector 0 and covariance matrix <span class="math inline">\(\Sigma_\beta\)</span> and <span class="math inline">\(T\)</span> is the sample size used for estimation.</p>
<p>Let <span class="math inline">\(g(\beta) = (g_l(\beta),..., g_m(\beta))'\)</span> be a continuously differentiable function with values in <span class="math inline">\(\mathbb{R}^m\)</span>, and assume that <span class="math inline">\(\partial g_i/\partial \beta' = (\partial g_i/\partial \beta_j)\)</span> is nonzero at <span class="math inline">\(\beta\)</span> for <span class="math inline">\(i = 1,\dots, m\)</span>. Then
<span class="math display">\[
\sqrt{T}(g(\hat\beta)-g(\beta))\overset{d}{\rightarrow}\mathcal{N}\left(0,\frac{\partial g}{\partial \beta'}\Sigma_\beta\frac{\partial g'}{\partial \beta}\right).
\]</span>
(This formula underlies the Delta method, see Eq. <a href="#eq:DeltaMethod">(<strong>??</strong>)</a>.)</p>
<p>Using this property, <span class="citation">Lütkepohl (<a href="references.html#ref-Lutkepohl_1990" role="doc-biblioref">1990</a>)</span> provides the asymptotic distributions of the <span class="math inline">\(\Psi_j\)</span>’s.</p>
<p>A limit of the last two approaches (Monte Carlo and the Delta method) is that they critically rely on asymptotic results. Boostrapping approaches are more robust in small-sample situations.</p>
<p><strong>Bootstrap</strong></p>
<p>IRFs’ confidence intervals are intervals where 90% (or 95%,75%,…) of the IRFs would lie, if we were to repeat the estimation a large number of times in similar conditions (<span class="math inline">\(T\)</span> observations). We obviously cannot do this, because we have only one sample: <span class="math inline">\(\{y_t\}_{t=1,..,T}\)</span>. But we can try to <em>construct</em> such samples.</p>
<p>Bootstrapping consists in:</p>
<ul>
<li>re-sampling <span class="math inline">\(N\)</span> times, i.e., constructing <span class="math inline">\(N\)</span> samples of <span class="math inline">\(T\)</span> observations, using the estimated
VAR coefficients and</li>
</ul>
<ol style="list-style-type: lower-alpha">
<li>a sample of residuals from the distribution <span class="math inline">\(N(0,BB')\)</span> (<strong>parametric approach</strong>), or</li>
<li>a sample of residuals drawn randomly from the set of the actual estimated residuals <span class="math inline">\(\{\hat\varepsilon_t\}_{t=1,..,T}\)</span>. (<strong>non-parametric approach</strong>).</li>
</ol>
<ul>
<li>re-estimating the SVAR <span class="math inline">\(N\)</span> times.</li>
</ul>
<p>Here is the algorithm:</p>
<ol style="list-style-type: decimal">
<li>Construct a sample
<span class="math display">\[
y_t^{(k)}=\widehat{\Phi}_1 y_{t-1}^{(k)} + \dots + \widehat{\Phi}_p y_{t-p}^{(k)} + \hat\varepsilon_t^{(k)},
\]</span>
with <span class="math inline">\(\hat\varepsilon_{t}^{(k)}=\hat\varepsilon_{s_t^{(k)}}\)</span>, where <span class="math inline">\(\{s_1^{(k)},..,s_T^{(k)}\}\)</span> is a random set from <span class="math inline">\(\{1,..,T\}^T\)</span>.</li>
<li>Re-estimate the SVAR and compute the IRFs <span class="math inline">\(\{\widehat{\Psi}_j\}^{(k)}\)</span>.</li>
</ol>
<p>Perform <span class="math inline">\(N\)</span> replications and report the median impulse response (and its confidence intervals).</p>
<p><strong>Bootstrap-after-bootstrap</strong> (<span class="citation">Kilian (<a href="references.html#ref-Kilian_1998" role="doc-biblioref">1998</a>)</span>)</p>
<p>The previous simple bootstrapping procedure deals with non-normality and small sample distribution, since we use the actual residuals. However, it does not deal with the <em>small sample bias</em>, stemming, in particular, from small-sample bias associated with OLS coefficient estimates <span class="math inline">\(\{\widehat{\Phi}_j\}_{j=1,..,p}\)</span>. The main idea of the bootstrap-after-bootstrap of <span class="citation">Kilian (<a href="references.html#ref-Kilian_1998" role="doc-biblioref">1998</a>)</span> is to run two consecutive boostraps: the objective of the first is to compute the bias, which can further be used to correct the initial estimates of the <span class="math inline">\(\Phi_i\)</span>’s. Further, these corrected estimates are used —in the second boostrap— to compute a set of IRFs (as in the standard boostrap).</p>
<p>More formally, the algorithm is as follows:</p>
<ol style="list-style-type: decimal">
<li>Estimate the SVAR coefficients <span class="math inline">\(\{\widehat{\Phi}_j\}_{j=1,..,p}\)</span> and <span class="math inline">\(\widehat{\Omega}\)</span>
</li>
<li>
<strong>First bootstrap.</strong> For each iteration <span class="math inline">\(k\)</span>:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Construct a sample
<span class="math display">\[
y_t^{(k)}=\widehat{\Phi}_1 y_{t-1}^{(k)} + \dots + \widehat{\Phi}_p y_{t-p}^{(k)} + \hat\varepsilon_t^{(k)},
\]</span>
with <span class="math inline">\(\hat\varepsilon_{t}^{(k)}=\hat\varepsilon_{s_t^{(k)}}\)</span>, where <span class="math inline">\(\{s_1^{(k)},..,s_T^{(k)}\}\)</span> is a random set from <span class="math inline">\(\{1,..,T\}^T\)</span>.</li>
<li>Re-estimate the VAR and compute the coefficients <span class="math inline">\(\{\widehat{\Phi}_j\}_{j=1,..,p}^{(k)}\)</span>.</li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li>Perform <span class="math inline">\(N\)</span> replications and compute the median coefficients <span class="math inline">\(\{\widehat{\Phi}_j\}_{j=1,..,p}^*\)</span>.</li>
<li>Approximate the bias terms by <span class="math inline">\(\widehat{\Theta}_j=\widehat{\Phi}_j^*-\widehat{\Phi}_j\)</span>.</li>
<li>Construct the bias-corrected terms <span class="math inline">\(\widetilde{\Phi}_j=\widehat{\Phi}_j-\widehat{\Theta}_j\)</span>.</li>
<li>
<strong>Second bootstrap.</strong> For each iteration <span class="math inline">\(k\)</span>:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Construct a sample now from
<span class="math display">\[y_t^{(k)}=\widetilde{\Phi}_1 y_{t-1}^{(k)} + \dots + \widetilde{\Phi}_p y_{t-p}^{(k)} + \hat\varepsilon_t^{(k)}.
\]</span>
</li>
<li>Re-estimate the VAR and compute the coefficients <span class="math inline">\(\{\widehat{\Phi}^*_j\}_{j=1,..,p}^{(k)}\)</span>.</li>
<li>Construct the bias-corrected estimates <span class="math inline">\(\widetilde{\Phi}_j^{*(k)}=\widehat{\Phi}_j^{*(k)}-\widehat{\Theta}_j\)</span>.</li>
<li>Compute the associated IRFs <span class="math inline">\(\{\widetilde{\Psi}_j^{*(k)}\}_{j\ge 1}\)</span>.</li>
</ol>
<ol start="7" style="list-style-type: decimal">
<li>Perform <span class="math inline">\(N\)</span> replications and compute the median and the confidence interval of the set of IRFs.</li>
</ol>
<p>It should be noted that correcting for the bias can generate non-stationary results (<span class="math inline">\(\tilde \Phi\)</span> with eigenvalue with modulus <span class="math inline">\(&gt;1\)</span>). Solution (<span class="citation">Kilian (<a href="references.html#ref-Kilian_1998" role="doc-biblioref">1998</a>)</span>):</p>
<p>In step 5, check if the largest eigenvalue of <span class="math inline">\(\tilde\Phi\)</span> is of modulus &lt;1.
If not, shrink the bias: for all <span class="math inline">\(j\)</span>s, set <span class="math inline">\(\widehat{\Theta}_j^{(i+1)}=\delta_{i+1}\widehat{\Theta}_j^{(i)}\)</span>, with <span class="math inline">\(\delta_{i+1}=\delta_i-0.01\)</span>, starting with <span class="math inline">\(\delta_1=1\)</span> and <span class="math inline">\(\widehat{\Theta}_j^{(1)} =\widehat{\Theta}_j\)</span>, and compute <span class="math inline">\(\widetilde{\Phi}_j^{(i+1)}=\widehat{\Phi}_j-\widehat{\Theta}_j^{(i+1)}\)</span> until the largest eigenvalue of <span class="math inline">\(\tilde\Phi^{(i+1)}\)</span> has modulus &lt;1.</p>
<p>Function <code>VAR.Boot</code> of package <code>VAR.etp</code> (<span class="citation">Kim (<a href="references.html#ref-VARetp" role="doc-biblioref">2022</a>)</span>) can be used to operate the bias-correction approach of <span class="citation">Kilian (<a href="references.html#ref-Kilian_1998" role="doc-biblioref">1998</a>)</span>:</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">VAR.etp</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.pfaffikus.de">vars</a></span><span class="op">)</span> <span class="co">#standard VAR models</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">dat</span><span class="op">)</span> <span class="co"># part of VAR.etp package</span></span>
<span><span class="va">corrected</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/VAR.etp/man/VAR.Boot.html">VAR.Boot</a></span><span class="op">(</span><span class="va">dat</span>,p<span class="op">=</span><span class="fl">2</span>,nb<span class="op">=</span><span class="fl">200</span>,type<span class="op">=</span><span class="st">"const"</span><span class="op">)</span></span>
<span><span class="va">noncorrec</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/vars/man/VAR.html">VAR</a></span><span class="op">(</span><span class="va">dat</span>,p<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">corrected</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span>,</span>
<span>      <span class="op">(</span><span class="va">corrected</span><span class="op">$</span><span class="va">coef</span><span class="op">+</span><span class="va">corrected</span><span class="op">$</span><span class="va">Bias</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span>,</span>
<span>      <span class="va">noncorrec</span><span class="op">$</span><span class="va">varresult</span><span class="op">$</span><span class="va">inv</span><span class="op">$</span><span class="va">coefficients</span><span class="op">)</span></span></code></pre></div>
<pre><code>##         inv(-1)   inc(-1)   con(-1)    inv(-2)   inc(-2)   con(-2)       const
## [1,] -0.3199291 0.1564734 0.9638388 -0.1248016 0.1217136 0.8938142 -0.01663028
## [2,] -0.3196310 0.1459888 0.9612190 -0.1605511 0.1146050 0.9343938 -0.01672199
## [3,] -0.3196310 0.1459888 0.9612190 -0.1605511 0.1146050 0.9343938 -0.01672199</code></pre>
</div>
</div>
<div id="forecasting" class="section level2" number="1.4">
<h2>
<span class="header-section-number">1.4</span> Forecasting<a class="anchor" aria-label="anchor" href="#forecasting"><i class="fas fa-link"></i></a>
</h2>
<p>Forecasting has always been an important part of the time series field (<span class="citation">De Gooijer and Hyndman (<a href="references.html#ref-DEGOOIJER2006443" role="doc-biblioref">2006</a>)</span>). Macroeconomic forecasts are done in many places: Public Administration (notably Treasuries), Central Banks, International Institutions (e.g. IMF, OECD), banks, big firms. These institutions are interested in the <strong>point estimates</strong> (<span class="math inline">\(\sim\)</span> most likely value) of the variable of interest. They also sometimes need to measure the <strong>uncertainty</strong> (<span class="math inline">\(\sim\)</span> dispersion of likely outcomes) associated to the point estimates.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;In its inflation report, the Bank of England displays charts showing the conditional distribution of future inflation, called fan charts. This fan charts show the uncertainty associated with future inflation. See &lt;a href="https://www.bankofengland.co.uk/quarterly-bulletin/1998/q1/the-inflation-report-projections-understanding-the-fan-chart"&gt;this page&lt;/a&gt;.&lt;/p&gt;'><sup>5</sup></a></p>
<p>Forecasts produced by professional forecasters are available on these web pages:</p>
<ul>
<li>
<a href="https://www.philadelphiafed.org/research-and-data/real-time-center/survey-of-professional-forecasters/">Philly Fed Survey of Professional Forecasters</a>.</li>
<li>
<a href="http://www.ecb.europa.eu/stats/prices/indic/forecast/html/index.en.html">ECB Survey of Professional Forecasters</a>.</li>
<li>
<a href="https://www.imf.org/external/pubs/ft/weo/2016/update/01/">IMF World Economic Outlook</a>.</li>
<li>
<a href="http://www.oecd.org/eco/economicoutlook.htm">OECD Global Economic Outlook</a>.</li>
<li>
<a href="http://ec.europa.eu/economy_finance/eu/forecasts/index_en.htm">European Commission Economic Forecasts</a>.</li>
</ul>
<p>How to formalize the forecasting problem? Assume the current date is <span class="math inline">\(t\)</span>. We want to forecast the value that variable <span class="math inline">\(y_t\)</span> will take on date <span class="math inline">\(t+1\)</span> (i.e., <span class="math inline">\(y_{t+1}\)</span>) based on the observation of a set of variables gathered in vector <span class="math inline">\(x_t\)</span> (<span class="math inline">\(x_t\)</span> may contain lagged values of <span class="math inline">\(y_t\)</span>).</p>
<p>The forecaster aims at minimizing (a function of) the forecast error. It is usal to consider the following (quadratic) loss function:
<span class="math display">\[
\underbrace{\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)}_{\mbox{Mean square error (MSE)}}
\]</span>
where <span class="math inline">\(y^*_{t+1}\)</span> is the forecast of <span class="math inline">\(y_{t+1}\)</span> (function of <span class="math inline">\(x_t\)</span>).</p>
<div class="proposition">
<p><span id="prp:smallestMSE" class="proposition"><strong>Proposition 1.15  (Smallest MSE) </strong></span>The smallest MSE is obtained with MSEthe expectation of <span class="math inline">\(y_{t+1}\)</span> conditional on <span class="math inline">\(x_t\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-17" class="proof"><em>Proof</em>. </span>See Appendix <a href="append.html#AppendixProof">2.5</a>.</p>
</div>
<div class="proposition" names="Smallest MSE for linear forecasts">
<p><span id="prp:smallestMSElinear" class="proposition"><strong>Proposition 1.16  </strong></span>Among the class of linear forecasts, the smallest MSE is obtained with the linear projection of <span class="math inline">\(y_{t+1}\)</span> on <span class="math inline">\(x_t\)</span>.
This projection, denoted by <span class="math inline">\(\hat{P}(y_{t+1}|x_t):=\boldsymbol\alpha'x_t\)</span>, satisfies:
<span class="math display" id="eq:proj">\[\begin{equation}
\mathbb{E}\left( [y_{t+1} - \boldsymbol\alpha'x_t]x_t \right)=\mathbf{0}.\tag{1.57}
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-18" class="proof"><em>Proof</em>. </span>Consider the function <span class="math inline">\(f:\)</span> <span class="math inline">\(\boldsymbol\alpha \rightarrow \mathbb{E}\left( [y_{t+1} - \boldsymbol\alpha'x_t]^2 \right)\)</span>. We have:
<span class="math display">\[
f(\boldsymbol\alpha) = \mathbb{E}\left( y_{t+1}^2 - 2 y_t x_t'\boldsymbol\alpha + \boldsymbol\alpha'x_t x_t'\boldsymbol\alpha] \right).
\]</span>
We have <span class="math inline">\(\partial f(\boldsymbol\alpha)/\partial \boldsymbol\alpha = \mathbb{E}(-2 y_{t+1} x_t + 2 x_t x_t'\boldsymbol\alpha)\)</span>. The function is minimised for <span class="math inline">\(\partial f(\boldsymbol\alpha)/\partial \boldsymbol\alpha =0\)</span>.</p>
</div>
<p>Eq. <a href="TS.html#eq:proj">(1.57)</a> implies that <span class="math inline">\(\mathbb{E}\left( y_{t+1}x_t \right)=\mathbb{E}\left(x_tx_t' \right)\boldsymbol\alpha\)</span>. (Note that <span class="math inline">\(x_t x_t'\boldsymbol\alpha=x_t (x_t'\boldsymbol\alpha)=(\boldsymbol\alpha'x_t) x_t'\)</span>.)</p>
<p>Hence, if <span class="math inline">\(\mathbb{E}\left(x_tx_t' \right)\)</span> is nonsingular,
<span class="math display" id="eq:linproj">\[\begin{equation}
\boldsymbol\alpha=[\mathbb{E}\left(x_tx_t' \right)]^{-1}\mathbb{E}\left( y_{t+1}x_t \right).\tag{1.58}
\end{equation}\]</span></p>
<p>The MSE then is:
<span class="math display">\[
\mathbb{E}([y_{t+1} - \boldsymbol\alpha'x_t]^2) = \mathbb{E}{(y_{t+1}^2)} - \mathbb{E}\left( y_{t+1}x_t' \right)[\mathbb{E}\left(x_tx_t' \right)]^{-1}\mathbb{E}\left(x_ty_{t+1} \right).
\]</span></p>
<p>Consider the regression <span class="math inline">\(y_{t+1} = \boldsymbol\beta'\mathbf{x}_t + \varepsilon_{t+1}\)</span>. The OLS estimate is:
<span class="math display">\[
\mathbf{b} = \left[ \underbrace{ \frac{1}{T} \sum_{i=1}^T \mathbf{x}_t\mathbf{x}_t'}_{\mathbf{m}_1} \right]^{-1}\left[  \underbrace{ \frac{1}{T} \sum_{i=1}^T \mathbf{x}_t'y_{t+1}}_{\mathbf{m}_2} \right].
\]</span>
If <span class="math inline">\(\{x_t,y_t\}\)</span> is covariance-stationary and ergodic for the second moments then the sample moments (<span class="math inline">\(\mathbf{m}_1\)</span> and <span class="math inline">\(\mathbf{m}_2\)</span>) converges in probability to the associated population moments and <span class="math inline">\(\mathbf{b} \overset{p}{\rightarrow} \boldsymbol\alpha\)</span> (where <span class="math inline">\(\boldsymbol\alpha\)</span> is defined in Eq. <a href="TS.html#eq:linproj">(1.58)</a>).</p>
<div class="example">
<p><span id="exm:fcstMAq" class="example"><strong>Example 1.11  (Forecasting an MA(q) process) </strong></span>Consider the MA(q) process:
<span class="math display">\[
y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q},
\]</span>
where <span class="math inline">\(\{\varepsilon_t\}\)</span> is a white noise sequence (Def. <a href="TS.html#def:whitenoise">1.1</a>).</p>
<p>We have:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{E}(y_{t+h}|\varepsilon_{t},\varepsilon_{t-1},\dots) =\\
&amp;&amp;\left\{
\begin{array}{lll}
\mu + \theta_h \varepsilon_{t} + \dots + \theta_q \varepsilon_{t-q+h}  \quad &amp;for&amp; \quad h \in [1,q]\\
\mu \quad &amp;for&amp; \quad h &gt; q
\end{array}
\right.
\end{eqnarray*}\]</span>
and
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{V}ar(y_{t+h}|\varepsilon_{t},\varepsilon_{t-1},\dots)= \mathbb{E}\left( [y_{t+h} - \mathbb{E}(y_{t+h}|\varepsilon_{t},\varepsilon_{t-1},\dots)]^2 \right) =\\
&amp;&amp;\left\{
\begin{array}{lll}
\sigma^2(1+\theta_1^2+\dots+\theta_{h-1}^2) \quad &amp;for&amp; \quad h \in [1,q]\\
\sigma^2(1+\theta_1^2+\dots+\theta_q^2) \quad &amp;for&amp; \quad h&gt;q.
\end{array}
\right.
\end{eqnarray*}\]</span></p>
<p>Remark: The previous reasoning relies on the assumption that the <span class="math inline">\(\varepsilon_t\)</span>s are observed. But this is generally not the case in practice. Note that consistent estimates are available if the MA process is invertible (see Eq. <a href="TS.html#eq:invertible">(1.28)</a>) .</p>
</div>
<div class="example">
<p><span id="exm:fcstARp" class="example"><strong>Example 1.12  (Forecasting an AR(p) process) </strong></span>(See <a href="https://jrenne.shinyapps.io/ARpFcst">this web interface</a>.) Consider the AR(p) process:
<span class="math display">\[
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \varepsilon_t,
\]</span>
where <span class="math inline">\(\{\varepsilon_t\}\)</span> is a white noise sequence (Def. <a href="TS.html#def:whitenoise">1.1</a>).</p>
<p>Using the notation of Eq. <a href="TS.html#eq:F">(1.9)</a>, we have:
<span class="math display">\[
\mathbf{y}_t - \boldsymbol\mu = F (\mathbf{y}_{t-1}- \boldsymbol\mu) + \boldsymbol\xi_t,
\]</span>
with <span class="math inline">\(\boldsymbol\mu = [\mu,\dots,\mu]'\)</span> (<span class="math inline">\(\mu\)</span> is defined in Eq. <a href="TS.html#eq:EAR">(1.13)</a>). Hence:
<span class="math display">\[
\mathbf{y}_{t+h} - \boldsymbol\mu = \boldsymbol\xi_{t+h} + F \boldsymbol\xi_{t+h-1} + \dots + F^{h-1} \boldsymbol\xi_{t+1} + F^h (\mathbf{y}_{t}- \mu).
\]</span>
Therefore:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(\mathbf{y}_{t+h}|y_{t},y_{t-1},\dots) &amp;=&amp; \boldsymbol\mu + F^{h}(\mathbf{y}_t - \boldsymbol\mu)\\
\mathbb{V}ar\left( [\mathbf{y}_{t+h} - \mathbb{E}(\mathbf{y}_{t+h}|y_{t},y_{t-1},\dots)] \right) &amp;=&amp; \Sigma + F\Sigma F' + \dots + F^{h-1}\Sigma (F^{h-1})',
\end{eqnarray*}\]</span>
where:
<span class="math display">\[
\Sigma = \left[
\begin{array}{ccc}
\sigma^2  &amp; 0&amp; \dots\\
0  &amp; 0 &amp; \\
\vdots  &amp; &amp; \ddots \\
\end{array}
\right].
\]</span></p>
<p>Alternative approach: Taking the (conditional) expectations of both sides of
<span class="math display">\[
y_{t+h} - \mu = \phi_1 (y_{t+h-1} - \mu) + \phi_2 (y_{t+h-2} - \mu) + \dots + \phi_p (y_{t-p} - \mu) + \varepsilon_{t+h},
\]</span>
we obtain:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(y_{t+h}|y_{t},y_{t-1},\dots) &amp;=&amp; \mu + \phi_1\left(\mathbb{E}[y_{t+h-1}|y_{t},y_{t-1},\dots] - \mu\right)+\\
&amp;&amp;\phi_2\left(\mathbb{E}[y_{t+h-2}|y_{t},y_{t-1},\dots] - \mu\right) + \dots +\\
&amp;&amp; \phi_p\left(\mathbb{E}[y_{t+h-p}|y_{t},y_{t-1},\dots] - \mu\right),
\end{eqnarray*}\]</span>
which can be exploited recursively.</p>
<p>The recursion begins with <span class="math inline">\(\mathbb{E}(y_{t-k}|y_{t},y_{t-1},\dots)=y_{t-k}\)</span> (for any <span class="math inline">\(k \ge 0\)</span>).</p>
</div>
<div class="example">
<p><span id="exm:fcstARMApq" class="example"><strong>Example 1.13  (Forecasting an ARMA(p,q) process) </strong></span>Consider the process:
<span class="math display" id="eq:armaForecast">\[\begin{equation}
y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q},\tag{1.59}
\end{equation}\]</span>
where <span class="math inline">\(\{\varepsilon_t\}\)</span> is a white noise sequence (Def. <a href="TS.html#def:whitenoise">1.1</a>). We assume that the MA part of the process is invertible (see Eq. <a href="TS.html#eq:invertible">(1.28)</a>), which implies that the information contained in <span class="math inline">\(\{y_{t},y_{t-1},y_{t-2},\dots\}\)</span> is identical to that in <span class="math inline">\(\{\varepsilon_{t},\varepsilon_{t-1},\varepsilon_{t-2},\dots\}\)</span>.</p>
<p>While one could use a recursive algorithm to compute the conditional mean (as in Example <a href="TS.html#exm:fcstARp">1.12</a>), it is convenient to employ the Wold decomposition of this process (see Theorem <a href="TS.html#thm:Wold">1.3</a> and Prop. <a href="TS.html#prp:computPsi">1.8</a> for the computation of the <span class="math inline">\(\psi_i\)</span>’s in the context of ARMA processes):
<!-- We have: -->
<!-- \begin{eqnarray*} -->
<!-- &&\mathbb{E}(y_{t+h}|y_{t},y_{t-1},\dots) =\\ -->
<!-- &&\left\{ -->
<!-- \begin{array}{ll} -->
<!-- \mu + \phi_1\left(\mathbb{E}(y_{t+h-1}|y_{t},y_{t-1},\dots) - \mu\right)+\\ -->
<!-- \phi_2\left(\mathbb{E}(y_{t+h-2}|y_{t},y_{t-1},\dots) - \mu\right) + \dots +\\ -->
<!-- \phi_p\left(\mathbb{E}(y_{t+h-p}|y_{t},y_{t-1},\dots) - \mu\right) +\\ -->
<!-- \theta_{h}\varepsilon_{t} + \theta_{h+1}\varepsilon_{t-1} + \dots + \theta_{q}\varepsilon_{t+h-q} &\mbox{for  $h \in[1,q]$},\\ -->
<!-- \\ -->
<!-- \mu + \phi_1\left(\mathbb{E}(y_{t+h-1}|y_{t},y_{t-1},\dots) - \mu\right)+\\ -->
<!-- \phi_2\left(\mathbb{E}(y_{t+h-2}|y_{t},y_{t-1},\dots) - \mu\right) + \dots +\\ -->
<!-- \phi_p\left(\mathbb{E}(y_{t+h-p}|y_{t},y_{t-1},\dots) - \mu\right) & \mbox{for $h>q$}. -->
<!-- \end{array} -->
<!-- \right. -->
<!-- \end{eqnarray*} -->
<!-- To compute the MSE, it is convenient to use the Wold decomposition of the process (see Theorem \@ref(thm:Wold)): -->
<span class="math display">\[
y_t = \mu + \sum_{i=0}^{+\infty} \psi_i \varepsilon_{t-i}.
\]</span>
This implies:
<span class="math display">\[\begin{eqnarray*}
y_{t+h} &amp;=&amp; \mu + \sum_{i=0}^{h-1} \psi_i \varepsilon_{t+h-i} + \sum_{i=h}^{+\infty} \psi_i \varepsilon_{t+h-i}\\
&amp;=&amp; \mu + \sum_{i=0}^{h-1} \psi_i \varepsilon_{t+h-i} + \sum_{i=0}^{+\infty} \psi_{i+h} \varepsilon_{t-i}.
\end{eqnarray*}\]</span></p>
<p>Since <span class="math inline">\(\mathbb{E}(y_{t+h}|y_t,y_{t-1},\dots)=\mu+\sum_{i=0}^{+\infty} \psi_{i+h} \varepsilon_{t-i}\)</span>, we get:
<span class="math display">\[
\mathbb{V}ar(y_{t+h}|y_t,y_{t-1},\dots) =\mathbb{V}ar\left(\sum_{i=0}^{h-1} \psi_i \varepsilon_{t+h-i}\right)= \sigma^2 \sum_{i=0}^{h-1} \psi_i^2.
\]</span></p>
</div>
<p>How to use the previous formulas in practice?</p>
<p>One has first to select a specification and to estimate the model.
Two methods to determine relevant specifications:</p>
<ol style="list-style-type: lower-alpha">
<li>Information criteria (see Definition <a href="TS.html#def:infocriteria">1.20</a>).</li>
<li>Box-Jenkins approach.</li>
</ol>
<p><span class="citation">Box and Jenkins (<a href="references.html#ref-boxjen76" role="doc-biblioref">1976</a>)</span> have proposed an approach that is now widely used.</p>
<ol style="list-style-type: decimal">
<li>Data transformation. The data should be transformed to “make them stationary”. To do so, one can e.g. take logarithms, take changes in the considered series, remove (deterministic) trends.</li>
<li>Select <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>. This can be based on the PACF approach (see Section <a href="TS.html#PACFapproach">1.2.4</a>), or on selection criteria (see Definition <a href="TS.html#def:infocriteria">1.20</a>).</li>
<li>Estimate the model parameters. See Section <a href="TS.html#estimARMA">1.2.8</a>.</li>
<li>Check that the estimated model is consistent with the data. See below.</li>
</ol>
<!-- **Sample autocorrelation** --><!-- Population autocorrelation: $\rho_i = \gamma_i / \gamma_0$. Natural estimate based on sample moments: $\hat{\rho_i} = \hat\gamma_j / \hat\gamma_0$ where: --><!-- $$ --><!-- \gamma_j = \frac{1}{T} \sum_{t=j+1}^{T} (y_t - \bar{y})(y_{t-j} - \bar{y}). --><!-- $$ --><!-- For an MA($q$) process, $\rho_j = 0$ for $j>q$. --><!-- If the data are generated by a Gaussian MA($q$) process, then: --><!-- $$ --><!-- \mathbb{V}ar(\hat\rho_j) \approx \frac{1}{T}\left\{1 + 2 \sum_{i=1}^q \rho_i^2 \right\} --><!-- $$ --><!-- In particular, if the data correspond to a Gaussian white noise then $\hat\rho_i \in [\pm 2/\sqrt{T}]$ about 95\% of the time. --><!-- **Partial autocorrelation** --><!-- The $m^{th}$ population partial autocorrelation (see also Def. \@ref(def:partialAC)) is the $m^{th}$ coefficient in a linear projection of $y_{t+1}$ on its $m$ most recent lags: --><!-- $$ --><!-- \hat{y}_{t+1|t} = \mu + \phi_{1,m}(y_t - \mu) + \phi_{2,m}(y_{t-1} - \mu) + \dots +\phi_{m,m}(y_{t-m+1} - \mu). --><!-- $$ --><!-- Using the OLS formula, it can be shown that the (population) vector $\boldsymbol\phi_{.,m}=[\phi_{1,m},\dots,\phi_{m,m}]'$ satisfies: --><!-- $$ --><!-- \boldsymbol\phi_{.,m} = \left[ --><!-- \begin{array}{cccc} --><!-- \gamma_0 & \gamma_1& \dots & \gamma_{m-1}\\ --><!-- \gamma_1 & \gamma_0& \dots & \gamma_{m-2}\\ --><!-- \vdots & & \ddots & \\ --><!-- \gamma_{m-1} & \gamma_{m-2}& \dots & \gamma_{0} --><!-- \end{array} --><!-- \right]^{-1}\left[ --><!-- \begin{array}{c} --><!-- \gamma_1\\ --><!-- \gamma_2\\ --><!-- \vdots\\ --><!-- \gamma_{m} --><!-- \end{array} --><!-- \right]. --><!-- $$ --><!-- For an AR($p$) process, $\phi_{m,m}=0$ for $m>p$, which reflects the fact that only the first $p$ lags are useful to forecast $y_{t}$. --><!-- A natural estimate $\hat\phi_{m,m}$ of $\phi_{m,m}$ is obtained by running the regression: --><!-- $$ --><!-- \hat{y}_{t+1} = \hat{c} + \hat\phi_{1,m}y_t + \hat\phi_{2,m}y_{t-1} + \dots + \hat\phi_{m,m}y_{t-m+1} + \hat\varepsilon_{t+1}. --><!-- $$ --><!-- Under the hypothesis that the data are generated by an AR($p$) process: --><!-- $$ --><!-- \mathbb{V}ar(\hat\phi_{m,m}) \approx \frac{1}{T} \quad \mbox{for} \quad m > p. --><!-- $$ --><!-- Besides, for $i,j>p$, $\hat\phi_{i,i}$ and $\hat\phi_{j,j}$ are asymptotically independent. --><p><strong>Assessing the performances of a forecasting model</strong></p>
<p>Once one has fitted a model on a given dataset (of length <span class="math inline">\(T\)</span>, say), one compute MSE (mean square errors) to evaluate the performance of the model. But this MSE is the <strong>in-sample</strong> one. It is easy to reduce in-sample MSE. Typically, if the model is estimated by OLS, adding covariates mechanically reduces the MSE (see Props. <a href="#prp:chgeR2"><strong>??</strong></a> and <a href="#prp:chgeInR2"><strong>??</strong></a>). That is, even if additional data are irrelevant, the <span class="math inline">\(R^2\)</span> of the regression increases. Adding irrelevant variables increases the (in-sample) <span class="math inline">\(R^2\)</span> but is bound to increase the <strong>out-of-sample</strong> MSE.</p>
<p>Therefore, it is important to analyse <strong>out-of-sample</strong> performances of the forecasting model:</p>
<ol style="list-style-type: lower-alpha">
<li>Estimate a model on a sample of reduced size (<span class="math inline">\(1,\dots,T^*\)</span>, with <span class="math inline">\(T^*&lt;T\)</span>)</li>
<li>Use the remaining available periods (<span class="math inline">\(T^*+1,\dots,T\)</span>) to compute <strong>out-of-sample</strong> forecasting errors (and compute their MSE). In an out-of-sample exercise, it is important to make sure that the data used to produce a forecasts (as of date <span class="math inline">\(T^*\)</span>) where indeed available on date <span class="math inline">\(T^*\)</span>.</li>
</ol>
<p><strong>Diebold-Mariano test</strong></p>
<p>How to compare different forecasting approaches? <span class="citation">Diebold and Mariano (<a href="references.html#ref-Diebold_Mariano_1995" role="doc-biblioref">1995</a>)</span> have proposed a simple test to address this question.</p>
<p>Assume that you want to compare approaches A and B. You have historical data sets and you have implemented both approaches in the past, providing you with two sets of forecasting errors: <span class="math inline">\(\{e^{A}_t\}_{t=1,\dots,T}\)</span> and <span class="math inline">\(\{e^{B}_t\}_{t=1,\dots,T}\)</span>.</p>
<p>It may be the case that your forecasts serve a specific purpose and that, for instance, you dislike positive forecasting errors and you care less about negative errors. We assume you are able to formalise this by means of a <strong>loss function <span class="math inline">\(L(e)\)</span></strong>. For instance:</p>
<ul>
<li>If you dislike large positive errors, you may set <span class="math inline">\(L(e)=\exp(e)\)</span>.</li>
<li>If you are concerned about both positive and negative errors (indifferently), you may set <span class="math inline">\(L(e)=e^2\)</span> (standard approach).</li>
</ul>
<p>Let us define the sequence <span class="math inline">\(\{d_t\}_{t=1,\dots,T} \equiv \{L(e^{A}_t)-L(e^{B}_t)\}_{t=1,\dots,T}\)</span> and assume that this sequence is covariance stationary. We consider the following null hypothesis: <span class="math inline">\(H_0:\)</span> <span class="math inline">\(\bar{d}=0\)</span>, where <span class="math inline">\(\bar{d}\)</span> denotes the population mean of the <span class="math inline">\(d_t\)</span>s. Under <span class="math inline">\(H_0\)</span> and under the assumption of covariance-stationarity of <span class="math inline">\(d_t\)</span>, we have (Theorem @ref{(hm:CLTcovstat)):
<span class="math display">\[
\sqrt{T} \bar{d}_T \overset{d}{\rightarrow} \mathcal{N}\left(0,\sum_{j=-\infty}^{+\infty} \gamma_j \right),
\]</span><br>
where the <span class="math inline">\(\gamma_j\)</span>s are the autocovariances of <span class="math inline">\(d_t\)</span>.</p>
<p>Hence, assuming that <span class="math inline">\(\hat{\sigma}^2\)</span> is a consistent estimate of <span class="math inline">\(\sum_{j=-\infty}^{+\infty} \gamma_j\)</span> (for instance the one given by the Newey-West formula, see Def. <a href="#def:NW"><strong>??</strong></a>), we have, under <span class="math inline">\(H_0\)</span>:
<span class="math display">\[
DM_T := \sqrt{T}\frac{\bar{d}_T}{\sqrt{\hat{\sigma}^2}} \overset{d}{\rightarrow}  \mathcal{N}(0,1).
\]</span>
<span class="math inline">\(DM_T\)</span> is the test statistics. For a test of size <span class="math inline">\(\alpha\)</span>, the critical region is:<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;This &lt;a href="https://jrenne.shinyapps.io/tests/"&gt;ShinyApp application&lt;/a&gt; illustrates the notion of statistical test (illustrating the p-value and the cirtical region, in particular).&lt;/p&gt;'><sup>6</sup></a>
<span class="math display">\[
]-\infty,-\Phi^{-1}(1-\alpha/2)] \cup [\Phi^{-1}(1-\alpha/2),+\infty[,
\]</span>
where <span class="math inline">\(\Phi\)</span> is the c.d.f. of the standard normal distribution.</p>
<div class="example">
<p><span id="exm:SwissOutOfSample" class="example"><strong>Example 1.14  (Forecasting Swiss GDP growth) </strong></span>We use a long historical time series of the Swiss GDP growth taken from the <span class="citation">Jordà, Schularick, and Taylor (<a href="references.html#ref-JST_2017" role="doc-biblioref">2017</a>)</span> dataset (see Figure <a href="TS.html#fig:autocov">1.3</a>, and Example <a href="TS.html#exm:SwissGrowthAIC">1.6</a>).</p>
<p>We want to forecast this GDP growth. We envision two specifications : an AR(1) specification (the one advocated by the AIC criteria, see Example <a href="TS.html#exm:SwissGrowthAIC">1.6</a>), and an ARMA(2,2) specification. We are interested in 2-year-ahead forecasts (i.e., <span class="math inline">\(h=2\)</span> since the data are yearly).</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://pkg.robjhyndman.com/forecast/">forecast</a></span><span class="op">)</span></span></code></pre></div>
<pre><code>## Registered S3 method overwritten by 'quantmod':
##   method            from
##   as.zoo.data.frame zoo</code></pre>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">JST</span>,<span class="va">iso</span><span class="op">==</span><span class="st">"CHE"</span><span class="op">)</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">gdp</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">data</span><span class="op">$</span><span class="va">gdp</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">first.date</span> <span class="op">&lt;-</span> <span class="cn">T</span><span class="op">-</span><span class="fl">50</span></span>
<span><span class="va">e1</span> <span class="op">&lt;-</span> <span class="cn">NULL</span>; <span class="va">e2</span> <span class="op">&lt;-</span> <span class="cn">NULL</span>;<span class="va">h</span><span class="op">&lt;-</span><span class="fl">2</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">T.star</span> <span class="kw">in</span> <span class="va">first.date</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">h</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">estim.model.1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.html">arima</a></span><span class="op">(</span><span class="va">y</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">T.star</span><span class="op">]</span>,order<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">estim.model.2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.html">arima</a></span><span class="op">(</span><span class="va">y</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">T.star</span><span class="op">]</span>,order<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">0</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">e1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">e1</span>,<span class="va">y</span><span class="op">[</span><span class="va">T.star</span><span class="op">+</span><span class="va">h</span><span class="op">]</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">estim.model.1</span>,n.ahead<span class="op">=</span><span class="va">h</span><span class="op">)</span><span class="op">$</span><span class="va">pred</span><span class="op">[</span><span class="va">h</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">e2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">e2</span>,<span class="va">y</span><span class="op">[</span><span class="va">T.star</span><span class="op">+</span><span class="va">h</span><span class="op">]</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">estim.model.2</span>,n.ahead<span class="op">=</span><span class="va">h</span><span class="op">)</span><span class="op">$</span><span class="va">pred</span><span class="op">[</span><span class="va">h</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">res.DM</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://pkg.robjhyndman.com/forecast/reference/dm.test.html">dm.test</a></span><span class="op">(</span><span class="va">e1</span>,<span class="va">e2</span>,h <span class="op">=</span> <span class="va">h</span>,alternative <span class="op">=</span> <span class="st">"greater"</span><span class="op">)</span></span>
<span><span class="va">res.DM</span></span></code></pre></div>
<pre><code>## 
##  Diebold-Mariano Test
## 
## data:  e1e2
## DM = -0.82989, Forecast horizon = 2, Loss function power = 2, p-value =
## 0.7946
## alternative hypothesis: greater</code></pre>
<p>With <code>alternative = "greater"</code> The alternative hypothesis is that method 2 is more accurate than method 1. Since we do not reject the null (the p-value being of 0.795), we are not led to use the more sophisticated model (ARMA(2,2)) and we keep the simple AR(1) model.</p>
<p>Assume now that we want to compare the AR(1) process to a VAR model (see Def. <a href="TS.html#def:SVAR">1.21</a>). We consider a bivariate VAR, where GDP growth is complemented with CPI-based inflation rate.</p>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.pfaffikus.de">vars</a></span><span class="op">)</span></span>
<span><span class="va">infl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">cpi</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">data</span><span class="op">$</span><span class="va">cpi</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">y_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">y</span>,<span class="va">infl</span><span class="op">)</span></span>
<span><span class="va">e3</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">T.star</span> <span class="kw">in</span> <span class="va">first.date</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">h</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">estim.model.3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/vars/man/VAR.html">VAR</a></span><span class="op">(</span><span class="va">y_var</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="va">T.star</span>,<span class="op">]</span>,p<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="va">e3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">e3</span>,<span class="va">y</span><span class="op">[</span><span class="va">T.star</span><span class="op">+</span><span class="va">h</span><span class="op">]</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">estim.model.3</span>,n.ahead<span class="op">=</span><span class="va">h</span><span class="op">)</span><span class="op">$</span><span class="va">fcst</span><span class="op">$</span><span class="va">y</span><span class="op">[</span><span class="va">h</span>,<span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">res.DM</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://pkg.robjhyndman.com/forecast/reference/dm.test.html">dm.test</a></span><span class="op">(</span><span class="va">e1</span>,<span class="va">e2</span>,h <span class="op">=</span> <span class="va">h</span>,alternative <span class="op">=</span> <span class="st">"greater"</span><span class="op">)</span></span>
<span><span class="va">res.DM</span></span></code></pre></div>
<pre><code>## 
##  Diebold-Mariano Test
## 
## data:  e1e2
## DM = -0.82989, Forecast horizon = 2, Loss function power = 2, p-value =
## 0.7946
## alternative hypothesis: greater</code></pre>
<p>Again, we do not find that the alternative model (here the VAR(1) model) is better than the AR(1) model to forecast GDP growth.</p>
</div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="index.html">Introduction to Time Series</a></div>
<div class="next"><a href="append.html"><span class="header-section-number">2</span> Appendix</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#TS"><span class="header-section-number">1</span> Time Series</a></li>
<li><a class="nav-link" href="#introduction-to-time-series"><span class="header-section-number">1.1</span> Introduction to time series</a></li>
<li>
<a class="nav-link" href="#univariate-processes"><span class="header-section-number">1.2</span> Univariate processes</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#moving-average-ma-processes"><span class="header-section-number">1.2.1</span> Moving Average (MA) processes</a></li>
<li><a class="nav-link" href="#ARsection"><span class="header-section-number">1.2.2</span> Auto-Regressive (AR) processes</a></li>
<li><a class="nav-link" href="#ar-ma-processes"><span class="header-section-number">1.2.3</span> AR-MA processes</a></li>
<li><a class="nav-link" href="#PACFapproach"><span class="header-section-number">1.2.4</span> PACF approach to identify AR/MA processes</a></li>
<li><a class="nav-link" href="#wold-decomposition"><span class="header-section-number">1.2.5</span> Wold decomposition</a></li>
<li><a class="nav-link" href="#impulse-response-functions-irfs-in-arma-models"><span class="header-section-number">1.2.6</span> Impulse Response Functions (IRFs) in ARMA models</a></li>
<li><a class="nav-link" href="#ARMAIRF"><span class="header-section-number">1.2.7</span> ARMA processes with exogenous variables (ARMA-X)</a></li>
<li><a class="nav-link" href="#estimARMA"><span class="header-section-number">1.2.8</span> Maximum Likelihood Estimation of ARMA processes</a></li>
<li><a class="nav-link" href="#specification-choice"><span class="header-section-number">1.2.9</span> Specification choice</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#VAR"><span class="header-section-number">1.3</span> Multivariate models</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#definition-of-vars-and-svarma-models"><span class="header-section-number">1.3.1</span> Definition of VARs (and SVARMA) models</a></li>
<li><a class="nav-link" href="#IRFSVARMA"><span class="header-section-number">1.3.2</span> IRFs in SVARMA</a></li>
<li><a class="nav-link" href="#covariance-stationary-varma-models"><span class="header-section-number">1.3.3</span> Covariance-stationary VARMA models</a></li>
<li><a class="nav-link" href="#estimVAR"><span class="header-section-number">1.3.4</span> VAR estimation</a></li>
<li><a class="nav-link" href="#BlockGranger"><span class="header-section-number">1.3.5</span> Block exogeneity and Granger causality</a></li>
<li><a class="nav-link" href="#identification-problem-and-standard-identification-techniques"><span class="header-section-number">1.3.6</span> Identification problem and standard identification techniques</a></li>
<li><a class="nav-link" href="#Signs"><span class="header-section-number">1.3.7</span> Sign restrictions</a></li>
<li><a class="nav-link" href="#forecast-error-variance-maximization"><span class="header-section-number">1.3.8</span> Forecast error variance maximization</a></li>
<li><a class="nav-link" href="#NonGaussian"><span class="header-section-number">1.3.9</span> Identification based on non-normality of the shocks</a></li>
<li><a class="nav-link" href="#factor-augmented-var-favar"><span class="header-section-number">1.3.10</span> Factor-Augmented VAR (FAVAR)</a></li>
<li><a class="nav-link" href="#Projections"><span class="header-section-number">1.3.11</span> Projection Methods</a></li>
<li><a class="nav-link" href="#Inference"><span class="header-section-number">1.3.12</span> Inference</a></li>
</ul>
</li>
<li><a class="nav-link" href="#forecasting"><span class="header-section-number">1.4</span> Forecasting</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Introduction to Time Series</strong>" was written by Jean-Paul Renne. It was last built on 2023-01-09.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
