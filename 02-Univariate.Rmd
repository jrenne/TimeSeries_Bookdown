# Univariate processes {#Univariate}

## Moving Average (MA) processes

Moving average (MA) processes are important basic processes. The moving-average process of order one (MA(1)) is defined as follows:

:::{.definition #MA1 name="Moving average process of order one"}
Process $y_t$ is a first-order moving average process if, for all $t$:
\begin{equation}
y_t = \mu + \varepsilon_t + \theta \varepsilon_{t-1},(\#eq:MA1111)
\end{equation}
where $\{\varepsilon_t\}_{t = -\infty}^{+\infty}$ is a white noise process (see Def. \@ref(def:whitenoise)). 
:::

If $\mathbb{E}(\varepsilon_t^2)=\sigma^2$, it is easily obtained that the unconditional mean and variance of $y_t$ are:
$$
\mathbb{E}(y_t) = \mu, \quad \mathbb{V}ar(y_t) = (1+\theta^2)\sigma^2.
$$

The first auto-covariance is:
$$
\gamma_1=\mathbb{E}\{(y_t - \mu)(y_{t-1} - \mu)\}=\mathbb{E}\{(\varepsilon_t + \theta \color{red}{\varepsilon_{t-1}})(\color{red}{\varepsilon_{t-1}} + \theta \varepsilon_{t-2})\} = \theta \sigma^2.
$$

It is easily seen that higher-order auto-covariances are zero ($\gamma_j=0$ for $j>1$). Therefore: An MA(1) process is covariance-stationary (Def. \@ref(def:covstat)).

From what precedes, the autocorrelation of order $j$ (see Def. \@ref(def:autocor)) of an MA(1) process is given by:
$$
\rho_j =
\left\{
\begin{array}{lll}
1 &\mbox{ if }& j=0,\\
\theta / (1 + \theta^2) &\mbox{ if }& j = 1\\
0 &\mbox{ if }& j>1.
\end{array}
\right.
$$

Notice that process $y_t$ defined through Eq. \@ref(eq:MA1111), with  $\mathbb{V}ar(\varepsilon_t)=\sigma^2$, has the same mean and autocovariances as 
$$
y_t = \mu + \varepsilon^*_t +\frac{1}{\theta}\varepsilon^*_{t-1},
$$
where $\mathbb{V}ar(\varepsilon^*_t)=\theta^2\sigma^2$. That is, knowing the mean and auto-covariances of an MA(1) process is not sufficient to identify the process, since two different processes possess the same moments. Only one of these two specifications is said to be *fundamental*, that is the one that satisfies $|\theta_1|<1$ (see Eq. \@ref(eq:invertible)).

:::{.definition #MAq name="MA(q) process"}
A $q^{th}$-order Moving Average process $\{y_t\}$ is defined through:
$$
y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q},
$$
where $\{\varepsilon_t\}_{t = -\infty}^{+\infty}$ is a white noise process (Def. \@ref(def:whitenoise)).
:::

:::{.proposition #covMAq name="Covariance-stationarity of an MA(q) process"}
Finite-order Moving Average processes are covariance-stationary.

Moreover, the autocovariances of an MA(q) process (as defined in Def. \@ref(def:MAq)) are given by:
\begin{equation}
\gamma_j = \left\{ \begin{array}{ll} \sigma^2(\theta_j\theta_0 + \theta_{j+1}\theta_{1} +  \dots + \theta_{q}\theta_{q-j}) &\mbox{for} \quad j \in \{0,\dots,q\} \\ 0 &\mbox{for} \quad j>q, \end{array} \right.(\#eq:autocovMA)
\end{equation}
where we use the notation $\theta_0=1$, and $\mathbb{V}ar(\varepsilon_t)=\sigma^2$.
:::

:::{.proof}
The unconditional expectation of $y_t$ does not depend on time, since  $\mathbb{E}(y_t)=\mu$. Turning to the autocovariances, we can extend the series of $\theta_j$'s by setting $\theta_j=0$ for $j>q$. We then have:
\begin{eqnarray*}
\mathbb{E}((y_t-\mu)(y_{t-j}-\mu)) &=& \mathbb{E}\left[(\theta_0 \varepsilon_t +\theta_1 \varepsilon_{t-1} + \dots +\theta_j \color{red}{\varepsilon_{t-j}}+\theta_{j+1} \color{blue}{\varepsilon_{t-j-1}} + \dots) \right.\times \\
&&\left. (\theta_0 \color{red}{\varepsilon_{t-j}} +\theta_1 \color{blue}{\varepsilon_{t-j-1}} + \dots)\right].
\end{eqnarray*}
Using the fact that $\mathbb{E}(\varepsilon_t\varepsilon_s)=0$ if $t \ne s$ (because $\{\varepsilon_t\}_{t = -\infty}^{+\infty}$ is a white noise process) leads to the result.
:::

Figure \@ref(fig:simMA) displays simulated paths of two MA processes (an MA(1) and an MA(4)). Such simulations can also be produced by using panel "ARMA(p,q)" of [this web interface](https://jrenne.shinyapps.io/MacroEc/).


```{r simMA, echo=TRUE, fig.cap="Simulation of MA processes.", fig.asp = .6, out.width = "95%", fig.align = 'left-aligned'}
library(AEC)
T <- 100;nb.sim <- 1
y.0 <- c(0)
c <- 1;phi <- c(0);sigma <- 1
theta <- c(1,1) # MA(1) specification
y.sim <- sim.arma(c,phi,theta,sigma,T,y.0,nb.sim)
par(mfrow=c(1,2))
par(plt=c(.2,.9,.2,.85))
plot(y.sim[,1],xlab="",ylab="",type="l",lwd=2,
     main=expression(paste(theta[0],"=1, ",theta[1],"=1",sep="")))
abline(h=c)
theta <- c(1,1,1,1,1) # MA(4) specification
y.sim <- sim.arma(c,phi,theta,sigma,T,y.0,nb.sim)
plot(y.sim[,1],xlab="",ylab="",type="l",lwd=2,
     main=expression(paste(theta[0],"=...=",theta[4],"=1",sep="")))
abline(h=c)
```


What if the order $q$ of an MA(q) process gets infinite? The notion of **infinite-order Moving Average process** exists and is important in time series analysis, as it relates to impulse response functions (as illustrated in Section \@ref(IRFARMA)). The (infinite) sequence of $\theta_j$ has to satisfy some conditions for such a process to be well-defined (see Theorem \@ref(thm:infMA) below). These conditions relate to the "summability" of the components of the sequence $\{\theta_{i}\}_{i\in\mathbb{N}}$:

:::{.definition #summability name="Absolute and square summability"}
The sequence $\{\theta_{i}\}_{i\in\mathbb{N}}$ is absolutely summable if $\sum_{i=0}^{\infty}|\theta_i| < + \infty$, and it is square summable if $\sum_{i=0}^{\infty} \theta_i^2 < + \infty$.
:::

According to Prop. \@ref(prp:absMs) (in the appendix), absolute summability implies square summability.

:::{.theorem #infMA name="Existence condition for an infinite MA process"}
If $\{\theta_{i}\}_{i\in\mathbb{N}}$ is square summable (see Def. \@ref(def:summability)) and if $\{\varepsilon_t\}_{t = -\infty}^{+\infty}$ is a white noise process (see Def. \@ref(def:whitenoise)), then
$$
\mu + \sum_{i=0}^{+\infty} \theta_{i} \varepsilon_{t-i}
$$
defines a well-behaved [covariance-stationary] process, called infinite-order MA process (MA($\infty$)).
:::
:::{.proof}
See Appendix 3.A in Hamilton. "Well behaved" means that $\Sigma_{i=0}^{T} \theta_{t-i} \varepsilon_{t-i}$ converges in mean square (Def. \@ref(def:convergenceLr)) to some random variable $Z_t$. The proof makes use of the fact that:
$$
\mathbb{E}\left[\left(\sum_{i=N}^{M}\theta_{i} \varepsilon_{t-i}\right)^2\right] = \sum_{i=N}^{M}|\theta_{i}|^2 \sigma^2,
$$
and that, when $\{\theta_{i}\}$ is square summable, $\forall \eta>0$, $\exists N$ s.t. the right-hand-side term in the last equation is lower than $\eta$ for all $M \ge N$ (static Cauchy criterion, Theorem \@ref(thm:cauchycritstatic)). This implies that $\Sigma_{i=0}^{T} \theta_{i} \varepsilon_{t-i}$ converges in mean square (stochastic Cauchy criterion, see Theorem \@ref(thm:cauchycritstochastic)).
:::

:::{.proposition #momentsMAinf name="First two moments of an infinite MA process"}

If $\{\theta_{i}\}_{i\in\mathbb{N}}$ is absolutely summable, i.e., if $\sum_{i=0}^{\infty}|\theta_i| < + \infty$, then

i. $y_t = \mu + \sum_{i=0}^{+\infty} \theta_{i} \varepsilon_{t-i}$ exists (Theorem \@ref(thm:infMA)) and is such that:
\begin{eqnarray*}
\mathbb{E}(y_t) &=& \mu\\
\gamma_0 = \mathbb{E}([y_t-\mu]^2) &=& \sigma^2(\theta_0^2 +\theta_1^2 + \dots)\\
\gamma_j = \mathbb{E}([y_t-\mu][y_{t-j}-\mu]) &=& \sigma^2(\theta_0\theta_j + \theta_{1}\theta_{j+1} + \dots).
\end{eqnarray*}
ii. Process $y_t$ has absolutely summable auto-covariances, which implies that the results of Theorem \@ref(thm:CLTcovstat) (Central Limit) apply.
:::

:::{.proof}
The absolute summability of $\{\theta_{i}\}$ and the fact that $\mathbb{E}(\varepsilon^2)<\infty$ imply that the order of integration and summation is interchangeable (see Hamilton, 1994, Footnote p. 52), which proves (i). For (ii), see end of Appendix 3.A in Hamilton (1994).
:::


## Auto-Regressive (AR) processes {#ARsection}

:::{.definition #AR1 name="First-order AR process (AR(1))"}
Process $y_t$ is an AR(1) process if its dynamics is defined by the following difference equation:
$$
y_t = c + \phi y_{t-1} + \varepsilon_t,
$$
where $\{\varepsilon_t\}_{t = -\infty}^{+\infty}$ is a white noise process (see Def. \@ref(def:whitenoise)). 
:::


If $|\phi|\ge1$, $y_t$ is not stationary. Indeed, we have:
\begin{eqnarray*}
y_{t+k} &=& c + \varepsilon_{t+k} + \phi  ( c + \varepsilon_{t+k-1})+ \phi^2  ( c + \varepsilon_{t+k-2})+ \dots + \\
&& \phi^{k-1}  ( c + \varepsilon_{t+1}) + \phi^k y_t.
\end{eqnarray*}
Therefore, the conditional variance
$$
\mathbb{V}ar_t(y_{t+k}) = \sigma^2(1 + \phi^2 + \phi^4 + \dots + \phi^{2(k-1)})
$$
 (where $\sigma^2$ is the variance of $\varepsilon_t$) does not converge when $k$ gets infinitely large. This implies that $\mathbb{V}ar(y_{t})$ does not exist.^[Indeed, by the law of total variance $\mathbb{V}ar(y_{t})=\mathbb{V}ar(\mathbb{E}_{t-k}(y_{t}))+\mathbb{E}(\mathbb{V}ar_{t-k}(y_{t}))>$. Since we have $\mathbb{V}ar(\mathbb{E}_{t-k}(y_{t})) \ge 0$ and $\mathbb{E}(\mathbb{V}ar_{t-k}(y_{t}))=\mathbb{V}ar_{t-k}(y_{t})$ that goes to $+\infty$ (by what precedes), it comes that $\mathbb{V}ar(y_{t})$ is not finite.] By contrast, if $|\phi| < 1$, we have:
$$
y_t = c + \varepsilon_t + \phi  ( c + \varepsilon_{t-1})+ \phi^2  ( c + \varepsilon_{t-2})+ \dots + \phi^k  ( c + \varepsilon_{t-k}) + \dots
$$
Hence, if $|\phi| < 1$, the unconditional mean and variance of $y_t$ are:
$$
\mathbb{E}(y_t) = \frac{c}{1-\phi} =: \mu \quad \mbox{and} \quad \mathbb{V}ar(y_t) = \frac{\sigma^2}{1-\phi^2}.
$$

Let us compute the $j^{th}$ autocovariance of the AR(1) process:
\begin{eqnarray*}
\mathbb{E}([y_{t} - \mu][y_{t-j} - \mu]) &=& \mathbb{E}([\varepsilon_t + \phi  \varepsilon_{t-1}+ \phi^2 \varepsilon_{t-2} + \dots + \color{red}{\phi^j \varepsilon_{t-j}} + \color{blue}{\phi^{j+1} \varepsilon_{t-j-1}} \dots]\times \\
&&[\color{red}{\varepsilon_{t-j}} + \color{blue}{\phi \varepsilon_{t-j-1}} + \phi^2 \varepsilon_{t-j-2} + \dots + \phi^k \varepsilon_{t-j-k} + \dots])\\
&=& \mathbb{E}(\color{red}{\phi^j \varepsilon_{t-j}^2}+\color{blue}{\phi^{j+2} \varepsilon_{t-j-1}^2}+\phi^{j+4} \varepsilon_{t-j-2}^2+\dots)\\
&=& \frac{\phi^j \sigma^2}{1 - \phi^2}.
\end{eqnarray*}

Therefore, the auto-correlation is given by $\rho_j = \phi^j$.

By what precedes, we have:



:::{.proposition #statioAR1 name="Covariance-stationarity of an AR(1) process"}
The AR(1) process, as defined in Def. \@ref(def:AR1), is covariance-stationary iff $|\phi|<1$.
:::


:::{.definition #ARp name="AR(p) process"}
Process $y_t$ is a $p^{th}$-order autoregressive process (AR(p)) if its dynamics is defined by the following difference equation (with $\phi_p \ne 0$):
\begin{equation}
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \varepsilon_t,(\#eq:AR)
\end{equation}
where $\{\varepsilon_t\}_{t = -\infty}^{+\infty}$ is a white noise process (see Def. \@ref(def:whitenoise)). 
:::

As we will see, the covariance-stationarity of process $y_t$ hinges on the eigenvalues of matrix $F$, defined as:
\begin{equation}
F = \left[
\begin{array}{ccccc}
\phi_1 & \phi_2 & \dots& & \phi_p \\
1 & 0 &\dots && 0 \\
0 & 1 &\dots && 0 \\
\vdots &  & \ddots && \vdots \\
0 & 0 &\dots &1& 0 \\
\end{array}
\right].(\#eq:F)
\end{equation}

Note that this matrix $F$ is such that if $y_t$ follows Eq. \@ref(eq:AR), then  process $\bv{y}_t$ follows: 
$$
\bv{y}_t = \bv{c} + F \bv{y}_{t-1} + \boldsymbol\xi_t
$$
with
$$
\bv{c} =
\left[\begin{array}{c}
c\\
0\\
\vdots\\
0
\end{array}\right],
\quad
\boldsymbol\xi_t =
\left[\begin{array}{c}
\varepsilon_t\\
0\\
\vdots\\
0
\end{array}\right],
\quad
\bv{y}_t =
\left[\begin{array}{c}
y_t\\
y_{t-1}\\
\vdots\\
y_{t-p+1}
\end{array}\right].
$$


<!-- :::{.definition #dynmult name="Dynamic multiplier"} -->
<!-- The **dynamic multiplier** of $y_{t+j}$ w.r.t. $\varepsilon_{t}$ is $\dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}}$. -->
<!-- ::: -->

<!-- Eq. \@ref(eq:Fyt) implies that we have: $\dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} = (F^j)_{[1,1]}$ -->

<!-- (for any $M,i,j$, $(M)_{[i,j]}$ denotes the $(i,j)$ element of matrix $M$). -->

<!-- Let us assume that the eigenvalues of $F$ (see Def. \@ref(def:determinant)), denoted by $\lambda_1,\dots,\lambda_p$, are distinct. -->

<!-- Then, there exists a nonsingular matrix $P$ such that: -->
<!-- $$ -->
<!-- F = P -->
<!-- \left[ -->
<!-- \begin{array}{cccc} -->
<!-- \lambda_1 & 0 & \dots& 0 \\ -->
<!-- 0 & \lambda_2 & 0 & \dots \\ -->
<!-- &&\ddots\\ -->
<!-- 0 & \dots & 0& \lambda_p\\ -->
<!-- \end{array} -->
<!-- \right] -->
<!-- P^{-1} = P D P^{-1}. -->
<!-- $$ -->

<!-- It can be seen that: -->
<!-- $$ -->
<!-- F^j = P -->
<!-- \left[ -->
<!-- \begin{array}{cccc} -->
<!-- \lambda_1^j & 0 & \dots& 0 \\ -->
<!-- 0 & \lambda_2^j & 0 & \dots \\ -->
<!-- &&\ddots\\ -->
<!-- 0 & \dots & 0& \lambda_p^j\\ -->
<!-- \end{array} -->
<!-- \right] -->
<!-- P^{-1}. -->
<!-- $$ -->

<!-- Hence, the dynamic multiplier of $y_{t+j}$ w.r.t. $\varepsilon_{t}$ is given by: -->
<!-- $$ -->
<!-- \dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} = \sum_i (P)_{[1,i]}(P^{-1})_{[i,1]}\lambda_i^j. -->
<!-- $$ -->

<!-- Denoting by $c_i$ the scalar $(P)_{[1,i]}(P^{-1})_{[i,1]}$, we have $\dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} = \sum_i c_i\lambda_i^j$. If the eigenvalues of $F$ are distinct and nonzero, then $c_i \ne 0$. Therefore, if $\exists i$ s.t. $|\lambda_i|>1$ then: -->
<!-- $$ -->
<!-- \left| \dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} \right| \underset{j \rightarrow \infty}{\rightarrow} \infty. -->
<!-- $$ -->

:::{.proposition #Feigen name="The eigenvalues of matrix F"}
The eigenvalues of $F$ (defined by Eq. \@ref(eq:F)) are the solutions of:
\begin{equation}
\lambda^p - \phi_1 \lambda^{p-1} - \dots - \phi_{p-1}\lambda - \phi_p = 0.(\#eq:Feigen)
\end{equation}
:::


:::{.proposition #stability name="Covariance-stationarity of an AR(p) process"}
These four statements are equivalent:

i. Process $\{y_t\}$, defined in Def. \@ref(def:ARp), is covariance-stationary.
ii. The eigenvalues of $F$ (as defined Eq. \@ref(eq:F)) lie strictly within the unit circle.
iii. The roots of Eq. \@ref(eq:outside) (below) lie strictly outside the unit circle.
\begin{equation}
1 - \phi_1 z - \dots - \phi_{p-1}z^{p-1} - \phi_p z^p = 0.(\#eq:outside)
\end{equation}
iv. The roots of Eq. \@ref(eq:inside) (below) lie strictly inside the unit circle.
\begin{equation}
\lambda^p - \phi_1 \lambda^{p-1} - \dots - \phi_{p-1}\lambda - \phi_p = 0.(\#eq:inside)
\end{equation}
:::
:::{.proof}
We consider the case where the eigenvalues of $F$ are distinct.^[The Jordan matrix decomposition can be used in the general case.] When the eigenvalues of $F$ are distinct, $F$ admits the following spectral decomposition: $F = PDP^{-1}$, where $D$ is diagonal. Using the notations introduced in Eq. \@ref(eq:F), we have:
$$
\bv{y}_{t} = \bv{c} + F \bv{y}_{t-1} + \boldsymbol\xi_{t}.
$$
Let's introduce $\bv{d} = P^{-1}\bv{c}$, $\bv{z}_t = P^{-1}\bv{y}_t$ and $\boldsymbol\eta_t = P^{-1}\boldsymbol\xi_t$. We have:
$$
\bv{z}_{t} = \bv{d} + D \bv{z}_{t-1} + \boldsymbol\eta_{t}.
$$
Because $D$ is diagonal, the different component of $\bv{z}_t$, denoted by $z_{i,t}$, follow AR(1) processes. The (scalar) autoregressive parameters of these AR(1) processes are the diagonal entries of $D$---which also are the eigenvalues of $F$---that we denote by $\lambda_i$.

Process $y_t$ is covariance-stationary iff $\bv{y}_{t}$ also is covariance-stationary, which is the case iff all $z_{i,t}$, $i \in \{1,\dots,p\}$, are covariance-stationary. By Prop. \@ref(prp:statioAR1), process $z_{i,t}$ is covariance-stationary iff $|\lambda_i|<1$. This proves that (i) is equivalent to (ii). Prop. \@ref(prp:Feigen) further proves that (ii) is equivalent to (iv). Finally, it is easily seen that (iii) is equivalent to (iv) (as long as $\phi_p \ne 0$).
:::


<!-- Note that we have: -->
<!-- \begin{equation} -->
<!-- \bv{y}_{t+j} = \boldsymbol\xi_{t+j} + F \boldsymbol\xi_{t+j-1} + F^2 \boldsymbol\xi_{t+j-2} + \dots + F^{j-1} \boldsymbol\xi_{t+1} + F^{j} \bv{y}_{t} -->
<!-- \end{equation} -->
<!-- or -->
<!-- \begin{equation} -->
<!-- \bv{y}_{t+j} = \boldsymbol\xi_{t+j} + F \boldsymbol\xi_{t+j-1} + F^2 \boldsymbol\xi_{t+j-2} + \dots + F^{j} \boldsymbol\xi_{t} + F^{j+1} \bv{y}_{t-1}.(\#eq:Fyt) -->
<!-- \end{equation} -->


Using the lag operator (see Eq \@ref(eq:lagOp)), if $y_t$ is a covariance-stationary AR(p) process (Def. \@ref(def:ARp)), we can write:
$$
y_t = \mu + \psi(L)\varepsilon_t,
$$
where
\begin{equation}
\psi(L) = (1 - \phi_1 L - \dots - \phi_p L^p)^{-1},
\end{equation}
and
\begin{equation}
\mu = \mathbb{E}(y_t) = \dfrac{c}{1-\phi_1 -\dots - \phi_p}.(\#eq:EAR)
\end{equation}


In the following lines of codes, we compute the eigenvalues of the $F$ matrices associated with the following processes (where $\varepsilon_t$ is a white noise):
\begin{eqnarray*}
x_t &=& 0.9 x_{t-1} -0.2 x_{t-2} + \varepsilon_t\\
y_t &=& 1.1 y_{t-1} -0.3 y_{t-2} + \varepsilon_t\\
w_t &=& 1.4 w_{t-1} -0.7 w_{t-2} + \varepsilon_t\\
z_t &=& 0.9 z_{t-1} +0.2 z_{t-2} + \varepsilon_t.
\end{eqnarray*}

```{r simulAR}
F <- matrix(c(.9,1,-.2,0),2,2)
lambda_x <- eigen(F)$values
F[1,] <- c(1.1,-.3)
lambda_y <- eigen(F)$values
F[1,] <- c(1.4,-.7)
lambda_w <- eigen(F)$values
F[1,] <- c(.9,.2)
lambda_z <- eigen(F)$values
rbind(lambda_x,lambda_y,lambda_w,lambda_z)
```
The absolute values of the eigenvalues associated with process $w_t$ are both equal to `r round(abs(lambda_w),3)[1]`. Therefore, according to Proposition \@ref(prp:stability), processes $x_t$, $y_t$, and $w_t$ are covariance-stationary, but not $z_t$ (because, for the latter process, the absolute value of one of the eigenvalues of matrix $F$ is larger than 1).

The computation of the autocovariances of $y_t$ is based on the so-called **Yule-Walker equations** (Eq. \@ref(eq:gammas)). Let's rewrite Eq. \@ref(eq:AR):
$$
(y_t-\mu) = \phi_1 (y_{t-1}-\mu) + \phi_2 (y_{t-2}-\mu) + \dots + \phi_p (y_{t-p}-\mu) + \varepsilon_t.
$$
Multiplying both sides by $y_{t-j}-\mu$ and taking expectations leads to the (Yule-Walker) equations:
\begin{equation}
\gamma_j = \left\{
\begin{array}{l}
\phi_1 \gamma_{j-1}+\phi_2 \gamma_{j-2}+ \dots + \phi_p \gamma_{j-p} \quad if \quad j>0\\
\phi_1 \gamma_{1}+\phi_2 \gamma_{2}+ \dots + \phi_p \gamma_{p} + \sigma^2 \quad for \quad j=0,
\end{array}
\right.(\#eq:gammas)
\end{equation}
where $\sigma^2$ is the variance of $\varepsilon_t$.

Using $\gamma_j = \gamma_{-j}$ (Prop. \@ref(prp:gammaMinus)), one can express $(\gamma_0,\gamma_1,\dots,\gamma_{p})$ as functions of $(\sigma^2,\phi_1,\dots,\phi_p)$. Indeed, we have:
$$
\left[\begin{array}{c}
\gamma_0 \\
\gamma_1 \\
\gamma_2 \\
\vdots\\
\gamma_p
\end{array}\right] =
\underbrace{\left[\begin{array}{cccccccc}
0 & \phi_1 & \phi_2 & \dots &&& \phi_p \\
\phi_1 & \phi_2 & \dots &&& \phi_p & 0 \\
\phi_2 & (\phi_1 + \phi_3) & \phi_4 & \dots & \phi_p& 0& 0 \\
\vdots\\
\phi_p & \phi_{p-1} & \dots &&\phi_2& \phi_1 & 0
\end{array}\right]}_{=H}\left[\begin{array}{c}
\gamma_0 \\
\gamma_1 \\
\gamma_2 \\
\vdots\\
\gamma_p
\end{array}\right] +
\left[\begin{array}{c}
\sigma^2 \\
0 \\
0 \\
\vdots\\
0
\end{array}\right],
$$

which is easily solved by inversing matrix $H$.

## AR-MA processes

:::{.definition #ARMApq name="ARMA(p,q) process"}
$\{y_t\}$ is an ARMA($p$,$q$) process if its dynamics is described by the following equation:
\begin{equation}
y_t = c + \underbrace{\phi_1 y_{t-1} + \dots + \phi_p y_{t-p}}_{\mbox{AR part}} + \underbrace{\varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q}}_{\mbox{MA part}},(\#eq:ARMApq)
\end{equation}
where  $\{\varepsilon_t\}_{t \in ] -\infty,+\infty[}$, of variance $\sigma^2$ (say), is a white noise process (see Def. \@ref(def:whitenoise)).
:::


:::{.proposition #statioARMApq name="Stationarity of an ARMA(p,q) process"}

The ARMA($p$,$q$) process defined in \@ref(def:ARMApq) is covariance stationary iff the roots of
$$
1 - \phi_1 z - \dots - \phi_p z^p=0
$$
lie strictly outside the unit circle or, equivalently, iff those of
$$
\lambda^p - \phi_1 \lambda^{p-1} - \dots - \phi_p=0
$$
lie strictly within the unit circle.
:::


:::{.proof}
The proof of Prop. \@ref(prp:stability) can be adapted to the present case.
:::

We can write:
$$
(1 - \phi_1 L - \dots - \phi_p L^p)y_t = c + (1 + \theta_1 L + \dots + \theta_q L^q)\varepsilon_t.
$$

If the roots of $1 - \phi_1 z - \dots - \phi_p z^p=0$ lie outside the unit circle, we have:
\begin{equation}
y_t = \mu + \psi(L)\varepsilon_t,(\#eq:ARMAwold)
\end{equation}
where
$$
\psi(L) = \frac{1 + \theta_1 L + \dots + \theta_q L^q}{1 - \phi_1 L - \dots - \phi_p L^p} \quad and \quad \mu = \dfrac{c}{1-\phi_1 -\dots - \phi_p}.
$$

Eq. \@ref(eq:ARMAwold) is the **Wold representation** of this ARMA process (see Theorem \@ref(thm:Wold) below). In Section \@ref(IRFARMA) (and more precisely in Proposition \@ref(prp:computPsi)), we will see how to obtain the first $h$ terms of the infinite polynomial $\Psi(L)$.

Importantly, note that the stationarity of the process depends only on the AR specification (or on the eigenvalues of matrix $F$, exactly as in Prop. \@ref(prp:stability)). When an ARMA process is stationary, the weights in $\psi(L)$ decay at a geometric rate.


## PACF approach to identify AR/MA processes {#PACFapproach}

We have seen that the $k^{th}$-order auto-correlation of an MA(q) process is null if $k>q$. This is exploited, in practice, to determine the order of an MA process. Moreover, since this is not the case for an AR process, this can be used to distinguish an AR from an MA process.

There exists an equivalent condition that satisfied by AR processes, and that can be used to determine whether a process can be modeled as such a process. This condition relates to partial auto-correlations:

<!-- :::{.definition #partialC name="Partial correlation"} -->
<!-- The partial correlation between $X$ and $Y$, given $Z$, is the correlation between: -->

<!-- a. the residuals of the linear regression of $X$ on $Z$ and -->
<!-- b. the residuals of the linear regression of $Y$ on $Z$. -->
<!-- ::: -->

:::{.definition #partialAC name="Partial auto-correlation"}
The partial auto-correlation ($\phi_{h,h}$) of process $\{y_t\}$ is defined as the partial correlation of $y_{t+h}$ and $y_t$ given $y_{t+h-1},\dots,y_{t+1}$. (see Def. \@ref(def:partialcorrel) for the definition of partial correlation.)
:::


If $h>p$, the regression of $y_{t+h}$ on $y_{t+h-1},\dots,y_{t+1}$ is:
$$
y_{t+h} = c + \phi_1 y_{t+h-1}+\dots+ \phi_p  y_{t+h-p} + \varepsilon_{t+h}.
$$
The residuals of the latter regressions ($\varepsilon_{t+h}$) are uncorrelated to $y_t$. Then the partial autocorrelation is zero for $h>p$.

Besides, it can be shown that $\phi_{p,p}=\phi_p$. Hence  $\phi_{p,p}=\phi_p$ but  $\phi_{h,h}=0$ for $h>p$. This can be used to determine the order of an AR process. By contrast (importantly) if $y_t$ follows an MA(q) process, then $\phi_{k,k}$ asymptotically approaches zero instead of cutting off abruptly.

As illustrated below, the functions `acf` and `pacf` of R allow to conveniently implement the (P)ACF approach. (In these lines of codes, note also the use of function `sim.arma` to simulate ARMA processes.)

```{r pacf, echo=TRUE, fig.cap="ACF/PACF analysis of two processes (MA process on the left, AR on the right). The correlations are computed on samples of length 1000.", fig.asp = .6, out.width = "100%", fig.align = 'left-aligned'}
library(AEC)
par(mfrow=c(3,2))
par(plt=c(.2,.9,.2,.95))
theta <- c(1,2,1);phi=0
y.sim <- sim.arma(c=0,phi,theta,sigma=1,T=1000,y.0=0,nb.sim=1)
par(mfg=c(1,1));plot(y.sim,type="l",lwd=2)
par(mfg=c(2,1));acf(y.sim)
par(mfg=c(3,1));pacf(y.sim)
theta <- c(1);phi=0.9
y.sim <- sim.arma(c=0,phi,theta,sigma=1,T=1000,y.0=0,nb.sim=1)
par(mfg=c(1,2));plot(y.sim,type="l",lwd=2)
par(mfg=c(2,2));acf(y.sim)
par(mfg=c(3,2));pacf(y.sim)
```


## Wold decomposition

The Wold decomposition is an important result in time series analysis:

:::{.theorem #Wold name="Wold decomposition"}
Any covariance-stationary process admits the following representation:
$$
y_t = \mu + \sum_{0}^{+\infty} \theta_i \varepsilon_{t-i} + \kappa_t,
$$
where

* $\theta_0 = 1$, $\sum_{i=0}^{\infty} \theta_i^2 < +\infty$ (square summability, see Def. \@ref(def:summability)).
* $\{\varepsilon_t\}$ is a white noise (see Def. \@ref(def:whitenoise)); $\varepsilon_t$ is the error made when forecasting $y_t$ based on a linear combination of lagged $y_t$'s ($\varepsilon_t = y_t - \hat{\mathbb{E}}[y_t|y_{t-1},y_{t-2},\dots]$).
* For any $j \ge 1$, $\kappa_t$ is not correlated with $\varepsilon_{t-j}$; but $\kappa_t$ can be perfectly forecasted based on a linear combination of lagged $y_t$'s (i.e. $\kappa_t = \hat{\mathbb{E}}(\kappa_t|y_{t-1},y_{t-2},\dots)$). $\kappa_t$ is called the **deterministic component** of $y_t$.
:::

:::{.proof}
See @Anderson_1971. Partial proof in [L. Christiano](http://faculty.wcas.northwestern.edu/~lchrist/finc520/wold.pdf)'s lecture notes.
:::

For an ARMA process, the Wold representation is given by Eq. \@ref(eq:ARMAwold). As detailed in Prop. \@ref(prp:computPsi), it can be computed by recursively replacing the lagged $y_t$'s in Eq. \@ref(eq:ARMApq). In this case, the deterministic component ($\kappa$) is null.


## Impulse Response Functions (IRFs) in ARMA models {#IRFARMA}

Consider the ARMA(p,q) process defined in Def. \@ref(def:ARMApq). Let us construct a novel (counterfactual) sequence of shocks $\{\tilde\varepsilon_t^{(s)}\}$:
$$
\tilde\varepsilon_t^{(s)} = \left\{
\begin{array}{lcc}
\varepsilon_{t} & if & t \ne s,\\
\varepsilon_{t} + 1 &if& t=s.
\end{array}
\right.
$$
Hence, the only difference between processes $\{\varepsilon_t^{(s)}\}$ and $\{\tilde\varepsilon_t^{(s)}\}$ pertains to date $s$, where $\varepsilon_s$ is replaced with $\varepsilon_s + 1$ in $\{\tilde\varepsilon_t^{(s)}\}$.

We denote by $\{\tilde{y}_t^{(s)}\}$ the process following Eq. \@ref(eq:ARMApq) where $\{\varepsilon_t\}$ is replaced with $\{\tilde\varepsilon_t^{(s)}\}$. The time series $\{\tilde{y}_t^{(s)}\}$ is the counterfactual series $\{y_t\}$ that would have prevailed if $\varepsilon_t$ had been shifted by one unit on date $s$ (and that would be the only change).

The relationship between $\{y_t\}$ and $\{\tilde{y}_t^{(s)}\}$ defines the **dynamic multipliers** of $\{y_t\}$. The dynamic multiplier $\frac{\partial y_t}{\partial \varepsilon_{s}}$ corresponds to the impact on $y_t$ of a unit increase in  $\varepsilon_s$ (on date $s$). Using the notation introduced before for $\tilde{y}_t^{(s)}$, we have:
$$
\tilde{y}_t^{(s)} = y_t + \frac{\partial y_t}{\partial \varepsilon_{s}}.
$$
Let us show that the dynamic multipliers are closely related to the infinite MA representation (or **Wold decomposition**, Theorem \@ref(thm:Wold)) of $y_t$:
$$
y_t = \mu + \sum_{i=0}^{+\infty} \psi_i \varepsilon_{t-i}.
$$
For $t<s$, we have $y_t = \tilde{y}_t^{(s)}$ (because $\tilde{\varepsilon}_{t-i}= \varepsilon_{t-i}$ for all $i \ge 0$ if $t<s$).

For $t \ge s$:
$$
\tilde{y}_t^{(s)} = \mu + \left( \sum_{i=0}^{t-s-1} \psi_i \varepsilon_{t-i} \right) + \psi_{t-s}(\varepsilon_{s}+1) + \left( \sum_{i=t-s+1}^{+\infty} \psi_i \varepsilon_{t-i} \right)=y_t + \frac{\partial y_t}{\partial \varepsilon_{s}}.
$$
Therefore, it comes that the only difference between $\tilde{y}_t^{(s)}$ and $y_t$ is $\psi_{t-s}$. As a result, for $t \ge s$, we have:
$$
\boxed{\dfrac{\partial y_t}{\partial \varepsilon_{s}}=\psi_{t-s}.}
$$
That is, $\{y_t\}$'s dynamic multiplier of order $k$ is the same object as the $k^{th}$ loading $\psi_k$ in the Wold decomposition of $\{y_t\}$. The sequence $\left\{\dfrac{\partial y_{t+h}}{\partial \varepsilon_{t}}\right\}_{h \ge 0} \equiv \left\{\psi_h\right\}_{h \ge 0}$ defines the **impulse response function (IRF)** of $y_t$ to the shock $\varepsilon_t$.

For ARMA processes, one can compute the IRFs (or the Wold decomposition) by using a simple recursive algorithm:

:::{.proposition #computPsi name="IRF of an ARMA(p,q) process"}
The coefficients $\psi_h$, that define the IRF of process $y_t$ to $\varepsilon_t$, can be computed recursively as follows:

1. Set $\psi_{-1}=\dots=\psi_{-p}=0$.
2. For $h \ge 0$, (recursively) apply:
$$
\psi_h = \phi_1 \psi_{h-1} + \dots + \phi_p \psi_{h-p} + \theta_h,
$$
where $\theta_h = 0$ for $h>q$.
:::

:::{.proof}
This is obtained by applying the operator $\frac{\partial}{\partial \varepsilon_{t}}$ on both sides of Eq. \@ref(eq:ARMApq):
$$
y_{t+h} = c + \phi_1 y_{t+h-1} + \dots + \phi_p y_{t+h-p} + \varepsilon_{t+h} + \theta_1 \varepsilon_{t+h-1} + \dots + \theta_q \varepsilon_{t+h-q}.
$$
:::

Note that Proposition \@ref(prp:computPsi) constitutes a simple way to compute the MA($\infty$) representation (or Wold representation) of an ARMA process.

One can use function `sim.arma` of package `AEC` to compute ARMA's IRFs (with the argument `make.IRF = 1`):

```{r IRFarma, echo=TRUE, fig.cap="IRFs associated with the three processes. Process 1 (MA(2)): $y_t = \\varepsilon_t + \\varepsilon_{t-1} + \\varepsilon_{t-2}$. Process 2 (ARMA(1,1)): $y_{t}=0.6y_{t-1} + \\varepsilon_t + 0.5\\varepsilon_{t-1}$. Process 3 (ARMA(4,2)): $y_{t}=0.5y_{t-3} + 0.4y_{t-4} + \\varepsilon_t + \\varepsilon_{t-1} + \\varepsilon_{t-2}$.", fig.asp = .6, out.width = "100%", fig.align = 'left-aligned'}
T <- 21 # number of periods for IRF
theta <- c(1,1,1);phi <- c(0);c <- 0
y.sim <- sim.arma(c,phi,theta,sigma=1,T,y.0=rep(0,length(phi)),
                  nb.sim=1,make.IRF = 1)
par(mfrow=c(1,3));par(plt=c(.25,.95,.2,.85))
plot(0:(T-1),y.sim[,1],type="l",lwd=2,
     main="(a) Process 1",xlab="Time after shock on epsilon",
     ylab="Dynamic multiplier (shock on epsilon at t=0)",col="red")

theta <- c(1,.5);phi <- c(0.6)
y.sim <- sim.arma(c,phi,theta,sigma=1,T,y.0=rep(0,length(phi)),
                  nb.sim=1,make.IRF = 1)
plot(0:(T-1),y.sim[,1],type="l",lwd=2,
     main="(b) Process 2",xlab="Time after shock on epsilon",
     ylab="",col="red")

theta <- c(1,1,1);phi <- c(0,0,.5,.4)
y.sim <- sim.arma(c,phi,theta,sigma=1,T,y.0=rep(0,length(phi)),
                  nb.sim=1,make.IRF = 1)
plot(0:(T-1),y.sim[,1],type="l",lwd=2,
     main="(c) Process 3",xlab="Time after shock on epsilon",
     ylab="",col="red")
```


Consider the annual Swiss GDP growth from the JST macro-history database. Let us first determine relevant orders for AR and MA processes using the (P)ACF approach.

```{r IRFgdp1, echo=TRUE, fig.cap="(P)ACF analysis of Swiss GDP growth.", fig.asp = .6, out.width = "100%", fig.align = 'left-aligned'}
library(AEC)
data(JST);data <- subset(JST,iso=="CHE")
par(plt=c(.1,.95,.1,.95))
T <- dim(data)[1]
growth <- log(data$gdp[2:T]/data$gdp[1:(T-1)])
par(mfrow=c(3,1));par(plt=c(.1,.95,.15,.95))
plot(data$year[2:T],growth,type="l",xlab="",ylab="",lwd=2)
abline(h=0,lty=2)
acf(growth);pacf(growth)
```

The two bottom plots of Figure \@ref(fig:IRFgdp1) suggest that either an MA(2) or an AR(1) could be used to model the GDP growth rate series. Figure \@ref(fig:IRFgdp2) shows the IRFs based on these two respective specifications.

```{r IRFgdp2, echo=TRUE, fig.cap="Dynamic response of Swiss annual growth to a shock on the innovation $\\varepsilon_t$ at date $t=0$. The solid line corresponds to an AR(1) specification; the dashed line corresponds to a MA(2) specification.", fig.asp = .6, out.width = "100%", fig.align = 'left-aligned'}
# Fit an AR process:
res <- arima(growth,order=c(1,0,0))
phi <- res$coef[1]
T <- 11
y.sim <- sim.arma(c=0,phi,theta=1,sigma=1,T,y.0=rep(0,length(phi)),
                  nb.sim=1,make.IRF = 1)
par(plt=c(.15,.95,.25,.95))
plot(0:(T-1),y.sim[,1],type="l",lwd=3,
     xlab="Time after shock on epsilon",
     ylab="Dynamic multiplier (shock on epsilon at t=0)",col="red")
# Fit a MA process:
res <- arima(growth,order=c(0,0,2))
phi <- 0;theta <- c(1,res$coef[1:2])
y.sim <- sim.arma(c=0,phi,theta,sigma=1,T,y.0=rep(0,length(phi)),
                  nb.sim=1,make.IRF = 1)
lines(0:(T-1),y.sim[,1],lwd=3,col="red",lty=2)
abline(h=0)
```


The same kind of algorithm (as in Prop. \@ref(prp:computPsi)) can be used to compute the impact of an increase in an exogenous variable $x_t$ within an ARMAX(p,q,r) model (see next section).

## ARMA processes with exogenous variables (ARMA-X) {#ARMAIRF}

<!-- Recall that $\{y_t\}$ follows an ARMAX(p,q,r) model if its dynamics is of the form (see Def. \@ref(def:ARMAX)): -->
<!-- \begin{eqnarray} -->
<!-- y_t &=& \underbrace{c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p}}_{\mbox{AR(p) part}} + \underbrace{\beta_0 x_t + \dots + \beta_{r} x_{t-r}}_{\mbox{X(r) part}} + \nonumber \\ -->
<!-- &&\underbrace{\varepsilon_t + \theta_1\varepsilon_{t-1}+\dots +\theta_{q}\varepsilon_{t-q}.}_{\mbox{MA(q) part}}(\#eq:armaxirf) -->
<!-- \end{eqnarray} -->
<!-- where $\{\varepsilon_t\}$ is an i.i.d. white noise sequence and $\{x_t\}$ is an exogenous variable. -->

<!-- This algorithm was presented in Prop. \@ref(prop:computPsiARMAX). -->



ARMA processes do not allow to investigate the influence of an exogenous variable (say $x_t$) on the variable of interest (say $y_t$). When $x_t$ and $y_t$ have reciprocal influences, the Vector Autoregressive (VAR) model may be used (this tools will be studied later, in Section \@ref(VAR)). However, when one suspects that $x_t$ has an "exogenous" influence on $y_t$, then a simple extension of the ARMA processes may be considered. Loosely speaking, $x_t$ has an "exogenous" influence on $y_t$ if $y_t$ does not affect $x_t$. This extension is referred to as ARMA-X.

To begin with, let us formalize this notion of exogeneity. Consider a white noise sequence $\{\varepsilon_t\}$ (Def. \@ref(def:whitenoise)). This white noise will enter the dynamics of $y_t$, alongside with $x_t$; but $x_t$ will be exogenous to $\varepsilon_t$. (We will also say that $x_t$ is exogenous to $y_t$.)

:::{.definition #exogeneity name="Exogeneity"}
We say that $x_t$ is (strictly) exogenous to $\{\varepsilon_t\}$ if
$$
\mathbb{E}(\varepsilon_t|\underbrace{\dots,x_{t+1}}_{\mbox{future}},\underbrace{x_t,x_{t-1},\dots}_{\mbox{present and past}}) = 0.
$$
:::


<!-- If $x_t$ is exogenous to $\{\varepsilon_t\}$ then, in particular: -->
<!-- \begin{equation} -->
<!-- \mathbb{E}(\varepsilon_t|x_{t-k},x_{t-k-1},\dots)=0 \mbox{ for any } k\ge0,(\#eq:expepsx) -->
<!-- \end{equation} -->
<!-- which implies that: -->
<!-- \begin{equation} -->
<!-- \mathbb{E}(\varepsilon_tx_t|x_{t-k},x_{t-k-1},\dots)=0 \mbox{ for any } k\ge0, -->
<!-- \end{equation} -->
<!-- or -->
<!-- \begin{equation} -->
<!-- \mathbb{C}ov(\varepsilon_t,x_t|x_{t-k},x_{t-k-1},\dots)=0 \mbox{ for any } k\ge0, -->
<!-- \end{equation} -->

<!-- Past values of the exogenous variable do not allow to predict present and future values of $\varepsilon_t$ (Eq. \@ref(eq:expepsx)). -->

Hence, if $\{x_t\}$ is strictly exogenous to  $\varepsilon_t$, then past, present and future values of $x_t$ do not allow to predict the $\varepsilon_t$'s.

In the following, we assume that $\{x_t\}$ is a covariance stationary process.

:::{.definition #ARMAX name="ARMAX(p,q,r) model"}
The process $\{y_t\}$ is an ARMAX(p,q,r) if it follows a difference equation of the form:
\begin{eqnarray}
y_t &=& \underbrace{c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p}}_{\mbox{AR(p) part}} + \underbrace{\beta_0 x_t + \dots + \beta_{r} x_{t-r}}_{\mbox{X(r) part}} + \nonumber \\
&&\underbrace{\varepsilon_t + \theta_1\varepsilon_{t-1}+\dots +\theta_{q}\varepsilon_{t-q},}_{\mbox{MA(q) part}} (\#eq:DLM)
\end{eqnarray}
where $\{\varepsilon_t\}$ is an i.i.d. white noise sequence and $\{x_t\}$ is exogenous to $y_t$.
:::

<!-- The Autoregressive Distributed Lag (ADL) Model ADL(p,r) is an ARMAX(p,0,r) model (see @Stock_Watson_2003, Chapter 16). -->

What is the effect of a one-unit increase in $x_t$ on $y_t$? To address this question, this notion of "effect" has to be formalized. Let us introduce two related sequences of values for $\{x\}$. Denote the first by $\{a\}$ and the second by $\{\tilde{a}^t\}$. Further, we posit $a_s = \tilde{a}_s^t$ for all $s \ne t$, and $\tilde{a}_t^t = a_t+1$.

With these notations, we define $\frac{\partial y_{t+h}}{\partial x_t}$ as follows:
\begin{equation}
\frac{\partial y_{t+h}}{\partial x_t} := \mathbb{E}_{t-1}(y_{t+h}|\{x\} = \{\tilde{a}^t\}) - \mathbb{E}_{t-1}(y_{t+h}|\{x\} = \{a\}).(\#eq:dynmultX)
\end{equation}
Under the exogeneity assumption, it is easily seen that
<!-- \begin{eqnarray*} -->
<!-- y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \\ -->
<!-- &&\beta_0 x_t + \dots + \beta_{r} x_{t-r} +\\ -->
<!-- &&\varepsilon_t + \theta_1\varepsilon_{t-1}+\dots +\theta_{q}\varepsilon_{t-q}. -->
<!-- \end{eqnarray*} -->

$$
\frac{\partial y_t}{\partial x_t} = \beta_0.
$$
Now, since
\begin{eqnarray*}
y_{t+1} &=& c + \phi_1 y_{t} + \dots + \phi_p y_{t+1-p} + \beta_0 x_{t+1} + \dots + \beta_{r} x_{t+1-r} +\\
&&\varepsilon_{t+1} + \theta_1\varepsilon_{t}+\dots +\theta_{q}\varepsilon_{t+1-q},
\end{eqnarray*}
and using the exogeneity assumption, we obtain:
$$
\frac{\partial y_{t+1}}{\partial x_t} := \phi_1 \frac{\partial y_{t}}{\partial x_t} + \beta_1 = \phi_1\beta_0 + \beta_1.
$$
This can be applied recursively to give $\dfrac{\partial y_{t+h}}{\partial x_t}$ for any $h \ge 0$:


:::{.proposition #computPsiARMAX name="Dynamic multipliers in ARMAX models"}
One can recursively compute the dynamic multipliers $\frac{\partial y_{t+h}}{\partial x_t}$ as follows:

i. Initialization: $\dfrac{\partial y_{t+h}}{\partial x_t}=0$ for $h<0$.
ii. For $h \ge 0$ and assuming that the first $h-1$ multipliers have been computed, we have:
\begin{eqnarray}
\dfrac{\partial y_{t+h}}{\partial x_t} &=& \phi_1 \dfrac{\partial y_{t+h-1}}{\partial x_t} + \dots + \phi_p \dfrac{\partial y_{t+h-p}}{\partial x_t} + \beta_h,(\#eq:dynmultX)
\end{eqnarray}
where we use the notation $\beta_h=0$ if $h>r$.
:::


Remark that the resulting dynamic multipliers are the same as those obtained for an ARMA(p,r) model where the $\theta_i$'s are replaced with $\beta_i$'s (see Proposition \@ref(prp:computPsi) in Section \@ref(ARMAIRF)).

It has to be stressed that the definition of the dynamic multipliers (Eq. \@ref(eq:dynmultX)) does not reflect a potential persistency of the shock occuring on date $t$  in process $\{x\}$ itself. Going in this direction would necessitate to model the joint dynamics of $x_t$ (for instance using a VAR model, see Section \@ref(VAR)).

:::{.example #OrangeJuice name="Influence of the number of freezing days on the price of orange juice"}

This example is based on data used in @Stock_Watson_2003 (Chapter 16). The objective is to study the influence of the number of freezing days on the price of orange juice. Let us first estimate a ARMAX(0,0,12) model:

```{r freez, warning=FALSE, message=FALSE}
library(AEC);library(AER)
data("FrozenJuice")
FJ <- as.data.frame(FrozenJuice)
date <- time(FrozenJuice)
price <- FJ$price/FJ$ppi
T <- length(price)
k <- 1
dprice <- 100*log(price[(k+1):T]/price[1:(T-k)])
fdd <- FJ$fdd[(k+1):T]
par(mfrow=c(3,1))
par(plt=c(.1,.95,.15,.75))
plot(date,price,type="l",xlab="",ylab="",
     main="(a) Price of orange Juice")
plot(date,c(NaN,dprice),type="l",xlab="",ylab="",
     main="(b) Monthly pct Change (y)")
plot(date,FJ$fdd,type="l",xlab="",ylab="",
     main="(c) Number of freezing days (x)")
```
```{r freez2, warning=FALSE, message=FALSE}
nb.lags <- 3
FDD <- FJ$fdd[(nb.lags+1):T]
names.FDD <- NULL
for(i in 1:nb.lags){
  FDD <- cbind(FDD,FJ$fdd[(nb.lags+1-i):(T-i)])
  names.FDD <- c(names.FDD,paste(" Lag ",toString(i),sep=""))}
colnames(FDD) <- c(" Lag 0",names.FDD)
dprice <- dprice[(length(dprice)-dim(FDD)[1]+1):length(dprice)]
eq <- lm(dprice~FDD)
# Compute the Newey-West std errors:
var.cov.mat <- NeweyWest(eq,lag = 7, prewhite = FALSE)
robust_se <- sqrt(diag(var.cov.mat))
# Stargazer output (with and without Robust SE)
stargazer::stargazer(eq, eq, type = "text",
                     column.labels=c("(no HAC)","(HAC)"),keep.stat="n",
                     se = list(NULL,robust_se),no.space = TRUE)
```

Let us now use function `estim.armax`, from package `AEC`to fit an ARMA-X(2,0,1) model:

```{r freez3}
nb.lags.exog <- 1 # number of lags of exog. variable
FDD <- FJ$fdd[(nb.lags.exog+1):T]
for(i in 1:nb.lags.exog){
  FDD <- cbind(FDD,FJ$fdd[(nb.lags.exog+1-i):(T-i)])}
dprice <- 100*log(price[(k+1):T]/price[1:(T-k)])
dprice <- dprice[(length(dprice)-dim(FDD)[1]+1):length(dprice)]
res.armax <- estim.armax(Y = dprice,p=3,q=0,X=FDD)
```

Figure \@ref(fig:freez4) shows the IRF associated with each of the two models.

```{r freez4, echo=TRUE, fig.cap="Response of changes in orange juice price (in percent) to the number of freezing days. The solid (respectively dashed) line corresponds to the ARMAX(0,0,3) (resp. ARMAX(3,0,1)) model. The first model is estimated by OLS (see above), the second by MLE.", fig.asp = .6, out.width = "95%", fig.align = 'left-aligned'}
nb.periods <- 20
IRF1 <- sim.arma(c=0,phi=c(0),theta=eq$coefficients[2:(nb.lags+1)],sigma=1,
                 T=nb.periods,y.0=c(0),nb.sim=1,make.IRF=1)
IRF2 <- sim.arma(c=0,phi=res.armax$phi,theta=res.armax$beta,sigma=1,
                 T=nb.periods,y.0=rep(0,length(res.armax$phi)),
                 nb.sim=1,make.IRF=1)
par(plt=c(.15,.95,.2,.95))
plot(IRF1,type="l",lwd=2,col="red",xlab="months after shock",
     ylab="Chge in price (percent)")
lines(IRF2,lwd=2,col="red",lty=2)
abline(h=0,col="grey")
```
:::

<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->
<!-- \begin{exmpl}[Effect of monetary policy shocks] -->
<!-- \begin{itemize} -->
<!-- \item \href{https://www.aeaweb.org/articles?id=10.1257/mac.20130329}{Gertler and Karadi (2015)}'s type of shocks (high-frequency change in Euro-dollar futures). -->
<!-- \item Effect on 12-month growth rate of Industrial Production (IP)? -->
<!-- \item Data from \href{http://econweb.ucsd.edu/~vramey/research.html\#data}{Ramey's website}. -->
<!-- \end{itemize} -->
<!-- \begin{figure} -->
<!-- \caption{IP and MP shocks} -->
<!-- \begin{center} -->
<!-- \includegraphics[width=.8\linewidth]{../../figures/Figure_Ramey.pdf} -->
<!-- \end{center} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->


<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->
<!-- \addtocounter{exmpl}{-1} -->
<!-- \begin{exmpl}[Effect of monetary policy shocks (cont'd)] -->
<!-- \begin{figure} -->
<!-- \caption{IRF of IP growth rate to a 1 std-deviation MP shock} -->
<!-- \begin{center} -->
<!-- \includegraphics[width=.8\linewidth]{../../figures/Figure_Ramey2.pdf} -->
<!-- \end{center} -->

<!-- \begin{tiny} -->
<!-- ARMAX(1,1,1). estimated by MLE (Section \ref{section:MLE_ARMA}). The initial shock corresponds to one standard deviation of the series of monetary-policy shocks. The blue lines delineate the 95\% confidence interval. -->
<!-- \end{tiny} -->

<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->



:::{.example #Ramey1 name="Real effect of a monetary policy shock"}

In this example, we make use of monetary shocks identified through high-frequency data (see @Gertler_Karadi_2015). This dataset comes from [Valerie Ramey's website](https://econweb.ucsd.edu/~vramey/research.html) (see @Ramey_2016_NBER).


```{r Ramey1}
library(AEC)
T <- dim(Ramey)[1]
# Construct growth series:
Ramey$growth <- Ramey$LIP - c(rep(NaN,12),Ramey$LIP[1:(length(Ramey$LIP)-12)])
# Prepare matrix of exogenous variables:
vec.lags <- c(9,12,18)
Matrix.of.Exog <- NULL
shocks <- Ramey$ED2_TC
for(i in 1:length(vec.lags)){Matrix.of.Exog <-
  cbind(Matrix.of.Exog,c(rep(NaN,vec.lags[i]),shocks[1:(T-vec.lags[i])]))}
# Look for dates where data are available:
indic.good.dates <- complete.cases(Matrix.of.Exog)
```

```{r Ramey1fig, echo=FALSE, fig.cap="The blue line corresponds to monetary-policy shocks identified by means of the Gertler and Karadi (2015)'s approach (high-frequency change in Euro-dollar futures). The black slid line is the year-on-year growth rate of industrial production.", fig.asp = .6, out.width = "95%", fig.align = 'left-aligned'}
par(mfrow=c(1,1),plt=c(.15,.95,.15,.95))
plot(Ramey$DATES[indic.good.dates],100*Ramey$growth[indic.good.dates],type="l",
     ylim=c(-15,10),xlab="",ylab="in percent",lwd=2)
abline(h=0,col="grey")
lines(Ramey$DATES[indic.good.dates],40*Ramey$ED2_TC[indic.good.dates],col="blue",lwd=2)
legend("bottomleft", # places a legend at the appropriate place c("Health","Defense"), # puts text in the legend 
       c("12-month growth rate of IP","Gertler-Karadi shocks (x 40)"),
       lty=c(1,1), # gives the legend appropriate symbols (lines)       
       lwd=c(2), # line width
       col=c("black","blue"), # gives the legend lines the correct color and width
       pt.bg=c(1),
       pt.cex = c(1),
       bg="white",
       seg.len = 4
)
```



```{r Ramey2}
# Estimate ARMAX:
p <- 1; q <- 0
x <- estim.armax(Ramey$growth[indic.good.dates],p,q,
                 X=Matrix.of.Exog[indic.good.dates,])
# Compute IRF:
irf <- sim.arma(0,x$phi,x$beta,x$sigma,T=60,y.0=rep(0,length(x$phi)),
                nb.sim=1,make.IRF=1,X=NaN,beta=NaN)
```

Figure \@ref(fig:Ramey3) displays the resulting IRF, with a 95\% confidence band. The code used to produce the confidence bands (i.e., to compute the standard deviation of the dynamic multipliers for the different horizons) is based on the Delta method (see Appendix \@ref(Delta)).^[This method consists in approximating $\mathbb{V}(ar)(f(\theta))$ with $\frac{\partial f(\theta)}{\partial \theta}' \mathbb{V}ar(\theta)\frac{\partial f(\theta)}{\partial \theta}$.]


```{r Ramey3, echo=FALSE, fig.cap="Response of industrial-production growth to monetary-policy shocks. Dashed lines correpsond to the $\\pm$  2-standard-deviation bands.", fig.asp = .6, out.width = "95%", fig.align = 'left-aligned'}
irf.function <- function(THETA){
  c <- THETA[1]
  phi <- THETA[2:(p+1)]
  if(q>0){
    theta <- c(1,THETA[(1+p+1):(1+p+q)])
  }else{theta <- 1}
  sigma <- THETA[1+p+q+1]
  r <- dim(Matrix.of.Exog)[2] - 1
  beta <- THETA[(1+p+q+1+1):(1+p+q+1+(r+1))]
  
  irf <- sim.arma(0,phi,beta,sigma=sd(Ramey$ED3_TC,na.rm=TRUE),
                  T=60,y.0=rep(0,length(x$phi)),nb.sim=1,make.IRF=1,
                  X=NaN,beta=NaN)
  return(irf)}

IRF.0 <- 100*irf.function(x$THETA)
eps <- .00000001
d.IRF <- NULL
for(i in 1:length(x$THETA)){
  THETA.i <- x$THETA
  THETA.i[i] <- THETA.i[i] + eps
  IRF.i <- 100*irf.function(THETA.i)
  d.IRF <- cbind(d.IRF,
                 (IRF.i - IRF.0)/eps)}
mat.var.cov.IRF <- d.IRF %*% x$I %*% t(d.IRF)

par(mfrow=c(1,1),plt=c(.15,.95,.2,.95))
plot(IRF.0,type="l",
     ylim=c(min(IRF.0-2*sqrt(diag(mat.var.cov.IRF))),max(IRF.0+2*sqrt(diag(mat.var.cov.IRF)))),
     xlab="Number of months after shock",lwd=2,ylab="in percent")
abline(h=0,col="black",lty=3,lwd=2)
lines(IRF.0+2*sqrt(diag(mat.var.cov.IRF)),col="blue",lty=2,lwd=2)
lines(IRF.0-2*sqrt(diag(mat.var.cov.IRF)),col="blue",lty=2,lwd=2)
```
:::

## Maximum Likelihood Estimation (MLE) of ARMA processes {#estimARMA}

Consider the general case (of any time series); assume we observe a sample $\bv{y}=[y_1,\dots,y_T]'$. In order to implement ML techniques, we need to evaluate the joint p.d.f. (or "likelihood") of $\bv{y}$, i.e., $\mathcal{L}(\boldsymbol\theta;\bv{y})$, where $\boldsymbol\theta$ is a vector of parameters that characterizes the dynamics of $y_t$. The Maximum Likelihood (ML) estimate of $\boldsymbol\theta$ is then given by:
$$
\boxed{\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \mathcal{L}(\boldsymbol\theta;\bv{y})  = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\bv{y}).}
$$

In the time series context, if process $y_t$ is Markovian, then there exists a useful way to rewrite the likelihood $\mathcal{L}(\boldsymbol\theta;\bv{y})$. Let us first recall the definition of a Markovian process:

:::{.definition #Markov name="Markovian process"}
Process $y_t$ is Markovian of order one if $f_{Y_t|Y_{t-1},Y_{t-2},\dots} = f_{Y_t|Y_{t-1}}$. More generally, it is Markovian of order $k$ if $f_{Y_t|Y_{t-1},Y_{t-2},\dots} = f_{Y_t|Y_{t-1},\dots,Y_{t-k}}$.
:::

Now, remember Bayes' formula:
$$
\mathbb{P}(X_2=x,X_1=y) = \mathbb{P}(X_2=x|X_1=y)\mathbb{P}(X_1=y).
$$
Using it leads to the following decomposition of our likelihood function:
\begin{eqnarray*}
f_{Y_T,\dots,Y_1}(y_T,\dots,y_1;\boldsymbol\theta) &=&f_{Y_T|Y_{T-1},\dots,Y_1}(y_T,\dots,y_1;\boldsymbol\theta) \times \\
&& f_{Y_{T-1},\dots,Y_1}(y_{T-1},\dots,y_1;\boldsymbol\theta).
\end{eqnarray*}
Using the previous expression recursively, one obtains:
\begin{equation}
f_{Y_T,\dots,Y_1}(y_T,\dots,y_1;\boldsymbol\theta) = f_{Y_1}(y_1;\boldsymbol\theta) \prod_{t=2}^{T} f_{Y_t|Y_{t-1},\dots,Y_1}(y_t,\dots,y_1;\boldsymbol\theta).(\#eq:recursMLE)
\end{equation}

Let us start with the Gaussian AR(1) process (which is Markovian of order one):
$$
y_t = c + \phi_1 y_{t-1} + \varepsilon_t, \quad \varepsilon_t \sim\,i.i.d.\, \mathcal{N}(0,\sigma^2).
$$
For $t>1$:
$$
f_{Y_t|Y_{t-1},\dots,Y_1}(y_t,\dots,y_1;\boldsymbol\theta) = f_{Y_t|Y_{t-1}}(y_t,y_{t-1};\boldsymbol\theta)
$$
and
$$
f_{Y_t|Y_{t-1}}(y_t,y_{t-1};\boldsymbol\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y_t - c - \phi_1 y_{t-1})^2}{2\sigma^2}\right).
$$

These expressions can be plugged into Eq. \@ref(eq:recursMLE). But what about $f_{Y_1}(y_1;\boldsymbol\theta)$? There exist two possibilities:

1. **Case 1**: We use the marginal distribution: $y_1 \sim \mathcal{N}\left(\dfrac{c}{1-\phi_1},\dfrac{\sigma^2}{1-\phi_1^2}\right)$.
2. **Case 2**: $y_1$ is considered to be deterministic. In a way, that means that the first observation is "sacrificed".

For a Gaussian AR(1) process, we have:

1. **Case 1**: The (exact) log-likelihood is:
\begin{eqnarray}
\log \mathcal{L}(\boldsymbol\theta;\bv{y})  &=& - \frac{T}{2} \log(2\pi) - T\log(\sigma) + \frac{1}{2}\log(1-\phi_1^2)\nonumber \\
&& - \frac{(y_1 - c/(1-\phi_1))^2}{2\sigma^2/(1-\phi_1^2)} - \sum_{t=2}^T \left[\frac{(y_t - c - \phi_1 y_{t-1})^2}{2\sigma^2} \right].
\end{eqnarray}
The Maximum Likelihood Estimator of $\boldsymbol\theta= [c,\phi_1,\sigma^2]$ is obtained by numerical optimization.

2. **Case 2**: The (conditional) log-likelihood is:
\begin{eqnarray}
\log \mathcal{L}^*(\boldsymbol\theta;\bv{y})  &=& - \frac{T-1}{2} \log(2\pi) - (T-1)\log(\sigma)\nonumber\\
&& - \sum_{t=2}^T \left[\frac{(y_t - c - \phi_1 y_{t-1})^2}{2\sigma^2} \right].(\#eq:Lstar)
\end{eqnarray}

Exact MLE and conditional MLE have the same asymptotic (i.e. large-sample) distribution. Indeed, when the process is stationary, $f_{Y_1}(y_1;\boldsymbol\theta)$ makes a relatively negligible contribution to $\log \mathcal{L}(\boldsymbol\theta;\bv{y})$.

The conditional MLE has a substantial advantage: in the Gaussian case, the conditional MLE is simply obtained by OLS. Indeed, let us introduce the notations:
$$
Y = \left[\begin{array}{c}
y_2\\
\vdots\\
y_T
\end{array}\right] \quad and \quad
X = \left[\begin{array}{cc}
1 &y_1\\
\vdots&\vdots\\
1&y_{T-1}
\end{array}\right].
$$
Eq. \@ref(eq:Lstar) then rewrites:
\begin{eqnarray}
\log \mathcal{L}^*(\boldsymbol\theta;\bv{y})  &=& - \frac{T-1}{2} \log(2\pi) - (T-1)\log(\sigma) \nonumber \\
&& - \frac{1}{2\sigma^2} (Y-X[c,\phi_1]')'(Y-X[c,\phi_1]'),
\end{eqnarray}
which is maximised for:
\begin{eqnarray}
[\hat{c},\hat\phi_1]' &=& (X'X)^{-1}X'Y (\#eq:AROLSmean) \\
\hat{\sigma^2} &=& \frac{1}{T-1} \sum_{t=2}^T (y_t - \hat{c} - \hat{\phi_1}y_{t-1})^2 \nonumber \\
&=& \frac{1}{T-1} Y'(I - X(X'X)^{-1}X')Y. (\#eq:AROLSsigma)
\end{eqnarray}

Let us turn to the case of an AR(p) process. We have:
\begin{eqnarray*}
\log \mathcal{L}(\boldsymbol\theta;\bv{y}) &=& \log f_{Y_p,\dots,Y_1}(y_p,\dots,y_1;\boldsymbol\theta) +\\
&& \underbrace{\sum_{t=p+1}^{T} \log f_{Y_t|Y_{t-1},\dots,Y_{t-p}}(y_t,\dots,y_{t-p};\boldsymbol\theta)}_{\log \mathcal{L}^*(\boldsymbol\theta;\bv{y})}.
\end{eqnarray*}
where $f_{Y_p,\dots,Y_{1}}(y_p,\dots,y_{1};\boldsymbol\theta)$ is the marginal distribution of $\bv{y}_{1:p} := [y_p,\dots,y_1]'$. The marginal distribution of $\bv{y}_{1:p}$ is Gaussian; it is therefore fully characterised by its mean and covariance matrix:
\begin{eqnarray*}
\mathbb{E}(\bv{y}_{1:p})&=&\frac{c}{1-\phi_1-\dots-\phi_p} \bv{1}_{p\times 1} \\
\mathbb{V}ar(\bv{y}_{1:p}) &=& \left[\begin{array}{cccc}
\gamma_0 & \gamma_1 & \dots & \gamma_{p-1} \\
\gamma_1 & \gamma_0 & \dots & \gamma_{p-2} \\
\vdots &  & \ddots & \vdots \\
\gamma_{p-1} & \gamma_{p-2} & \dots & \gamma_{0} \\
\end{array}\right],
\end{eqnarray*}
where the $\gamma_i$'s are computed using the Yule-Walker equations (Eq. \@ref(eq:gammas)). Note that they depend, in a non-linear way, on the model parameters. Hence, the maximization of the exact log-likelihood necessitates numerical oprimization procedures. By contrast, the maximization of the conditional log-likelihood $\log \mathcal{L}^*(\boldsymbol\theta;\bv{y})$ only requires OLS, using Eqs. \@ref(eq:AROLSmean) and  \@ref(eq:AROLSsigma), with:
$$
Y = \left[\begin{array}{c}
y_{p+1}\\
\vdots\\
y_T
\end{array}\right] \quad and \quad
X = \left[\begin{array}{cccc}
1 & y_p & \dots & y_1\\
\vdots&\vdots&&\vdots\\
1&y_{T-1}&\dots&y_{T-p}
\end{array}\right].
$$

Again, for stationary processes, conditional and exact MLE have the same asymptotic  (large-sample) distribution. In small samples, the OLS formula is however biased. Indeed, consider the regression (where $y_t$ follows an AR(p) process):
\begin{equation}
y_t = \boldsymbol\beta'\bv{x}_t + \varepsilon_t,(\#eq:OLSregARp)
\end{equation}
with $\bv{x}_t = [1,y_{t-1},\dots,y_{t-p}]'$ and $\boldsymbol\beta = [c,\phi_1,\dots,\phi_p]'$. 

The bias results from the fact that $\bv{x}_t$ correlates to the $\varepsilon_s$'s for $s<t$. To be sure:
\begin{equation}
\bv{b} = \boldsymbol{\beta} + (X'X)^{-1}X'\boldsymbol\varepsilon,(\#eq:olsar1)
\end{equation}
and because of the specific form of $X$, we have non-zero correlation between $\bv{x}_t$ and $\varepsilon_s$ for $s<t$, therefore $\mathbb{E}[(X'X)^{-1}X'\boldsymbol\varepsilon] \ne 0$. Again, asymptotically, the previous expectation goes to zero, and we have:


:::{.proposition #cgceOLSARp name="Large-sample porperties of the OLS estimator of AR(p) models"}
Assume $\{y_t\}$ follows the AR(p) process:
$$
y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \varepsilon_t
$$
where $\{\varepsilon_{t}\}$ is an i.i.d. white noise process. If $\bv{b}$ is the OLS estimator of $\boldsymbol\beta$ (Eq. \@ref(eq:OLSregARp)), we have:
$$
\sqrt{T}(\bv{b}-\boldsymbol{\beta}) =  \underbrace{\left[\frac{1}{T}\sum_{t=p}^T \bv{x}_t\bv{x}_t' \right]^{-1}}_{\overset{p}{\rightarrow} \bv{Q}^{-1}}
\underbrace{\sqrt{T} \left[\frac{1}{T}\sum_{t=1}^T \bv{x}_t\varepsilon_t \right]}_{\overset{d}{\rightarrow} \mathcal{N}(0,\sigma^2\bv{Q})},
$$
where $\bv{Q} = \mbox{plim }\frac{1}{T}\sum_{t=p}^T \bv{x}_t\bv{x}_t'= \mbox{plim }\frac{1}{T}\sum_{t=1}^T \bv{x}_t\bv{x}_t'$ is given by:
\begin{equation}
\bv{Q} = \left[
\begin{array}{ccccc}
1 & \mu &\mu & \dots & \mu \\
\mu & \gamma_0 + \mu^2 & \gamma_1 + \mu^2 & \dots & \gamma_{p-1} + \mu^2\\
\mu & \gamma_1 + \mu^2 & \gamma_0 + \mu^2 & \dots & \gamma_{p-2} + \mu^2\\
\vdots &\vdots &\vdots &\dots &\vdots \\
\mu & \gamma_{p-1} + \mu^2 & \gamma_{p-2} + \mu^2 & \dots & \gamma_{0} + \mu^2
\end{array}
\right].(\#eq:Qols)
\end{equation}
:::
:::{.proof}
Rearranging Eq. \@ref(eq:olsar1), we have:
$$
\sqrt{T}(\bv{b}-\boldsymbol{\beta}) =  (X'X/T)^{-1}\sqrt{T}(X'\boldsymbol\varepsilon/T).
$$
Let us consider the autocovariances of $\bv{v}_t = \bv{x}_t \varepsilon_t$, denoted by $\gamma^v_j$. Using the fact that $\bv{x}_t$ is a linear combination of past $\varepsilon_t$'s and that $\varepsilon_t$ is a white noise, we get that $\mathbb{E}(\varepsilon_t\bv{x}_t)=0$. Therefore
$$
\gamma^v_j = \mathbb{E}(\varepsilon_t\varepsilon_{t-j}\bv{x}_t\bv{x}_{t-j}').
$$
If $j>0$, we have
\begin{eqnarray*}
\mathbb{E}(\varepsilon_t\varepsilon_{t-j}\bv{x}_t\bv{x}_{t-j}')&=&\mathbb{E}(\mathbb{E}[\varepsilon_t\varepsilon_{t-j}\bv{x}_t\bv{x}_{t-j}'|\varepsilon_{t-j},\bv{x}_t,\bv{x}_{t-j}])\\
&=&\mathbb{E}(\varepsilon_{t-j}\bv{x}_t\bv{x}_{t-j}'\mathbb{E}[\varepsilon_t|\varepsilon_{t-j},\bv{x}_t,\bv{x}_{t-j}])=0.
\end{eqnarray*}
Note that, for $j>0$, we have $\mathbb{E}[\varepsilon_t|\varepsilon_{t-j},\bv{x}_t,\bv{x}_{t-j}]=0$ because $\{\varepsilon_t\}$ is an i.i.d. white noise sequence. If $j=0$, we have:
$$
\gamma^v_0 = \mathbb{E}(\varepsilon_t^2\bv{x}_t\bv{x}_{t}')= \mathbb{E}(\varepsilon_t^2) \mathbb{E}(\bv{x}_t\bv{x}_{t}')=\sigma^2\bv{Q}.
$$
The convergence in distribution of $\sqrt{T}(X'\boldsymbol\varepsilon/T)=\sqrt{T}\frac{1}{T}\sum_{t=1}^Tv_t$ results from Theorem \@ref(thm:CLTcovstat) (applied on $\bv{v}_t=\bv{x}_t\varepsilon_t$), using the $\gamma_j^v$ computed above.
:::

These two cases (exact or conditional log-likelihoods) can be implemented when asking R to fit an AR process by means of function `arima`. Let us for instance use the output gap of the `US3var` dataset (US quarterly data, covering the period 1959:2 to 2015:1, used in @Gourieroux_Monfort_Renne_2017).

```{r UseArima}
library(AEC)
y <- US3var$y.gdp.gap
ar3.Case1 <- arima(y,order = c(3,0,0),method="ML")
ar3.Case2 <- arima(y,order = c(3,0,0),method="CSS")
rbind(ar3.Case1$coef,ar3.Case2$coef)
```

The two sets of estimated coefficients appear to be very close to each other.

Let us now turn to Moving-Average processes. Start with the MA(1):
$$
y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1},\quad \varepsilon_t \sim i.i.d.\mathcal{N}(0,\sigma^2).
$$
The $\varepsilon_t$'s are easily computed recursively, starting with $\varepsilon_t = y_t - \mu - \theta_1 \varepsilon_{t-1}$. We obtain:
$$
\varepsilon_t = y_t - \theta_1 y_{t-1} + \theta_1^2 y_{t-2}^2 + \dots + (-1)^{t-1} \theta_1^{t-1} y_{1} + (-1)^t\theta_1^{t}\varepsilon_{0}.
$$
Assume that one wants to recover the sequence of $\{\varepsilon_t\}$'s based on observed values of $y_t$ (from date 1 to date $t$). One can use the previous expression, but what value should be used for $\varepsilon_0$? If one does not use the true value of $\varepsilon_0$ but 0 (say), one does not obtain $\varepsilon_t$, but only an estimate of it ($\hat\varepsilon_t$, say), with:
$$
\hat\varepsilon_t = \varepsilon_t - (-1)^t\theta_1^{t}\varepsilon_{0}.
$$
Clearly, if $|\theta_1|<1$, then the error becomes small for large $t$. Formally, when $|\theta_1|<1$, we have:
$$
\hat\varepsilon_t \overset{p}{\rightarrow} \varepsilon_t.
$$
Hence, when $|\theta_1|<1$, a consistent estimate of the conditional log-likelihood is given by:
\begin{equation}
\log \hat{\mathcal{L}}^*(\boldsymbol\theta;\bv{y}) = -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \sum_{t=1}^T \frac{\hat\varepsilon_t^2}{2\sigma^2}.(\#eq:MALstar)
\end{equation}
Loosely speaking, if $|\theta_1|<1$ and if $T$ is sufficiently large:
$$
\mbox{approximate conditional MLE $\approx$ exact MLE.}
$$

Note that $\hat{\mathcal{L}}^*(\boldsymbol\theta;\bv{y})$ is a complicated nonlinear function of $\mu$ and $\theta$. Its maximization therefore has to be based on numerical optimization procedures.

Let us not consider the case of a Gaussian MA($q$) process:
\begin{equation}
y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q} , \quad \varepsilon_t \sim i.i.d.\mathcal{N}(0,\sigma^2). (\#eq:estimMAq)
\end{equation}

Let us assume that this process is an **invertible MA process**. That is, assume that the roots of:
\begin{equation}
\lambda^q + \theta_1 \lambda^{q-1} + \dots + \theta_{q-1} \lambda + \theta_q = 0 (\#eq:invertible)
\end{equation}
lie strictly inside of the unit circle. In this case, the polynomial $\Theta(L)=1 + \theta_1 L + \dots + \theta_q L^q$ is *invertible* and Eq. \@ref(eq:estimMAq) writes:
$$
\varepsilon_t = \Theta(L)^{-1}(y_t - \mu),
$$
which implies that, if we knew all past values of $y_t$, we would also know $\varepsilon_t$. In this case, we can consistently estimate the $\varepsilon_t$'s by recursively computing the $\hat\varepsilon_t$'s as follows (for $t>0$):
\begin{equation}
\hat\varepsilon_t = y_t - \mu - \theta_1 \hat\varepsilon_{t-1} - \dots  - \theta_q \hat\varepsilon_{t-q},(\#eq:condiVarepsiMABB)
\end{equation}
with
\begin{equation}
\hat\varepsilon_{0}=\dots=\hat\varepsilon_{-q+1}=0.(\#eq:condiVarepsiMA)
\end{equation}

In this context, a consistent estimate of the conditional log-likelihood is still given by Eq. \@ref(eq:MALstar), using Eqs. \@ref(eq:condiVarepsiMABB) and \@ref(eq:condiVarepsiMA) to recursively compute the $\hat\varepsilon_t$'s.

Note that we could determine the exact likelihood of an MA process. Indeed, vector $\bv{y} = [y_1,\dots,y_T]'$ is a Gaussian-distributed vector of mean $\boldsymbol\mu = [\mu,\dots,\mu]'$ and of variance:
$$
\boldsymbol\Omega = \left[\begin{array}{ccccccc}
\gamma_0 & \gamma_1&\dots&\gamma_q&{\color{red}0}&{\color{red}\dots}&{\color{red}0}\\
\gamma_1 & \gamma_0&\gamma_1&&\ddots&{\color{red}\ddots}&{\color{red}\vdots}\\
\vdots & \gamma_1&\ddots&\ddots&&\ddots&{\color{red}0}\\
\gamma_q &&\ddots&&&&\gamma_q\\
{\color{red}0} &&&\ddots&\ddots&\ddots&\vdots\\
{\color{red}\vdots}&{\color{red}\ddots}&\ddots&&\gamma_1&\gamma_0&\gamma_1\\
{\color{red}0}&{\color{red}\dots}&{\color{red}0}&\gamma_q&\dots&\gamma_1&\gamma_0
\end{array}\right],
$$
where the $\gamma_j$'s are given by Eq. \@ref(eq:autocovMA). The p.d.f. of $\bv{y}$ is then given by (see Prop. \@ref(prp:pdfMultivarGaussian)):
$$
(2\pi)^{-T/2}|\boldsymbol\Omega|^{-1/2}\exp\left( -\frac{1}{2} (\bv{y}-\boldsymbol\mu)' \boldsymbol\Omega^{-1} (\bv{y}-\boldsymbol\mu)\right).
$$
For large samples, the computation of this likelihood however becomes numerically demanding. 

Finally, let us consider the MLE of an ARMA($p$,$q$) processes:
$$
y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} +
\dots + \theta_q \varepsilon_{t-q} , \; \varepsilon_t \sim i.i.d.\,\mathcal{N}(0,\sigma^2).
$$
If the MA part of this process is invertible, the log-likelihood function can be consistently approximated by its conditional counterpart (of the form of Eq. \@ref(eq:MALstar)), using consistent estimates $\hat\varepsilon_t$ of the $\varepsilon_t$. The $\hat\varepsilon_t$'s are computed recursively as:
\begin{equation}
\hat\varepsilon_t = y_t - c - \phi_1 y_{t-1} - \dots - \phi_p y_{t-p} - \theta_1 \hat\varepsilon_{t-1} - \dots - \theta_q \hat\varepsilon_{t-q},(\#eq:recvareps)
\end{equation}
given some initial conditions, for instance:

a. $\hat\varepsilon_0=\dots=\hat\varepsilon_{-q+1}=0$ and $y_{0}=\dots=y_{-p+1}=\mathbb{E}(y_i)=\mu$. (Recursions in Eq. \@ref(eq:recvareps) then start for $t=1$.)
b. $\hat\varepsilon_p=\dots=\hat\varepsilon_{p-q+1}=0$ and actual values of the $y_{i}$'s for $i \in [1,p]$. In that case, the first $p$ observations of $y_t$ will not be used. Recursions in Eq. \@ref(eq:recvareps) then start for $t=p+1$.

## Specification choice

The previouss section explains how to fit a given ARMA specification. But how to choose an appropriate specification? A possibility is to employ the (P)ACF approach (see Figure \@ref(fig:pacf)). However, the previous approach leads to either an AR or a MA process (and not an ARMA process). If one wants to consider various ARMA(p,q) specifications, for $p \in \{1,\dots,P\}$ and $q \in \{1,\dots,Q\}$, say, then one can resort to **information criteria**.

In general, when choosing a specification, one faces the following dilemma:

a. Too rich a specification may lead to "overfitting"/misspecification, implying additional estimation errors (in out-of-sample forecasts).
b. Too simple a specification may lead to potential omission of valuable information (e.g., contained in older lags).

The lag selection approach based on the so-called **information criteria** consists in maximizing the fit of the data, but adding a penalty for the "richness" of the model. More precisely, using this approach amounts to minimizing a loss function that (a) negatively depends on the fitting errors and (b) positively depends on the number of parameters in the model. 

:::{.definition #infocriteria name="Information Criteria"}
The Akaike (AIC), Hannan-Quinn (HQ) and Schwarz information (BIC) criteria are of the form
$$
c^{(i)}(k) = \underbrace{\frac{- 2 \log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\bv{y})}{T}}_{\mbox{decreases w.r.t. $k$}} \quad +
\underbrace{
\frac{k\phi^{(i)}(T)}{T},}_{\mbox{increases w.r.t. $k$}}
$$
with  $(i) \in\{AIC,HQ,BIC\}$ and where $\hat{\boldsymbol\theta}_T(k)$ denotes the ML estimate of $\boldsymbol\theta_0(k)$, which is a vector of parameters of length $k$.

\begin{center}
\begin{tabular}{lcl}
\hline
Criterion (i) && $\phi^{(i)}(T)$\\
\hline
Akaike &AIC & $2$ \\
Hannan-Quinn & HQ & $2\log(\log(T))$ \\
Schwarz &BIC & $\log(T)$ \\
\hline
\end{tabular}
\end{center}

The lag suggested by criterion $(i)$ is then given by:
$$
\boxed{\hat{k}^{(i)} = \underset{k}{\mbox{argmin}} \quad c^{(i)}(k).}
$$
:::

In the case of an ARMA(p,q) process, $k=2+p+q$.


:::{.proposition #infocriteria name="Consistency of the criteria-based lag selection"}
The lag selection procedure is consistent if
$$
\lim_{T \rightarrow \infty} \phi(T) = \infty \quad and \quad \lim_{T \rightarrow \infty} \phi(T)/T = 0.
$$
This is notably the case of the HQ and the BIC criteria.
:::

:::{.proof}
The true number of lags is denoted by $k_0$. We will show that  $\lim_{T \rightarrow \infty} \mathbb{P}(\hat{k}_T \ne k_0)=0$.

* Case $k < k_0$:  The model with $k$ parameter is misspecified, therefore:
$$
\mbox{plim}_{T \rightarrow \infty}  \log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\bv{y})/T < \mbox{plim}_{T \rightarrow \infty}  \log \mathcal{L}(\hat{\boldsymbol\theta}_T(k_0);\bv{y})/T.
$$
Hence, if $\lim_{T \rightarrow \infty} \phi(T)/T = 0$, we have: $\lim_{T \rightarrow \infty} \mathbb{P}(c(k_0) \ge c(k)) \rightarrow 0$ and
$$
\lim_{T \rightarrow \infty} \mathbb{P}(\hat{k}<k_0) \le \lim_{T \rightarrow \infty} \mathbb{P}\left\{c(k_0) \ge c(k) \mbox{ for some $k < k_0$}\right\} = 0.
$$
* Case $k > k_0$: under the null hypothesis, the likelihood ratio (LR) test statistic satisfies:
$$
2 \left(\log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\bv{y})-\log \mathcal{L}(\hat{\boldsymbol\theta}_T(k_0);\bv{y})\right) \sim \chi^2(k-k_0).
$$
If $\lim_{T \rightarrow \infty} \phi(T) = \infty$, we have: $\mbox{plim}_{T \rightarrow \infty} -2 \left(\log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\bv{y})-\log \mathcal{L}(\hat{\boldsymbol\theta}_T(k_0);\bv{y})\right)/\phi(T) = 0$. Hence $\mbox{plim}_{T \rightarrow \infty} T[c(k_0) - c(k)]/\phi(T) \le -1$ and $\lim_{T \rightarrow \infty} \mathbb{P}(c(k_0) \ge c(k)) \rightarrow 0$, which implies, in the same spirit as before, that $\lim_{T \rightarrow \infty} \mathbb{P}(\hat{k}>k_0) = 0$.

Therefore, $\lim_{T \rightarrow \infty} \mathbb{P}(\hat{k}=k_0) = 1$.
:::

:::{.example #ICOLS name="Linear regression"}
Consider a linear regression with normal disturbances:
$$
y_t = \bv{x}_t' \boldsymbol\beta + \varepsilon_t, \quad \varepsilon_t \sim i.i.d. \mathcal{N}(0,\sigma^2).
$$
The associated log-likelihood is of the form of Eq. \@ref(eq:MALstar). In that case, we have:
\begin{eqnarray*}
c^{(i)}(k) &=& \frac{- 2 \log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\bv{y})}{T} + \frac{k\phi^{(i)}(T)}{T}\\
&\approx& \log(2\pi) + \log(\widehat{\sigma^2}) + \frac{1}{T}\sum_{t=1}^T \frac{\varepsilon_t^2}{\widehat{\sigma^2}} + \frac{k\phi^{(i)}(T)}{T}.
\end{eqnarray*}
For a large $T$, for all consistent estimation scheme, we have:
$$
\widehat{\sigma^2} \approx \frac{1}{T}\sum_{t=1}^T \varepsilon_t^2 = SSR/T.
$$
Hence $\hat{k}^{(i)} \approx \underset{k}{\mbox{argmin}} \quad  \log(SSR/T) + \dfrac{k\phi^{(i)}(T)}{T}$.
:::


:::{.example #SwissGrowthAIC name="Swiss GDP growth"}

Consider a long historical time series of the Swiss GDP growth (see Figure \@ref(fig:autocov)), taken from the @JST_2017 dataset. Let us look for the best ARMA specification using the AIC criteria:

```{r lagSelectARMA, warning=FALSE, message=FALSE}
library(AEC);data(JST)
data <- subset(JST,iso=="CHE")
T <- dim(data)[1]
y <- c(NaN,log(data$gdp[2:T]/data$gdp[1:(T-1)]))
# Use AIC criteria to look for appropriate specif:
max.p <- 3;max.q <- 3;
all.AIC <- NULL
for(p in 0:max.p){
  for(q in 0:max.q){
    res <- arima(y,order=c(p,0,q))
    if(res$aic<min(all.AIC)){best.p<-p;best.q<-q}
    all.AIC <- c(all.AIC,res$aic)}}
print(c(best.p,best.q))
```

The best specification therefore is an AR(1) model. That is, although an AR(2) (say) would result in a better fit of the data, the fit improvement is not be large enough to compensate for the additional AIC cost associated with an additional parameter.
:::

